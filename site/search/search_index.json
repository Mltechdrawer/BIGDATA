{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Big DataWelcome to Big Data","text":""},{"location":"#big-data_1","title":"\ud83d\udcca Big Data","text":"<p>Big Data is a compulsory course of the 3rd year in the Degree in Data Science and Engineering (1st semester, 6 ECTS). It provides students with the theoretical foundations and practical skills to design, implement, and analyse data-intensive applications that are reliable, scalable, and maintainable.</p> <p>The course combines theory, practical exercises, and laboratory sessions. Weekly activities are closely related to the theoretical content, reinforcing knowledge through a group project with iterative development and presentations. Students will acquire essential competences in parallel programming, distributed systems, and performance engineering \u2014 all key skills for modern data engineering and analytics.</p> <p>The learning approach emphasises individual and team work, fostering the ability to apply theoretical concepts to real-world Big Data problems. This course serves as a solid foundation for advanced subjects related to distributed systems, cloud computing, and large-scale machine learning.</p> <p>Faculty:</p> <ul> <li>Jos\u00e9 Juan Hern\u00e1ndez Cabrera. Practical Sessions Responsible  </li> <li>Jos\u00e9 \u00c9vora G\u00f3mez.  </li> <li>Mar\u00eda Dolores Afonso Su\u00e1rez. Course Coordinator  </li> </ul> <p></p> <p></p> <p> </p> <p>\u00a9 2025 Mar\u00eda Dolores Afonso Su\u00e1rez. Este material se distribuye bajo licencia Creative Commons Atribuci\u00f3n 4.0 Internacional (CC BY 4.0).</p>"},{"location":"contents/","title":"Contents","text":"<p>Big Data is structured in two blocks that guide students from the theoretical foundations of complexity and architectures, to the practical aspects of distributed and parallel programming. The aim is to provide the knowledge and skills required to understand, design, and implement scalable and efficient data-intensive applications.</p> <p>BLOCK 1. Theoretical Concepts of Big Data covers complexity management, monitoring and performance engineering, and architectures for Big Data systems. These topics provide the conceptual basis for understanding the challenges of reliability, scalability, and fault tolerance in large-scale data systems.</p> <p>BLOCK 2. Distributed and Parallel Programming introduces the fundamentals of parallel programming, MapReduce, distributed file systems, cluster-oriented application development, and vector programming. These topics consolidate the student\u2019s ability to design and implement Big Data solutions that exploit parallelism, distribution, and modern computational infrastructures.</p>"},{"location":"introduction/","title":"Introduction to Big Data","text":"<p>A new era of data-driven innovation</p>"},{"location":"introduction/#from-early-computing-to-the-digital-revolution","title":"From Early Computing to the Digital Revolution","text":"<p>The journey of Big Data starts with the evolution of computing itself:</p> <ul> <li>1950s: John von Neumann formalized computer architecture, setting the basis for digital computation. At the same time, advances in numerical algorithms and game theory influenced how data and complexity would be modeled.  </li> <li>1960s\u20131970s: Paul Eisler\u2019s printed circuit boards and John Bardeen\u2019s transistor discoveries paved the way for miniaturization. Jack Kilby\u2019s microchip and Douglas Engelbart\u2019s vision of interactive computing changed how people interacted with technology.  </li> <li>1980s\u20131990s: Ted Nelson conceptualized hypertext, Robert Metcalfe created Ethernet, and Tim Berners-Lee at CERN invented the World Wide Web, democratizing access to global information. Alan Kay and Engelbart further advanced the idea of graphical user interfaces (GUI).  </li> <li>2000s: Complex systems modeling, network growth, and ubiquitous devices fueled an explosion of data that set the stage for the Big Data era.</li> </ul> <p> Big Data era</p>"},{"location":"introduction/#the-convergence-of-statistics-and-artificial-intelligence","title":"The Convergence of Statistics and Artificial Intelligence","text":"<p>Big Data analytics is powered by advances in mathematics, statistics, and AI:</p> <ul> <li>Thomas Bayes: Introduced probabilistic reasoning, enabling decision-making under uncertainty.  </li> <li>Geoffrey Hinton: Pioneered deep learning, with neural networks that can process speech, images, and language at scale.  </li> <li>Richard Sutton: Advanced reinforcement learning, allowing systems to learn by trial and error, simulating real-world decision-making.  </li> </ul> <p>Together, these approaches transformed massive datasets into knowledge and actionable insights.</p> <p></p>"},{"location":"introduction/#what-is-big-data","title":"What Is Big Data?","text":"<p>Big Data refers to datasets that are too large, fast, or diverse to be managed by traditional methods. Its defining characteristics are often expressed as the 5Vs:</p> <ul> <li>Volume: massive quantities of data.  </li> <li>Velocity: real-time or near real-time data flows.  </li> <li>Variety: text, images, video, audio, and sensor data.  </li> <li>Veracity: ensuring accuracy and reliability.  </li> <li>Value: extracting useful insights for decision-making.  </li> </ul> <p></p>"},{"location":"introduction/#how-big-is-big","title":"How big is \u201cBig\u201d?","text":"<p>An exabyte (EB) equals one million terabytes (TB). For example: - 40 EB = 40,000 PB = 40,000,000 TB = 40,000,000,000 GB. This unimaginable scale is equivalent to millions of modern hard drives, highlighting the storage and processing challenges of Big Data.</p>"},{"location":"introduction/#big-data-architectures","title":"Big Data Architectures","text":"<ul> <li>Data Lake: Centralized repository that stores raw, unstructured, and semi-structured data. Highly flexible, but lacks governance and fast querying.  </li> <li>Data Warehouse: Structured, schema-based system optimized for analytics and business intelligence (BI). Enforces ACID transactions and strong data consistency.  </li> <li>Data Lakehouse: Hybrid architecture combining the scalability of lakes with the reliability of warehouses. Supports both raw and structured data, ACID compliance, and efficient queries.  </li> </ul> <p>The Lakehouse approach is becoming the standard in modern Big Data platforms.</p> <p></p>"},{"location":"introduction/#highlights-of-big-data-in-the-21st-century","title":"Highlights of Big Data in the 21st Century","text":"<p>Recent and emerging trends shaping Big Data include:</p> <ol> <li>Rise of cloud-native Big Data solutions.  </li> <li>Real-time data processing at scale.  </li> <li>Privacy and governance as central challenges.  </li> <li>Edge computing for localized analytics.  </li> <li>Integration of AI and machine learning into data platforms.  </li> <li>Big Data applications in healthcare and medicine.  </li> <li>Data democratization and self-service analytics.  </li> <li>Data lakes and lakehouses as dominant architectures.  </li> <li>Advances in Natural Language Processing (NLP) for text and speech.  </li> <li>Quantum computing as a future enabler of large-scale analytics.  </li> <li>Big Data for sustainability and climate change.  </li> <li>Data Fabric and Data Mesh architectures for decentralized governance.  </li> </ol>"},{"location":"introduction/#the-impact-of-big-data-on-society","title":"The Impact of Big Data on Society","text":"<p>Big Data influences multiple aspects of daily life and industry:</p> <ul> <li>Spam and fraud detection.  </li> <li>Recommender systems powering e-commerce and entertainment.  </li> <li>Emotional AI analyzing human sentiment.  </li> <li>Sensorization and IoT for smart cities, vehicles, and health.  </li> <li>Video analysis for security, transport, and medical imaging.  </li> </ul> <p> Spam</p> <p> Fraud</p> <p> Emotional Intelligence</p> <p> Sensorization</p> <p> Recommendation systems</p> <p>These applications show Big Data\u2019s power to deliver innovation, while also raising challenges of ethics, privacy, and fairness.</p>"},{"location":"introduction/#why-study-big-data","title":"Why Study Big Data?","text":"<p>By the end of this course, students will be able to:</p> <ul> <li>Design scalable and distributed systems.  </li> <li>Implement parallel programming and data-intensive algorithms.  </li> <li>Integrate machine learning and AI into Big Data workflows.  </li> <li>Address social and ethical challenges in the use of large-scale data.  </li> </ul> <p>Big Data is not only about technology \u2014 it is about shaping the future of science, business, and society in a data-driven world.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/","title":"Additive vs. Multiplicative Costs in Graphs","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#1-additive-cost","title":"1. Additive Cost","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#definition","title":"Definition","text":"<p>In an additive cost, the total value of a path is the sum of the individual costs of each edge:</p> <p>Total Cost (A \u2192 C) = A_AB + A_BC</p> <p>This model is used when each step adds an independent cost, such as: - Distance traveled - Execution time - Energy consumed - Accumulated price  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#example","title":"Example","text":"<p>Imagine a graph where nodes represent cities and edge weights are distances (in km):</p> Route Cost (km) A \u2192 B 5 B \u2192 C 7 A \u2192 C \u2014 <p>Then, the total additive cost from A to C via B is:</p> <p>Cost (A \u2192 B \u2192 C) = 5 + 7 = 12</p> <p>If there are multiple paths, we choose the one with the minimum total sum using algorithms such as Dijkstra or Floyd\u2013Warshall.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#2-multiplicative-cost","title":"2. Multiplicative Cost","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#definition_1","title":"Definition","text":"<p>In a multiplicative cost, the total value of a path is the product of the individual costs (or probabilities, or reliability factors) of each edge:</p> <p>Total Cost (A \u2192 C) = A_AB \u00d7 A_BC</p> <p>This is used when each step transforms or attenuates the effect of the previous one, such as: - Success probabilities - Transmission coefficients (signal loss or energy) - Relationship strength or affinity - Growth or decay factors  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#example_1","title":"Example","text":"<p>Suppose edge weights represent probabilities of success:</p> Route Probability A \u2192 B 0.9 B \u2192 C 0.8 A \u2192 C \u2014 <p>Then, the probability of reaching C from A through B is:</p> <p>P(A \u2192 B \u2192 C) = 0.9 \u00d7 0.8 = 0.72</p> <p>Each step reduces the overall probability of success.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#3-comparison","title":"3. Comparison","text":"Concept Additive Cost Multiplicative Cost Operation Sum (+) Product (\u00d7) Typical Context Distance, time, energy Probability, reliability, intensity Total Behavior Increases with each step Decreases or amplifies Example Algorithms Dijkstra, Floyd\u2013Warshall Markov models, probabilistic propagation"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#4-graphical-interpretation","title":"4. Graphical Interpretation","text":"<p>Graph 1 (Additive \u2013 distances): A \u2014(5)\u2014 B \u2014(7)\u2014 C \u2192 total cost = 12  </p> <p>Graph 2 (Multiplicative \u2013 probabilities): A \u2014(0.9)\u2014 B \u2014(0.8)\u2014 C \u2192 total cost = 0.72  </p> <p>In the first case, traveling becomes more expensive with each step. In the second, success becomes less likely with each step.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/","title":"Applications: Application of Matrix Multiplication to Graph Theory","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#introduction","title":"Introduction","text":"<p>\"Programming is usually taught by examples\" \u2014 Niklaus Wirth (1934-2024) </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#brief-biography","title":"Brief Biography","text":"<p>Niklaus Wirth was an influential computer scientist, creator of programming languages such as Pascal, Modula, and Oberon. He received the Turing Award in 1984 for his contributions to programming language design and software methodologies. His teaching approach always emphasized the use of practical examples to explain complex concepts.</p> <p>Matrix multiplication plays a crucial role in many Big Data applications, especially when data is modeled as graphs or networks. Graphs allow us to represent and analyze massive, interconnected datasets in areas such as social networks, biological systems, computer networks, and recommendation systems. By leveraging matrix multiplication, we can efficiently compute paths, connectivity, influence, and ranking in graphs \u2014 operations that are fundamental to large-scale data analysis. In Big Data, where datasets may include millions of nodes and billions of connections, matrix-based approaches provide a scalable mathematical framework for tasks such as link prediction, shortest path discovery, clustering, and random walks.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#applications-of-graphs-across-domains","title":"Applications of graphs across domains","text":"<ul> <li>Social Networks: Nodes represent users, and edges represent friendships or follower relationships.  Example</li> <li>Transportation Networks: Nodes are locations (cities, airports), and edges are routes (roads, flights). Example </li> <li>Web Pages: Nodes are web pages, and edges are hyperlinks between them. Example </li> <li>Computer Networks: Nodes are devices, and edges are communication links.  Example </li> <li>Biological Networks: Nodes are genes or proteins, and edges are interactions or regulatory relationships.  Example </li> <li>Recommendation Systems: Nodes represent users and items, with edges indicating preferences or ratings.  Example </li> <li>Knowledge Graphs: Nodes represent entities (people, places, things), and edges represent semantic relationships. Example </li> <li>Scheduling Problems: Nodes represent tasks, and edges represent dependencies. Example </li> </ul> <p>Example </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#social-networks","title":"Social Networks","text":"<p>Efficient Relationship Modelling: Graphs can naturally represent social connections and quickly identify relationships like mutual friends or groups. Pathfinding: Algorithms like Breadth-First Search (BFS) can find the shortest path between two people, useful for \"degrees of separation.\"</p> <p></p> <p>Social Network</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#transportation-networks","title":"Transportation Networks","text":"<p>Optimal Path Finding: Algorithms like Dijkstra's or A(asterisk) can find the shortest or fastest route between locations. Network Analysis: Graphs make it easy to analyze connectivity (e.g., detecting if certain cities are isolated).</p> <p></p> <p>Transportation Network</p> <p></p> <p>Transportation Network</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#web-pages","title":"Web Pages","text":"<p>Link Analysis: Algorithms such as PageRank use the graph structure of the web to rank pages based on the number of incoming and outgoing links. Crawling and Indexing: Graph traversal techniques help search engines crawl the web efficiently.</p> <p></p> <p>This visualization represents the network of web pages connected to voson.anu.edu.au obtained by a web crawl, modified from Fig. 12.9 of the NodeXL Book [27, p. 192].</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#computer-networks","title":"Computer Networks","text":"<p>Routing Algorithms: Graphs enable efficient packet routing by finding the shortest path in a network. Failure Detection: It helps identify potential points of failure and improve the robustness of the network.</p> <p></p> <p>Transportation Network</p> <p></p> <p>Transportation Network detail</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#biological-networks","title":"Biological Networks","text":"<p>Pathway Analysis: Graphs model complex biological pathways, helping to identify critical interactions in gene regulation or protein networks. Community Detection: Helps identify clusters or communities of related genes or proteins, useful in understanding functional modules.</p> <p></p> <p>Biological Network</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#recommendation-systems","title":"Recommendation Systems","text":"<p>User-Item Relationship Modelling: Graphs can represent complex relationships between users and items, enabling collaborative filtering. Graph-based Ranking: Algorithms can rank items based on their connections to other highly-rated items.</p> <p></p> <p>Recommendation System Network</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#knowledge-graphs","title":"Knowledge Graphs","text":"<p>Semantic Search: Allows for more natural language search by understanding relationships between entities. Reasoning and Inference: Graph algorithms can help infer new knowledge based on existing relationships.</p> <p></p> <p>Knowledge Graph</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#scheduling-problems","title":"Scheduling Problems","text":"<p>Dependency Resolution: Graphs can represent task dependencies, making it easier to identify the correct order of execution. Cycle Detection: Can detect circular dependencies that would prevent proper scheduling.</p> <p></p> <p>Tasks Graph representation</p>  Graph analisys <p>Besides paths and influence, graph analysis usually includes:</p> <p> Communities (natural groups)</p> <p> Centrality (ways of measuring importance)</p> <p> Anomalies (detecting unusual nodes)</p> <p> Temporal evolution (network dynamics)</p> <p> Diffusion/propagation (information, viruses, etc.)</p> <p> Patterns and motifs (recurring local structures)</p> <p> Matching (users\u2013items, resources\u2013tasks)</p> <p> Robustness (what happens if nodes/edges fail)</p> <p>Exercises</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/","title":"Length of Paths","text":"<p>In graph theory, one of the most interesting questions we can ask is: How many different ways are there to go from one node to another in exactly k steps? This concept is fundamental in Big Data applications, where we often want to explore connectivity in very large networks such as social graphs, transportation systems, or biological networks.</p> <p>The tool that allows us to answer this question is the adjacency matrix of a graph and its powers. By multiplying the adjacency matrix by itself, we can discover not just direct connections, but also paths of longer lengths.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#explanation","title":"Explanation","text":"<ul> <li>Let A be the adjacency matrix of a graph with nodes ( V = {v_1, v_2, ..., v_n} ).  </li> <li>The entry ( A_{ij} = 1 ) if there is an edge from node ( v_i ) to node ( v_j ), and 0 otherwise.  </li> <li>When we compute ( A^2 ), the entry ( (A^2)_{ij} ) gives the number of paths of length 2 between ( v_i ) and ( v_j ).  </li> <li>More generally, ( (A^k)_{ij} ) tells us the number of paths of length k from ( v_i ) to ( v_j ).  </li> </ul> <p>This makes adjacency matrix powers a very powerful tool for analyzing networks in Big Data: with a single matrix operation we can count millions of possible connections!</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#example","title":"Example","text":"<p>Consider a small graph with 6 nodes: A, B, C, D, E, F. Suppose we want to know the number of different paths from A to F.</p> <ul> <li>1-step path: directly from A to F (if the edge exists).  </li> <li>2-step path: A \u2192 B \u2192 F.  </li> <li>3-step path: A \u2192 B \u2192 D \u2192 F.  </li> <li>5-step path: A \u2192 B \u2192 C \u2192 E \u2192 B \u2192 F.  </li> </ul> <p>Mathematically, these are obtained from the adjacency matrix A: - From A itself, we see all direct edges (paths of length 1). - From A\u00b2, we get all paths of length 2. - From A\u00b3, we get all paths of length 3.  </p> <p>This illustrates how matrix multiplication helps us uncover paths of arbitrary length in a graph, which is extremely useful for analyzing reachability in large-scale networks.</p> <p> </p> \ud83d\udca1 Specific problem: <p>  Detecting users that can be influenced in a limited number of steps.</p> <p>Suppose a user A publishes an information (news, rumour or advertising product), and we want to know which users can be reached through interactions in exactly n steps. For example, we want to see who could receive the information if it is shared among friends up to three times (i.e. in paths of length 3). </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#repeated-squaring-and-complexity","title":"Repeated Squaring and Complexity","text":"<p>To compute shortest paths efficiently in large networks, we need more than a naive approach. Repeated squaring is a powerful method that reduces the number of computations while still giving us exact results.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#explanation_1","title":"Explanation","text":"<ul> <li>The idea: instead of computing powers sequentially, we square the matrix repeatedly.  </li> <li>Formally:  </li> <li>( W^2 = W \\otimes W ) (using min-plus multiplication).  </li> <li>( W^4 = W^2 \\otimes W^2 ).  </li> <li>Continue until ( W^{2^{\\lceil \\log(n-1) \\rceil}} ).  </li> <li>This algorithm has a time complexity of ( O(n^3 \\log n) ), which is much more efficient than naive approaches for large graphs.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#example_1","title":"Example","text":"<p>For a given weight matrix W, repeated squaring allows us to quickly discover shortest paths between all pairs of nodes, even in very large networks.</p> <p>ag_4, ag_5 y ag_6</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#random-walks-and-markov-chains","title":"Random Walks and Markov Chains","text":"<p>Sometimes, we are not interested in deterministic paths but in probabilistic movement across a network. This is where random walks and Markov chains come into play, both of which can be described using matrix multiplication.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#explanation_2","title":"Explanation","text":"<ul> <li>A random walk is a process where, at each step, we move to a random neighbor of the current node.  </li> <li>This can be represented by a transition matrix T, where each entry ( T_{ij} ) gives the probability of moving from node i to node j.  </li> <li>By computing ( T^n ), we find the probabilities of being in each node after n steps.  </li> <li>Over time, this leads to a stationary distribution, which tells us the long-term behavior of the system.</li> </ul> <p>This framework is fundamental in PageRank, recommendation systems, and modeling diffusion in social and biological networks.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#example_2","title":"Example","text":"<p>Consider a graph with nodes A\u2013F. The transition matrix T encodes the probability of moving between nodes. - If we start at node A and compute ( T^n ) for a large n, we obtain a vector that represents the probability of being at each node after many steps.</p> <p>ag_7</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#case-study-casino-random-walk","title":"Case Study: Casino Random Walk","text":"<p>To better understand random walks, let\u2019s look at a simple but illustrative example: the casino.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#explanation_3","title":"Explanation","text":"<ul> <li>Imagine a player who always bets $1.  </li> <li>The probability of winning or losing is equal.  </li> <li>This can be modeled as a random walk on a line graph, where each node represents the player\u2019s wealth at a given time.  </li> <li>As the game evolves, the player\u2019s wealth moves left (loss) or right (win) along the line.  </li> </ul> <p>This model allows us to study important aspects of the game, such as the probability distribution of the player\u2019s wealth after a certain number of bets, or the risk of eventual ruin.</p> <p>ag_8</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/","title":"Exercises on Big-O Notation","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-1-mapping-the-complexity-landscape","title":"Variant 1 \u2014 Mapping the Complexity Landscape","text":"<p>Goal. Empirically identify the time complexity of several algorithms using synthetic datasets of increasing size, and contrast it with the theoretical complexity.</p> <p>Tasks. 1. Choose three different complexity classes (O(1), O(log n), O(n), O(n log n), etc.). 2. Measure execution times for increasing input sizes using <code>System.nanoTime()</code>. 3. Plot results and normalize (<code>time/n</code>, <code>time/n log n</code>, etc.) to recognize the class. 4. Write a report for each algorithm with hypothesis, evidence, and discussion.</p> <p>Deliverables. - Report (4\u20136 pages) with plots and analysis. - Code with measurement scripts.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-2-scaling-limits-in-practice","title":"Variant 2 \u2014 Scaling Limits in Practice","text":"<p>Goal. Explore the practical limits of different algorithmic complexities by running experiments until execution times become impractical, and compare results to theoretical expectations.</p> <p>Tasks. 1. Select three complexity classes (e.g., O(n), O(n log n), O(n\u00b2)). 2. Define a \u201ctime budget\u201d (e.g., 5 seconds per experiment). 3. Increase input size <code>n</code> step by step until execution exceeds the budget. 4. Record the maximum feasible <code>n</code> for each algorithm and compare with theoretical growth. 5. Discuss discrepancies between theoretical predictions and practical limits (e.g., due to JVM, caching, or hardware).  </p> <p>Deliverables. - Report (3\u20135 pages) with tables of <code>n_max</code> values and discussion. - Code with input scaling scripts.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-3-comparing-growth-rates-on-the-same-plot","title":"Variant 3 \u2014 Comparing Growth Rates on the Same Plot","text":"<p>Goal. Visually demonstrate differences in algorithmic growth rates by plotting multiple complexity classes together for the same input range.  </p> <p>Tasks. 1. Select at least four algorithms with distinct complexities (e.g., O(1), O(log n), O(n), O(n\u00b2)). 2. Measure execution times for the same sequence of input sizes. 3. Plot all results on the same chart (possibly log-log scale for clarity). 4. Highlight where one complexity \u201covertakes\u201d another (e.g., O(n\u00b2) becomes much slower than O(n log n)). 5. Provide an interpretation that connects plots to theoretical curves.  </p> <p>Deliverables. - Report (4\u20136 pages) with combined plots and written interpretation. - Annotated code to reproduce graphs.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#common-notes","title":"Common Notes","text":"<ul> <li>Use definitions of Big O notation, best/worst/average cases.</li> <li>Always explain what your analysis counts (iterations, comparisons, assignments).</li> <li>Differentiate time and space when relevant.</li> <li>(Optional) Use JMH for micro-benchmarks in Java.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/","title":"Example: Adjacency Matrix Multiplication in a Biological Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#scenario","title":"Scenario","text":"<p>Consider a biological network where nodes represent genes (or proteins) and edges represent regulatory or interaction relationships. We have four genes: A, B, C, and D, with the following known interactions:</p> <ul> <li>A \u2192 B: Gene A activates gene B  </li> <li>B \u2192 C: Gene B activates gene C  </li> <li>C \u2192 D: Gene C activates gene D  </li> <li>A \u2192 D: Gene A also activates gene D directly  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 0 C 0 0 0 1 D 0 0 0 0 <p>Each row represents the regulating gene (source) and each column represents the regulated gene (target). A value of 1 means there is a direct regulatory interaction between two genes.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The matrix A\u00b2 shows indirect (two-step) interactions \u2014 which genes influence others through an intermediate regulator.</p> A B C D A 0 0 1 0 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 Gene A indirectly influences C through B (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 Gene B indirectly influences D through C (B \u2192 C \u2192 D).  </li> <li>A\u00b2\u208dA,D\u208e = 0 \u2192 Although A directly regulates D, there is no distinct two-step pathway (already covered in A).  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#biological-insights","title":"Biological Insights","text":"<ul> <li>Indirect or cascade regulation: Detect genes that affect others through intermediate regulators (e.g., signaling pathways).  </li> <li>Signal propagation: Analyze how activation of one gene can spread through the network over multiple steps.  </li> <li>Hub genes: Identify genes with high connectivity in A or A\u00b2 \u2014 potential master regulators or essential genes.  </li> <li>Network redundancy: If A\u00b2\u208di,j\u208e &gt; 1, multiple regulatory paths exist between i and j, suggesting robustness in gene regulation.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#extensions","title":"Extensions","text":"<ul> <li>A\u00b3 \u2192 shows interactions involving three steps (A \u2192 B \u2192 C \u2192 D).  </li> <li>(I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 total reachability or accumulated influence of a gene across the network.  </li> <li>Weighted adjacency matrices \u2192 edge weights can represent interaction strength, affinity, or probability of binding.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/","title":"Ejemplos","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#velocidad-de-ejecucion","title":"Velocidad de ejecuci\u00f3n","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#python","title":"Python","text":"<pre><code>import random\nfrom time import *\n\nn = 1024\n\nA = [[random.random() for _ in range(n)] for _ in range(n)]\nB = [[random.random() for _ in range(n)] for _ in range(n)]\nC = [[0 for _ in range(n)] for _ in range(n)]\n\nstart = time()\nfor i in range(n):\n    for j in range(n):\n        for k in range(n):\n            C[i][j] += A[i][k] * B[k][j]\n\nend = time()\n\nprint(\"%.6f\" % (end - start))\n\n# Python. Runing time aroud: 409 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#java","title":"Java","text":"<pre><code>import java.util.Random;\n\npublic class Matrix {\n\n    static int n = 1024;\n    static double[][] a = new double[n][n];\n    static double[][] b = new double[n][n];\n    static double[][] c = new double[n][n];\n\n    public static void main(String[] args) {\n        Random random = new Random();\n        for (int i = 0; i &lt; n; i++) {\n            for (int j = 0; j &lt; n; j++) {\n                a[i][j] = random.nextDouble();\n                b[i][j] = random.nextDouble();\n                c[i][j] = 0;\n            }\n        }\n\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; n; i++) {\n            for (int j = 0; j &lt; n; j++) {\n                for (int k = 0; k &lt; n; k++) {\n                    c[i][j] += a[i][k] * b[k][j];\n                }\n            }\n        }\n        long stop = System.currentTimeMillis();\n\n        System.out.println((stop - start) * 1e-3);\n    }\n}\n\n// Java. Runing time aroud: 7.76 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#rust","title":"Rust","text":"<pre><code>use std::time::SystemTime;\nuse rand::Rng;\n\nfn main() {\n    let n: usize = 1024;\n    let mut a: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n    let mut b: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n    let mut c: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n\n    let mut rng = rand::thread_rng();\n    for i in 0..n {\n        for j in 0..n {\n            a[i][j] = rng.gen::&lt;f64&gt;();\n            b[i][j] = rng.gen::&lt;f64&gt;();\n        }\n    }\n\n    let start: SystemTime = SystemTime::now();\n    for i in 0..n {\n        for j in 0..n {\n            for k in 0..n {\n                c[i][j] += a[i][k] * b[k][j];\n            }\n        }\n    }\n    let elapsed: Result&lt;std::time::Duration, std::time::SystemTimeError&gt; = start.elapsed();\n\n    println!(\"Elapsed: {:.2?}\", elapsed);\n}\n\n// Rust. Runing time aroud: 7.91 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#c","title":"C","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;sys/time.h&gt;\n\n#define n 1024\ndouble a[n][n];\ndouble b[n][n];\ndouble c[n][n];\n\nstruct timeval start, stop;\n\nint main() {\n    for (int i = 0; i &lt; n; ++i) {\n        for (int j = 0; j &lt; n; ++j) {\n            a[i][j] = (double) rand() / RAND_MAX;\n            b[i][j] = (double) rand() / RAND_MAX;\n            c[i][j] = 0;\n        }\n    }\n\n    gettimeofday(&amp;start, NULL);\n    for (int i = 0; i &lt; n; ++i) {\n        for (int j = 0; j &lt; n; ++j) {\n            for (int k = 0; k &lt; n; ++k) {\n                c[i][j] += a[i][k] * b[k][j];\n            }\n        }\n    }\n    gettimeofday(&amp;stop, NULL);\n\n    double diff = stop.tv_sec - start.tv_sec\n                + 1e-6 * (stop.tv_usec - start.tv_usec);\n    printf(\"%0.6f\\n\", diff);\n\n    return 0;\n}\n/* C. Runing time aroud: 0.677867 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/","title":"Example: Adjacency Matrix Multiplication in a Computer Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#scenario","title":"Scenario","text":"<p>Consider a network of four devices: A (host), B (switch), C (router-1), and D (router-2). The edges are directed communication links (row = source, column = destination).</p> <p>Direct links: - A \u2192 B - B \u2192 C, B \u2192 D - C \u2192 D - D \u2192 (no outgoing links)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 0 B 0 0 1 1 C 0 0 0 1 D 0 0 0 0 <p>Each row represents the source device, and each column represents the destination device. A value of 1 indicates a direct communication link between two devices.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The matrix A\u00b2 shows how many two-hop communication paths exist between devices.</p> A B C D A 0 0 1 1 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 Device A can reach C in two hops (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dA,D\u208e = 1 \u2192 Device A can reach D in two hops (A \u2192 B \u2192 D).  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 Device B can reach D in two hops (B \u2192 C \u2192 D).  </li> <li>Rows with all zeros (like D) indicate sink nodes \u2014 devices that do not forward traffic.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#possible-analyses","title":"Possible Analyses","text":"<ul> <li>Two-hop reachability: Identify which devices are reachable within two transmissions (useful for TTL, latency estimation, or broadcast domain boundaries).  </li> <li>Critical relay nodes: Columns with nonzero entries (e.g., C and D) reveal which devices act as communication relays \u2014 their failure could break the network.  </li> <li>Routing and segmentation: Determine which destinations require multiple hops to optimize routing or add redundancy.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#extensions","title":"Extensions","text":"<ul> <li>A\u00b3 \u2192 identifies devices reachable in three hops (additional relays).  </li> <li>(I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 cumulative reachability up to k hops.  </li> <li>Weighted adjacency matrices \u2192 represent bandwidth, loss, or latency to find the most efficient routes.  </li> <li>Redundancy analysis \u2192 if A\u00b2\u208di,j\u208e &gt; 1, multiple two-hop paths exist between i and j.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/","title":"Experiments \u2013 Complexity Management","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#introduction","title":"Introduction","text":"<p>\u201cThere are a lot of ways known to do it wrong and which one is right is not clear.\u201d \u2014 James Gosling (1955\u2013)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#brief-biography","title":"Brief Biography","text":"<p>James Gosling, born on May 19, 1955, in Calgary, Canada, is a computer scientist widely known as the \u201cfather of Java.\u201d He studied computer science at the University of Calgary and later earned a Ph.D. in computer science from Carnegie Mellon University. Gosling joined Sun Microsystems in the 1980s, where he led the development of the Java programming language, officially released in 1995. Java quickly became one of the most influential programming languages in the world due to its portability, security, and \u201cwrite once, run anywhere\u201d philosophy. Beyond Java, Gosling has contributed to compiler design, operating systems, and software development tools. He has worked at several leading technology companies, including Sun Microsystems, Oracle, Google, and Amazon Web Services.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#the-creation-of-java","title":"The Creation of Java","text":"<p>In the early 1990s, Gosling and his team at Sun Microsystems initiated the \u201cGreen Project\u201d to develop a language suitable for consumer devices and embedded systems. Gosling designed Java with a strong emphasis on simplicity, object-oriented design, memory management, and platform independence. The language introduced features like automatic garbage collection and a virtual machine (the JVM) that allowed programs to run across different hardware and operating systems. Java\u2019s release in 1995 revolutionized programming by becoming the backbone of enterprise applications, web development, and later Android applications. Today, Java remains one of the most widely used and enduring programming languages, a testament to Gosling\u2019s vision and leadership.</p> <p>Complexity in computation is not only about algorithms but also about implementation choices, programming languages, and hardware utilization. This experiment explores matrix multiplication as a case study to analyze performance differences across programming languages and approaches.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#reminder-matrix-multiplication","title":"Reminder: Matrix Multiplication","text":"<p>If A and B are <code>n \u00d7 n</code> matrices, their product C = AB is also an <code>n \u00d7 n</code> matrix. Each element of the resulting matrix is obtained by combining a row of A with a column of B.</p> <p>Formally:</p> <p></p> <p>[ C[i,j] = \\sum_{k=1}^n A[i,k] \\cdot B[k,j] ]</p> <p>This operation is fundamental in scientific computing, graphics, and machine learning, but it is also computationally intensive, requiring O(n\u00b3) operations in its na\u00efve form.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#algorithm-naive-approach","title":"Algorithm (Na\u00efve Approach)","text":"<p>Pseudocode of the classic algorithm:</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#python","title":"Python","text":"<p>Python Example</p> <p></p> <p>Running time 409.45 seconds with 1024 x 1024 matrices</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#rust","title":"Rust","text":"<p>Rust Example</p> <p></p> <p>Running time 7.91 second with 1024 x 1024 matrices</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#java","title":"Java","text":"<p>Java Example</p> <p></p> <p>Running time 7.76 seconds with 1024 x 1024 matrices 52x faster than python</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#c","title":"C","text":"<p>C Example</p> <p></p> <p>Running time 0.677867 seconds with 1024 x 1024 matrices</p> <p>11x faster than java</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#tiobe-programming-community-index","title":"TIOBE Programming Community Index","text":"<p>The image represents the TIOBE Programming Community Index, which measures the relative popularity of programming languages over time. It does not indicate the best language but rather the most used and most visible in the global developer community, based on search engines, courses, and vendors.</p> \ud83d\udca1 Details <p>Java (green): Dominated the early 2000s with more than 25% share but has steadily declined, although it remains highly relevant.</p> <p>C (black): Extremely stable and consistently strong, often alternating with Java in the top positions.</p> <p>Python (light blue): Shows explosive growth after 2015, becoming the most popular language since 2020. This reflects the rise of data science, machine learning, and artificial intelligence.</p> <p>C++ (orange): Popular in the early 2000s, now stabilised at around 8\u201310%.</p> <p>C# (dark blue): Grew quickly with the .NET ecosystem in the 2000s and maintains a solid mid-level share.</p> <p>PHP (aqua): Very popular in web development between 2005\u20132010 but declined as JavaScript frameworks and other technologies took over.</p> <p>JavaScript (yellow): Maintains a stable share, though its dominance in web applications is not fully reflected in TIOBE\u2019s methodology.</p> <p>Other languages (SQL, Assembly, Visual Basic, etc.): Remain present in niche applications.</p> \ud83d\udca1 Conclusions <p>C and Java were the long-time leaders of the programming world.</p> <p>Python\u2019s meteoric rise illustrates how industry trends (AI, data analytics) can change the landscape of programming.</p> <p>The index shows that language popularity evolves with technological needs, and students should be aware of both long-standing and emerging languages.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#exercise-1-complexity-experiment","title":"EXERCISE 1 Complexity Experiment","text":"<p>Given the matrix multiplication algorithm, how would you optimize the storage and management of the input data to improve the efficiency of the computation? Consider both memory access patterns and the use of specialized data structures.</p> <p>Solution</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#benchmarking-in-complexity-management","title":"Benchmarking in Complexity Management","text":"<p>Benchmarking is the methodology of comparing processes with respect to specific performance measures such as execution time, memory usage, throughput, or scalability.  </p> <p>It allows us to: - Evaluate performance under different conditions. - Compare technologies and frameworks. - Identify bottlenecks in computation. - Optimize resources. - Ensure scalability as systems and data grow.  </p> <p>The benchmarking process generally follows three key steps:</p> <ol> <li>Setup the experiment: Define datasets, algorithms, and parameters.  </li> <li>Execution: Run tests under controlled conditions.  </li> <li>Analysis: Interpret results, compare metrics, and extract insights.  </li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#case-study-examples","title":"Case Study Examples","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#1-performance-evaluation","title":"1. Performance Evaluation","text":"<p>Imagine a company processing large datasets in real time for fraud detection. Using a framework such as Apache Spark, benchmarking helps measure how many transactions per second the system can handle before latency increases. This determines the optimal configuration to maintain performance while minimizing delays.</p> \ud83d\udca1 Proposal <p>Here\u2019s a real-world case where benchmarking helps: fraud detection with Spark.</p> <p>If the system starts to show latency at 50,000 transactions per second, what changes could you propose to improve performance?</p> \ud83d\udca1 Reflexion <p>If the system shows latency at 50,000 transactions per second, you could improve performance by:</p> <p> Scaling horizontally \u2192 add more nodes or executors to increase parallelism.</p> <p> Optimizing resources \u2192 tune memory and CPU allocation.</p> <p> Partitioning the data stream \u2192 add more Kafka/Spark partitions for better distribution.</p> <p> Reducing state size \u2192 use watermarks or windowing to avoid unbounded memory growth.</p> <p> Optimizing the code \u2192 avoid unnecessary shuffles and use efficient libraries.</p> <p>In summary, the goal is to increase parallelism, make better use of resources, and control state growth, so the system can process more transactions without adding latency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#2-technology-comparison","title":"2. Technology Comparison","text":"<p>A research team compares Apache Hadoop and Apache Flink for batch processing. - Hadoop: More efficient for massive, distributed storage and processing. - Flink: Better for real-time analytics. Benchmarking on workloads like social media data helps them select the best tool.</p> \ud83d\udca1 Proposal <p>This is an example of how benchmarking guides technology choice: Hadoop vs Flink.</p> <p>If you had to process social media streams, which system would you benchmark and why?</p> \ud83d\udca1 Reflexion If I had to process social media streams, I would benchmark Apache Flink because it is optimized for real-time and low-latency stream processing. Hadoop is more efficient for large-scale batch processing, but social media data requires continuous analysis, so Flink would likely perform better for this use case."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#3-resource-optimization","title":"3. Resource Optimization","text":"<p>In cloud environments, resources are billed by usage. Benchmarking reveals how different CPU/memory configurations affect the runtime of machine learning tasks. For example, doubling memory but keeping CPU constant may cut processing time by half \u2014 optimizing cost-performance balance.</p> \ud83d\udca1 Proposal <p>This is how benchmarking can guide resource allocation in cloud environments.</p> <p>If doubling memory halves the processing time, would you consider it cost-effective even if memory is twice as expensive as CPU?</p> \ud83d\udca1 Reflexion In order to process social media streams, maybe benchmark Apache Flink is a good choice because it is optimized for real-time and low-latency stream processing. Hadoop is more efficient for large-scale batch processing, but social media data requires continuous analysis, so Flink would likely perform better for this use case."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#4-identifying-bottlenecks","title":"4. Identifying Bottlenecks","text":"<p>In ETL (Extract, Transform, Load) workflows, benchmarking can reveal network transfer speed as the bottleneck between Amazon S3 and Hadoop clusters. This insight guides investment in better infrastructure or alternative transfer methods.</p> \ud83d\udca1 Proposal <p>Benchmarking can reveal whether the bottleneck lies in the network rather than in storage or compute.</p> <p>If your ETL pipeline is slow, how would you use benchmarking to determine whether the problem is CPU, storage, or network?</p> \ud83d\udca1 Reflexion To identify bottlenecks in an ETL pipeline, I would run benchmarks that measure the performance of each stage separately\u2014data transfer, storage, and processing. If the results show that network transfer between storage (e.g., S3) and the processing engine (e.g., Hadoop) is significantly slower than computation or storage access, then the network is the bottleneck. This insight helps decide whether to improve network capacity or adjust data transfer methods."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#5-ensuring-scalability","title":"5. Ensuring Scalability","text":"<p>An online retailer analyzing customer behavior starts with 1M transactions but expects exponential growth. Scalability benchmarks test workloads of 10M, 50M, or 100M transactions to ensure the system scales without performance degradation.</p> \ud83d\udca1 Proposal <p>Scalability benchmarking helps companies anticipate growth and ensure that performance remains stable as data volume increases.</p> <p>If your dataset grew 100 times larger, how would you benchmark your system to check whether it can scale without performance degradation?</p> \ud83d\udca1 Reflexion To benchmark scalability, the dataset size can be gradually increased (e.g., 10x, 50x, 100x) while measuring how the system\u2019s performance changes. If the system maintains acceptable response times and throughput as the data grows, then it scales effectively. Otherwise, the benchmark highlights where improvements in infrastructure or algorithms are required."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#exercise-2-complexity-experiment","title":"EXERCISE 2 Complexity Experiment","text":"<p>Execute the algorithms associated to different methods or programming languages with different datasets (size of the matrix) and extract measures (execution time).</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#optimizing-matrix-multiplication","title":"Optimizing Matrix Multiplication","text":"<p>Students are expected to: 1. Implement algorithms for matrix multiplication (na\u00efve and optimized). 2. Execute them in different languages or frameworks (e.g., Python, Java, C, Rust). 3. Vary dataset size (matrix dimension) and record metrics (execution time, memory usage). 4. Compare results across methods. 5. Draw conclusions about efficiency, scalability, and hardware utilization.</p> <p></p> <p>Comparison of common time complexities (Big-O). The graph shows how algorithm performance scales with input size, from constant time O(1) to exponential O(2^n), highlighting the dramatic differences in growth rates.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#documenting-benchmarks","title":"Documenting Benchmarks","text":"<p>When reporting experiments, follow a structured format:</p> <p>Title - Clear and concise, e.g. Benchmarking Matrix Multiplication Algorithms on Big Data Platforms.  </p> <p>Abstract - Briefly state the task, algorithms compared, performance measures, and conclusions.  </p> <p>Body - Describe setup, methodology, datasets, and tools. - Provide tables or charts (execution times, speedups, scalability curves).  </p> <p>Conclusions - Summarize key findings. - State recommendations for future research or practice.  </p> <p>Example Abstract: We study the behavior of several matrix multiplication algorithms used for large-scale computation. We analyze execution time, memory usage, and scalability. Our experiments provide a reproducible benchmark across a variety of datasets to guide future research in Big Data performance engineering. Based on our results, we recommend combining optimized libraries (BLAS, MKL) with GPU acceleration for the most efficient solutions.</p> <p>Scientific Papers</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/","title":"Exercises on Graphs in Big Data Context","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-1-exploring-graph-representations","title":"Variant 1 \u2014 Exploring Graph Representations","text":"<p>Goal. Compare adjacency matrix and adjacency list representations in terms of memory usage and query efficiency.  </p> <p>Tasks. 1. Build a synthetic social network graph with different densities (sparse vs dense). 2. Store the graph both as an adjacency list and as an adjacency matrix. 3. Measure memory consumption and query time for operations like:    - Checking if two nodes are connected.    - Finding neighbors of a node. 4. Plot results as graph size increases.  </p> <p>Deliverables. - Report (4\u20136 pages) with plots and analysis. - Code that builds both representations and measures performance.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-2-degrees-of-separation-in-social-networks","title":"Variant 2 \u2014 Degrees of Separation in Social Networks","text":"<p>Goal. Empirically measure how the average path length grows in social network graphs, and relate it to the \u201csmall-world\u201d phenomenon.  </p> <p>Tasks. 1. Generate synthetic social networks with increasing numbers of nodes (Erd\u0151s\u2013R\u00e9nyi and Barab\u00e1si\u2013Albert models). 2. Compute the average shortest path length using BFS or matrix multiplications. 3. Plot how path length grows with graph size for each model. 4. Discuss implications for real-world social networks.  </p> <p>Deliverables. - Report (3\u20135 pages) with plots and interpretation. - Code to generate graphs and compute path lengths.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-3-shortest-path-performance-in-large-graphs","title":"Variant 3 \u2014 Shortest Path Performance in Large Graphs","text":"<p>Goal. Empirically compare Dijkstra\u2019s algorithm and BFS (for unweighted graphs) in terms of runtime and scalability.  </p> <p>Tasks. 1. Generate random graphs with varying sizes and edge weights. 2. Run BFS for unweighted graphs and Dijkstra for weighted graphs. 3. Measure execution time and number of operations as input size grows. 4. Plot results and analyze when Dijkstra\u2019s overhead becomes significant.  </p> <p>Deliverables. - Report (4\u20136 pages) with analysis and plots. - Annotated code to reproduce experiments.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-4-ranking-web-pages-with-pagerank","title":"Variant 4 \u2014 Ranking Web Pages with PageRank","text":"<p>Goal. Explore how PageRank values evolve with graph size and structure.  </p> <p>Tasks. 1. Generate directed graphs simulating web pages and hyperlinks. 2. Implement the PageRank algorithm using iterative matrix multiplications. 3. Measure convergence speed (iterations until stable) as graph size increases. 4. Compare rankings across different graph topologies (chain, star, random).  </p> <p>Deliverables. - Report (4\u20136 pages) with convergence plots and discussion. - Code implementing PageRank.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-5-task-scheduling-and-dependency-graphs","title":"Variant 5 \u2014 Task Scheduling and Dependency Graphs","text":"<p>Goal. Analyze project scheduling using Directed Acyclic Graphs (DAGs).  </p> <p>Tasks. 1. Generate random DAGs representing task dependencies. 2. Implement topological sorting to find valid execution orders. 3. Measure execution time and memory usage as graph size increases. 4. Extend the experiment by adding random cycles to test cycle detection.  </p> <p>Deliverables. - Report (4\u20136 pages) with examples and analysis. - Code for DAG generation, sorting, and cycle detection.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#common-notes","title":"Common Notes","text":"<ul> <li>Clearly distinguish between time complexity (theoretical) and observed runtime.  </li> <li>Vary graph density to show how sparse vs dense cases differ.  </li> <li>Optionally use existing graph libraries (e.g., NetworkX, SNAP, GraphX) to scale experiments.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/","title":"Example: Adjacency Matrix Multiplication in a Knowledge Graph","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#scenario","title":"Scenario","text":"<p>Consider a small knowledge graph where nodes represent entities and edges represent semantic relationships.</p> <p>Entities: - A: \"Albert Einstein\" - B: \"Physics\" - C: \"Nobel Prize\" - D: \"Switzerland\"</p> <p>Relationships: - A \u2192 B: Einstein is associated with Physics - A \u2192 D: Einstein lived in Switzerland - B \u2192 C: Physics is related to the Nobel Prize - D \u2192 C: Switzerland hosts Nobel Prize institutions  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 0 C 0 0 0 0 D 0 0 1 0 <p>Each row represents the source entity, and each column represents the target entity. A value of 1 means there is a direct semantic relationship between the entities.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The result A\u00b2 represents indirect (two-step) relationships, i.e., entities connected through an intermediate node.</p> A B C D A 0 0 2 0 B 0 0 0 0 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 2 \u2192   Albert Einstein (A) is semantically connected to the Nobel Prize (C) through two distinct paths:</li> <li>A \u2192 B \u2192 C (Einstein \u2192 Physics \u2192 Nobel Prize)  </li> <li>A \u2192 D \u2192 C (Einstein \u2192 Switzerland \u2192 Nobel Prize)</li> </ul> <p>This indicates a reinforced semantic relationship: multiple paths in the graph link Einstein to the Nobel Prize.</p> <ul> <li>All other entries are 0 \u2192 no indirect two-step relationships exist among the other entities.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#insights-and-applications","title":"Insights and Applications","text":"<ul> <li>Implicit relationship inference: Identify hidden or indirect semantic links (e.g., inferring that \"Einstein is related to the Nobel Prize\" even if not explicitly stated).  </li> <li>Knowledge discovery: Find entities that share common semantic intermediaries (useful in semantic search and reasoning systems).  </li> <li>Semantic strength analysis: Higher values in A\u00b2 or A\u00b3 indicate multiple reinforcing semantic paths between entities.  </li> <li>Query expansion: Use A\u00b2 or A\u00b3 to retrieve contextually related entities in knowledge retrieval systems.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#extensions","title":"Extensions","text":"<ul> <li>A\u00b3 \u2192 captures relationships through three-step paths (longer semantic chains).  </li> <li>(I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 measures total semantic connectivity for an entity.  </li> <li>Weighted adjacency matrices \u2192 edges can represent relationship confidence, semantic strength, or co-occurrence frequency.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/","title":"Lectures","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#introduction","title":"Introduction","text":"<p>Programmers are always surrounded by complexity; we cannot avoid it. \u2014 Tony Hoare (1934\u2013)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#brief-biography","title":"Brief Biography","text":"<p>Charles Antony Richard Hoare, known as Tony Hoare, was born in Colombo (Sri Lanka, then Ceylon) in 1934. Originally studying Classics at Oxford, he later turned to mathematics and computer science, becoming one of the most influential figures in programming languages and algorithms. In 1960, he created Quicksort, one of the most widely used and efficient sorting algorithms. Hoare worked on compiler design, programming language theory, and software verification. He was a professor at the University of Oxford and later a principal researcher at Microsoft Research. In recognition of his contributions, he received the ACM Turing Award in 1980.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#null-references-the-billion-dollar-mistake","title":"Null References \u2013 The \"Billion-Dollar Mistake\"","text":"<p>In 1965, while designing the programming language ALGOL W, Tony Hoare introduced the concept of null references (or null pointers) as a way to indicate the absence of a value in reference variables. Decades later, he admitted this design decision was a serious error, calling it his \"billion-dollar mistake\", since null references have caused countless errors, system crashes, vulnerabilities, and financial losses across the software industry. Hoare discussed this in a 2009 talk at QCon London, reflecting on the immense cost of null-related bugs. Modern programming languages such as Kotlin, Swift, and Rust have since introduced explicit type systems that differentiate between nullable and non-nullable values, reducing the risk of null pointer exceptions and improving software reliability.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o","title":"BiG O","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o_1","title":"Big O","text":"<p>The order of complexity in algorithms is a way to measure an algorithm's efficiency in terms of time and/or space as the input size grows. This measure is expressed using Big O notation, which provides a way to describe the asymptotic behaviour of the algorithm, meaning how it behaves when the input becomes very large.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#types-of-complexity","title":"Types of Complexity","text":"<ul> <li>Time Complexity: Measures the execution time of the algorithm based on the input size n. It focuses on how many operations the algorithm performs as the input size increases.  </li> <li>Space Complexity: Measures the amount of memory an algorithm needs as a function of the input size n.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o-notation","title":"Big O Notation","text":"<p>Big O notation describes the worst-case scenario, representing the upper limit of time or space required as n grows. </p> <p></p>  Details <p>Also called Landau's symbol, is a symbolism used in complexity theory, computer science, and mathematics to describe the asymptotic behavior of functions.</p> <p>German mathematician Edmund Landau, who was one of the first to use this type of notation in mathematical analysis, to describe the behaviour of mathematical functions when they tend to infinity.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o1","title":"O(1)","text":"<p>O(1): Constant. The execution time does not change with the size of the input. Example: Print a message. </p> <p></p>  Details <p>Constant time complexity (O(1)) because the execution time of the System.out.println statement is independent of the input size.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o1_1","title":"O(1)","text":"<p>O(1): Constant. Why is it \u201cprint a message\u201d O(1)? Because the execution time of the <code>System.out.println</code> statement is independent of the input size.  </p> <p></p>  Details <p>Constant time complexity (O(1)) because the execution time of the System.out.println statement is independent of the input size.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#olog-n","title":"O(log n)","text":"<p>O(log n): Logarithmic. The execution time grows logarithmically as the input size increases. Example: Binary search. </p> <p></p>  Details <p>For an array of size n = 1,000,000, binary search will take roughly log\u20612(1,000,000)\u224820 comparisons to find the target.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#olog-n_1","title":"O(log n)","text":"<p>O(log n): Logarithmic. Why is \u201cbinary search\u201d O(log n)? Because at each step, it halves the search space. So, if you double the size of the array, the number of steps required to find an element increases only by 1 (since log\u2082(2n) = log\u2082(n) + 1).  </p> <p></p>  Details <p>Logarithmic Growth: Binary search is logarithmic because at each step, it halves the search space. So if you double the size of the array, the number of steps required to find an element increases only by 1 (since log\u20612(2n)=log\u20612(n)+1\\log_2(2n) = \\log_2(n) + 1log2\u200b(2n)=log2\u200b(n)+1).</p> <p>For example:</p> <p> - For an array of size 1,000,000, binary search requires about 20 comparisons.</p> <p> - For an array of size 2,000,000, it requires 21 comparisons.</p> <p> - Even though the input size doubled, the time to search increased by only a single step, which is characteristic of logarithmic growth.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on","title":"O(n)","text":"<p>O(n): Linear. The execution time grows proportionally with the input size. Example: Iterating through an array.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_1","title":"O(n)","text":"<p>O(n): Linear. Why is \u201citerating through an array\u201d O(n)? The for loop iterates exactly n times. The number of operations (iterations) grows linearly with the input size. If n = 1,000,000 (or 2,000,000) the loop runs 1,000,000 (2,000,000) times.  </p> <p></p>  Details <p> Why is This O(n) Linear Time Complexity?</p> <p>In this example, the for loop runs exactly n times, which means the number of iterations grows linearly with the input size n.</p> <p>For example:</p> <p> - If n = 10, the loop runs 10 times.</p> <p> - If n = 1000, the loop runs 1000 times.</p> <p> - If n = 1,000,000, the loop runs 1,000,000 times.</p> <p>O(n) means that the execution time of the algorithm is proportional to the size of the input. If you double the size of the input (n), the time to complete the loop will approximately double as well.</p> <p>So, if it takes 1 second to run 1,000,000 iterations, it will take roughly 2 seconds to run 2,000,000 iterations (assuming constant factors like hardware performance stay the same).</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on-log-n","title":"O(n log n)","text":"<p>O(n log n): Linear-logarithmic. Common in sorting algorithms like Merge Sort and Heap Sort. Example: Sorting (Merge Sort).  </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on-log-n_1","title":"O(n log n)","text":"<p>O(n log n): Linear-logarithmic. Why is sorting (merge sort) O(n log n)?  </p> <p></p>  Details <p> Why is this O(n log n)?</p> <p>Sorting an array of n elements using efficient algorithms like Merge Sort or Timsort takes O(n log n) time:</p> <p> - The array is divided into smaller subarrays recursively (halving the array size at each step), which takes logarithmic time O(log\u2061n)O(\\log n)O(logn).</p> <p> - Merging the sorted subarrays takes linear time O(n)O(n)O(n) because every element needs to be checked at least once.</p> <p> - Therefore, the overall time complexity is O(n \\log n).</p> <p>Example:</p> <p>For an array of size 100,000, the time complexity would be proportional to:</p> <p> - n=100,000n = 100,000n=100,000.</p> <p> - log\u2061n=log\u20612100,000\u224816.61\\log n = \\log_2 100,000 \\approx 16.61logn=log2\u200b100,000\u224816.61</p> <p> - So, the number of operations would be proportional to 100,000\u00d716.61\u22481,661,000100,000 \\times 16.61 \\approx 1,661,000100,000\u00d716.61\u22481,661,000 operations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on2","title":"O(n\u00b2)","text":"<p>O(n\u00b2): Quadratic. The execution time is proportional to the square of the input size (e.g., sorting algorithms like Bubble Sort or Selection Sort). Example: Bubble sort. </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on2_1","title":"O(n\u00b2)","text":"<p>O(n\u00b2): Quadratic. Why is Bubble Sort O(n\u00b2)?  </p> <p></p>  Details <p> Why is Bubble sort O(n\u00b2)?</p> <p>The Bubble Sort algorithm has two nested loops: one that iterates over the array and another that iterates over the unsorted part of the array to compare and swap adjacent elements.</p> <p> The outer loop runs n times, and for each iteration of the outer loop, the inner loop also runs n times. This leads to a total of n\u00d7n=n\u00b2 operations. In simple terms, as the input size n grows, the time it takes to sort the array grows quadratically. </p> <p>For example:</p> <p> - If n = 1000, the algorithm performs approximately 1000\u00d71000=1,000,0001000 \\times 1000 = 1,000,0001000\u00d71000=1,000,000 operations.</p> <p> If n = 2000, the algorithm performs approximately 2000\u00d72000=4,000,0002000 \\times 2000 = 4,000,0002000\u00d72000=4,000,000 operations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on3","title":"O(n\u00b3)","text":"<p>O(n\u00b3): Polynomial. The runtime of the algorithm increases cubically as the input size grows. Example: Matrix Multiplication (Triple Nested Loops).</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on3_1","title":"O(n\u00b3)","text":"<p>O(n\u00b3): Polynomial. Why is Matrix Multiplication O(n\u00b3)? </p> <p></p>  Details <p> Why is This O(n\u00b3)?</p> <p>Triple Nested Loops: The three nested loops each iterate n times:</p> <p> - The outer loop iterates over the rows of matrix AAA. </p> <p> - The middle loop iterates over the columns of matrix BBB.</p> <p> - The inner loop computes the dot product for each element of the result matrix.</p> <p>The total number of iterations is n\u00d7n\u00d7n=n^3, which is why the algorithm has O(n\u00b3) time complexity.</p> <p>Example:</p> <p> - If n = 100, the algorithm performs approximately 100\u00d7100\u00d7100=1,000,000100 \\times 100 \\times 100 = 1,000,000100\u00d7100\u00d7100=1,000,000 operations.</p> <p> - If n = 200, the algorithm performs approximately 200\u00d7200\u00d7200=8,000,000200 \\times 200 \\times 200 = 8,000,000200\u00d7200\u00d7200=8,000,000 operations.</p> <p> As n increases, the number of operations grows very quickly, which reflects the cubic nature of the time complexity.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o2n","title":"O(2\u207f)","text":"<p>O(2\u207f): Exponential. The execution time doubles with each increase in the input size, common in brute force problems. Example: Fibonacci Sequence (recursive). </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o2n_1","title":"O(2\u207f)","text":"<p>O(2\u207f): Exponential. Why is Fibonacci Sequence O(2\u207f)? </p> <p></p>  Details <p> Why is this O(2\u207f)?</p> <p>The total number of calls for fib(n) is close to 2^n, which is why the time complexity is O(2\u207f).</p> <p>Formal Reason:</p> <p> The recurrence relation that describes the time complexity is: </p> <p> T(n)=T(n\u22121)+T(n\u22122)+O(1) </p> <p>This recurrence reflects the fact that each call to fib(n) involves two recursive calls: fib(n - 1) and fib(n - 2). The solution to this recurrence is O(2^n), which means the number of recursive calls grows exponentially with n.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_2","title":"O(n!)","text":"<p>O(n!): Factorial. Extremely inefficient, grows very quickly, seen in permutation and combinatorial problems. Example: Permutation Generation. </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_3","title":"O(n!)","text":"<p>O(n!): Factorial. Why is Permutation Generation O(n!)? </p>  Details <p> Why is Permutation Generation O(n!)?</p> <p> - For every element in the list, we generate all permutations of the remaining elements.</p> <p> - This results in a recursive process where the time complexity grows factorially as the size of the list increases.</p> <p> - The number of possible permutations of a list of n elements is n!, which means that the time complexity of generating all permutations is O(n!).</p> <p>Example:</p> <p> For a list of 5 elements ({1, 2, 3, 4, 5}), there are 5!=1205! = 1205!=120 possible permutations. The function generates all 120 permutations and measures how long this process takes.</p> <p> If you increase the number of elements to 6, the number of permutations increases to 6!=7206! = 7206!=720. For 7 elements, it becomes 7!=5,0407! = 5,0407!=5,040, and so on. The execution time grows factorially with the size of the input list.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o_2","title":"Big O","text":"<p>Big O notation describes the worst-case scenario, representing the upper limit of time or space required as n grows.</p> <p></p> <p>Exercises</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/","title":"Example: Adjacency Matrix Multiplication in a Recommendation System","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#scenario","title":"Scenario","text":"<p>Consider a simple recommendation system with two types of nodes: - Users: U\u2081, U\u2082 - Items: I\u2081, I\u2082, I\u2083  </p> <p>Edges indicate that a user has positively rated an item (for instance, a rating \u2265 4 stars).</p> <p>Relationships: - U\u2081 \u2192 I\u2081, I\u2082 - U\u2082 \u2192 I\u2082, I\u2083  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"<p>We arrange the nodes as [U\u2081, U\u2082, I\u2081, I\u2082, I\u2083]. The adjacency matrix is bipartite, meaning users connect only to items.</p> U\u2081 U\u2082 I\u2081 I\u2082 I\u2083 U\u2081 0 0 1 1 0 U\u2082 0 0 0 1 1 I\u2081 0 0 0 0 0 I\u2082 0 0 0 0 0 I\u2083 0 0 0 0 0 <p>Each row represents a user or item, and each column represents a possible connection (rating or preference).</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#operation-1-a-at-user-similarity","title":"Operation 1: A \u00d7 A\u1d40 (User Similarity)","text":"<p>Multiplying A by its transpose yields user\u2013user similarity, based on shared items.</p> U\u2081 U\u2082 I\u2081 I\u2082 I\u2083 U\u2081 2 1 0 0 0 U\u2082 1 2 0 0 0 I\u2081 0 0 1 0 0 I\u2082 0 0 0 1 0 I\u2083 0 0 0 0 1"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#interpretation","title":"Interpretation","text":"<ul> <li>The top-left block (users \u00d7 users) measures similarity:</li> <li>(U\u2081, U\u2082) = 1 \u2192 They both liked item I\u2082.  </li> <li>(U\u2081, U\u2081) = 2 \u2192 U\u2081 rated two items in total.  </li> <li>Higher values indicate stronger preference overlap.</li> <li>The system can recommend item I\u2083 to U\u2081, since U\u2082, a similar user, rated it positively.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#operation-2-at-a-item-similarity","title":"Operation 2: A\u1d40 \u00d7 A (Item Similarity)","text":"<p>This multiplication yields item\u2013item similarity, based on the users who rated them.</p> U\u2081 U\u2082 I\u2081 I\u2082 I\u2083 I\u2081 0 0 1 1 0 I\u2082 0 0 1 2 1 I\u2083 0 0 0 1 1"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#interpretation_1","title":"Interpretation","text":"<ul> <li>(I\u2081, I\u2082) = 1 \u2192 Both rated by U\u2081.  </li> <li>(I\u2082, I\u2083) = 1 \u2192 Both rated by U\u2082.  </li> <li>Items with higher co-ratings can be recommended together (e.g., recommend I\u2083 to someone who liked I\u2082).</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#analytical-insights","title":"Analytical Insights","text":"<ul> <li>User similarity: Identify users with shared preferences (collaborative filtering).  </li> <li>Item similarity: Detect related or substitutable items (content-based filtering).  </li> <li>Community detection: Group users with similar behavior patterns.  </li> <li>Preference prediction: Estimate missing ratings via matrix operations (e.g., A\u00b2 or matrix factorization).  </li> <li>Recommendation propagation: Use higher powers (A + A\u00b2 + \u2026) to infer multi-hop preference relations.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/","title":"Example: Adjacency Matrix Multiplication in a Scheduling Problem","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#scenario","title":"Scenario","text":"<p>Consider a set of tasks with dependencies, meaning some tasks cannot start until others are completed.</p> <p>Tasks: - A: Design the architecture - B: Implement the code - C: Test the system - D: Deploy the application  </p> <p>Dependencies (A \u2192 B means \u201cB depends on A\u201d): - A \u2192 B (coding requires the design) - B \u2192 C (testing requires code) - C \u2192 D (deployment requires tests) - A \u2192 C (some tests depend directly on the design)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 1 0 B 0 0 1 0 C 0 0 0 1 D 0 0 0 0 <p>Each row represents a preceding task, and each column represents a dependent task. A value of 1 indicates that a task must be completed before another can start.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>This operation shows indirect second-level dependencies \u2014 tasks that depend on others through an intermediate task.</p> A B C D A 0 0 1 1 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 Task A indirectly influences C (A \u2192 B \u2192 C), in addition to the direct A \u2192 C relation.  </li> <li>A\u00b2\u208dA,D\u208e = 1 \u2192 Task A indirectly influences D through A \u2192 B \u2192 C \u2192 D.  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 Task B indirectly influences D through B \u2192 C \u2192 D.  </li> <li>Task D has no outgoing dependencies (it\u2019s the final task in the workflow).</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#analytical-insights","title":"Analytical Insights","text":"<ol> <li> <p>Indirect dependency detection:    Identifies which tasks are connected by multiple dependency levels (useful for critical path analysis).</p> </li> <li> <p>Critical path analysis:    Reveals which initial tasks have the greatest downstream impact on the overall schedule.</p> </li> <li> <p>Delay propagation:    If one task is delayed, A\u00b2 shows which tasks will be affected within two dependency levels.</p> </li> <li> <p>Planning optimization:    Highlights redundant or unnecessary dependencies that may create scheduling bottlenecks.</p> </li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#extensions","title":"Extensions","text":"<ul> <li>A\u00b3 \u2192 shows third-level dependencies (longer paths in the scheduling graph).  </li> <li>(I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 measures total task reachability (direct and indirect dependencies).  </li> <li>Weighted adjacency matrices \u2192 edges can represent task duration, priority, or risk, allowing cumulative impact analysis.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/","title":"scientificpapers.md","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#how-to-write-a-scientific-paper-academic-style-guide-with-examples","title":"How to Write a Scientific Paper (Academic Style Guide with Examples)","text":"<p>Writing a scientific article requires following internationally accepted conventions that ensure clarity, reproducibility, and rigor. A scientific paper must follow a clear and structured format to effectively communicate research findings. Below is a detailed guide to the main sections, including style notes and sample phrases commonly used in academic writing.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#1-title-and-authors","title":"1. Title and Authors","text":"<ul> <li>The title should be concise, informative, and reflect the central contribution.  </li> <li>Avoid vague terms such as \"A Study on...\"; instead, use specific keywords.  </li> <li>Include all authors with institutional affiliations and a corresponding author email.  </li> </ul> \ud83d\udca1 Example titles <p>- Optimizing Neural Network Training with Hybrid Gradient Techniques</p> <p>- A Comparative Study of Quantum Algorithms for Linear Systems</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#2-abstract","title":"2. Abstract","text":"<ul> <li>Length: typically 150\u2013250 words.  </li> <li>Provides a complete but concise overview of the study.  </li> <li>Must include:  </li> <li>Problem or challenge.  </li> <li>Methods and approach.  </li> <li>Key results.  </li> <li>Main conclusion.  </li> </ul> \ud83d\udca1 Example titles <p>- This paper addresses the challenge of...</p> <p>- The proposed method was evaluated on...</p> <p>- Results show a significant improvement in...</p> <p>- These findings suggest that...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#3-keywords","title":"3. Keywords","text":"<ul> <li>Include 4\u20136 specific terms.  </li> <li>Facilitate search and indexing.  </li> </ul> \ud83d\udca1 Examples <p>- \"graph neural networks,\" \"computational linguistics,\" \"data privacy,\" \"blockchain security.\"</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#4-introduction","title":"4. Introduction","text":"<ul> <li>Establishes the context and significance of the research.  </li> <li>Summarizes existing work and identifies the gap.  </li> <li>Ends with objectives and contributions.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Recent studies have demonstrated that...</p> <p>- However, little attention has been paid to...</p> <p>- The aim of this paper is to...</p> <p>- Our contributions are as follows...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#5-problem-statement","title":"5. Problem Statement","text":"<ul> <li>Defines clearly what problem is being addressed.  </li> <li>May include formal definitions, hypotheses, or models.  </li> </ul> \ud83d\udca1 Problem Statement <p>- The problem can be formally defined as...</p> <p>- We hypothesize that...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#6-methodology-method-proposal-solution","title":"6. Methodology (Method / Proposal / Solution)","text":"<ul> <li>Explains how the research was conducted.  </li> <li>Must allow replication by other researchers.  </li> <li>Include: algorithms, models, datasets, tools, hardware/software.  </li> </ul> \ud83d\udca1 Sample phrases <p>- We implemented the proposed approach using...</p> <p>- Experiments were carried out on...</p> <p>- The parameters were set to...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Present experiments systematically and report results clearly.  </li> <li>Use consistent formatting for tables and figures.  </li> <li>Always provide units and ensure charts are readable.  </li> <li>Discuss results in relation to prior work.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Table 1 summarizes the performance of...</p> <p>- As shown in Figure 2, the proposed method outperforms...</p> <p>- These findings are consistent with...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#8-conclusions","title":"8. Conclusions","text":"<ul> <li>Recap the main findings without repeating details.  </li> <li>Emphasize originality and significance.  </li> </ul> \ud83d\udca1 Sample phrases <p>- In conclusion, this work demonstrates that...</p> <p>- The results confirm that...</p> <p>- The study contributes to the field by...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#9-future-work","title":"9. Future Work","text":"<ul> <li>Outline possible directions for extending the study.  </li> <li>Acknowledge current limitations.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Future research could explore...</p> <p>- One limitation of this study is...</p> <p>- Further validation is required in...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#10-references","title":"10. References","text":"<ul> <li>Use a recognized citation style (IEEE, APA, etc.).  </li> <li>Ensure every citation in text has a corresponding reference.  </li> </ul> \ud83d\udca1 Sample phrases <p>- As discussed by Smith et al. [3]...</p> <p>- Following the method in [12]...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#11-general-style-and-recommendations","title":"11. General Style and Recommendations","text":"<ul> <li>Use precise, formal, and objective language.  </li> <li>Prefer passive or impersonal voice (e.g., \"was conducted\" instead of \"we conducted\").  </li> <li>Proofread for clarity and coherence.  </li> </ul> \ud83d\udca1 Sample academic transitions <p>- Moreover, </p> <p>- In contrast, </p> <p>- Therefore, </p> <p>- It is worth noting that... </p> <p>This academic-style guide not only explains the structure of a paper but also provides language templates that students can adapt in their own writing.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/","title":"Example: Adjacency Matrix Multiplication in a Social Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#scenario","title":"Scenario","text":"<p>We have a small social network with four users: A, B, C, and D. Connections are directed \u2014 for instance, like Twitter follows (A \u2192 B means A follows B).</p> Relationship Description A \u2192 B A follows B B \u2192 C B follows C C \u2192 D C follows D A \u2192 D A also follows D"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 0 C 0 0 0 1 D 0 0 0 0 <p>Each row shows who follows whom.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>This multiplication tells us the number of paths of length 2 between users \u2014 i.e., \u201cfriend of a friend\u201d or \u201cfollower of a followed user.\u201d</p> A B C D A 0 0 1 0 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 A can reach C in two steps (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 B can reach D in two steps (B \u2192 C \u2192 D).  </li> <li>Other entries are 0 \u2192 no two-step paths exist.</li> </ul> <p>This helps answer questions such as: - \u201cWho can I reach in two hops?\u201d - \u201cHow many intermediaries are between two users?\u201d - \u201cWho could be suggested as a new friend (indirectly connected users)?\u201d</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#possible-extensions","title":"Possible Extensions","text":"<ul> <li>A\u00b3 \u2192 paths of length 3 (\u201cfriend of a friend of a friend\u201d).  </li> <li>(A + A\u00b2 + A\u00b3 + \u2026) \u2192 overall reachability or connectivity-based centrality.  </li> <li>Weighted networks: non-binary values can represent interaction strength or frequency.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/","title":"Example: Adjacency Matrix Multiplication in a Transportation Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#scenario","title":"Scenario","text":"<p>We have a transportation network connecting four cities: A (Madrid), B (Barcelona), C (Valencia), and D (Seville).</p> <p>The direct routes (by road, rail, or air) are:</p> Route Description A \u2192 B Direct route from Madrid to Barcelona B \u2192 C Direct route from Barcelona to Valencia C \u2192 D Direct route from Valencia to Seville A \u2192 D Direct route from Madrid to Seville"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 0 C 0 0 0 1 D 0 0 0 0 <p>Each row represents the origin city, and each column represents the destination city. A value of 1 means there is a direct connection between the two cities.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The result A\u00b2 shows how many indirect routes (two-leg connections) exist between cities.</p> A B C D A 0 0 1 0 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 There is an indirect route from Madrid to Valencia through Barcelona (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 There is an indirect route from Barcelona to Seville through Valencia (B \u2192 C \u2192 D).  </li> <li>All other values are 0 \u2192 no two-leg routes exist between those cities.</li> </ul> <p>This analysis can be used to: - Identify reachable cities in two steps (e.g., flights with one layover). - Measure network efficiency by counting how many destinations can be reached in a few steps. - Detect bottlenecks or critical connections in the transport system.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#possible-extensions","title":"Possible Extensions","text":"<ul> <li>A\u00b3 \u2192 identifies routes with two layovers (three legs).  </li> <li>(A + A\u00b2 + A\u00b3 + \u2026) \u2192 measures total reachability or overall connectivity.  </li> <li>Weighted networks \u2192 values represent distance, time, or cost, enabling optimization of routes.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/","title":"Other Uses of Matrix Multiplication in Big Data (Beyond Graphs)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#machine-learning-and-deep-learning","title":"Machine Learning and Deep Learning","text":"<ul> <li>Neural network training: each layer performs matrix multiplication operations (weights \u00d7 inputs).  </li> <li>Example: in an embedding-based recommendation model, products and users are represented as vectors, and similarity is computed through matrix multiplications. </li> </ul> Machine Learning and Deep Learning <p>Neural network training: each layer performs matrix multiplication between weights and inputs to compute the activations.   For example, in a layer with weight matrix W and input vector X, the output is Y = W \u00d7 X + b, where b is the bias term.</p> <p>Example: In an embedding-based recommendation model, users and products are represented as vectors in a latent space.   If U is the user embedding matrix (each row represents a user) and P is the product embedding matrix (each row a product),   the similarity or affinity between users and products is computed as S = U \u00d7 P\u1d40,   where S<sub>ij</sub> expresses how much user i is predicted to like product j.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<ul> <li>Vector representations (word embeddings, BERT, transformers): require millions of matrix multiplication operations to compute attention and contextual relationships.  </li> <li>Example: the self-attention mechanism in transformers is based on matrix multiplications between queries, keys, and values.  </li> </ul> Natural Language Processing (NLP) <p>Vector representations: (word embeddings, BERT, transformers) require millions of matrix multiplication operations to compute attention and contextual relationships between tokens. Each word or token is represented as a numerical vector in a high-dimensional space.</p> <p>Example: In the self-attention mechanism of transformers, three matrices are computed: Queries (Q), Keys (K), and Values (V).   The attention scores are calculated through matrix multiplications as follows:   Attention(Q, K, V) = softmax((Q \u00d7 K\u1d40) / \u221ad<sub>k</sub>) \u00d7 V   This allows each token to attend to others according to their contextual relevance, forming the core of models like BERT and GPT.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#image-and-signal-processing","title":"Image and Signal Processing","text":"<ul> <li>In computer vision, convolutions can be represented as matrix multiplications between an image and filters.  </li> <li>In Big Data, this is applied to process large-scale image collections (e.g., facial recognition in massive datasets).</li> </ul> Image and Signal Processing <p>In computer vision, convolutions can be represented as matrix multiplications between an image (a matrix of pixel values) and small filters or kernels.   Each convolution operation slides the filter across the image, performing local matrix multiplications to extract features such as edges, colors, or textures.</p> <p>In Big Data, this approach is used to process large-scale image collections \u2014 for instance, in facial recognition systems or automatic image classification across massive datasets.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#matrix-factorization-in-recommendation-systems","title":"Matrix Factorization in Recommendation Systems","text":"<ul> <li>Techniques such as Singular Value Decomposition (SVD) or Matrix Factorization decompose large matrices (users \u00d7 items) into products of smaller matrices.  </li> <li>This allows for uncovering hidden patterns and predicting ratings.  </li> </ul> Matrix Factorization in Recommendation Systems <p>Techniques such as Singular Value Decomposition (SVD) or Matrix Factorization decompose large rating matrices (users \u00d7 items) into the product of smaller matrices that represent latent features of users and items.</p> <p>This allows for uncovering hidden patterns and predicting missing ratings, forming the foundation of modern collaborative filtering methods used in platforms like Netflix and Spotify.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#scientific-simulations-and-bioinformatics","title":"Scientific Simulations and Bioinformatics","text":"<ul> <li>In molecular dynamics, climatology, or genomics, huge matrices are used to represent interactions.  </li> <li>The system\u2019s evolution is simulated through successive matrix multiplications. </li> </ul> Scientific Simulations and Bioinformatics <p>In fields such as molecular dynamics, climatology, or genomics, large matrices are used to represent complex interactions among system components \u2014 such as forces between atoms, correlations between genes, or environmental variables.</p> <p>The system\u2019s evolution is simulated through successive matrix multiplications, which update the state of the model over time.   This enables the study of complex phenomena such as protein folding, mutation propagation, or large-scale climate patterns.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/","title":"Example: Adjacency Matrix Multiplication in a Web Page Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#scenario","title":"Scenario","text":"<p>Consider a network of four web pages: A, B, C, and D. The edges represent hyperlinks between pages (directed links, where the row is the source and the column is the destination).</p> <p>Direct links: - A \u2192 B, A \u2192 D - B \u2192 C, B \u2192 D - C \u2192 A - D \u2192 (no outgoing links)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 1 C 1 0 0 0 D 0 0 0 0 <p>Each row represents the source page and each column represents the target page. A value of 1 indicates a direct hyperlink between two pages.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The matrix A\u00b2 shows how many two-click paths exist between web pages.</p> A B C D A 0 0 1 1 B 1 0 0 0 C 0 1 0 1 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 From page A, you can reach C in two clicks (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dA,D\u208e = 1 \u2192 From page A, you can reach D in two clicks (A \u2192 B \u2192 D).  </li> <li>A\u00b2\u208dB,A\u208e = 1 \u2192 From page B, you can reach A in two clicks (B \u2192 C \u2192 A).  </li> <li>A\u00b2\u208dC,B\u208e = 1 and A\u00b2\u208dC,D\u208e = 1 \u2192 From page C, you can reach B (C \u2192 A \u2192 B) and D (C \u2192 A \u2192 D).  </li> <li>The entire row for D is zeros \u2192 D is a sink node (no outgoing links).</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#possible-analyses","title":"Possible Analyses","text":"<ul> <li>Two-click reachability: Determine what percentage of the site is accessible within two clicks from a landing page.  </li> <li>Sink or dead-end detection: Identify pages with no outgoing links to improve internal navigation.  </li> <li>Conversion path design: Ensure that key pages (e.g., checkout or signup) are reachable within two clicks.  </li> <li>Internal SEO prioritization: Pages with many indirect incoming links (high column sums in A\u00b2) may attract more navigation flow.</li> </ul> <p>Further extensions: - A\u00b3 \u2192 paths reachable in three clicks. - (I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 cumulative reachability across multiple navigation steps. - Weighted adjacency matrices can model link importance or visibility.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/","title":"Applications \u2013 Monitoring and Performance Engineering","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#introduction","title":"Introduction","text":"<p>The purpose of software engineering is to control complexity, not to create it. \u2014 Jon Louis Bentley (1953\u2013)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#brief-biography","title":"Brief Biography","text":"<p>Jon Louis Bentley is an American computer scientist best known for his influential work in algorithm design, data structures, and software engineering principles. He is the creator of the k-d tree, a widely used data structure for multidimensional searching, and the author of the classic book Programming Pearls, which explores the art of writing efficient and elegant programs. Bentley\u2019s research has shaped modern approaches to algorithmic efficiency and code simplicity, emphasizing that clarity and maintainability are central to performance. Throughout his career, he has worked at Carnegie Mellon University and Bell Labs, collaborating with leading figures such as Brian Kernighan. His teachings and writings continue to inspire generations of programmers to view software engineering as both a scientific and creative discipline. </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#applications","title":"Applications","text":"<p>Performance engineering in real-world applications focuses on translating theory into actionable coding techniques. It involves structuring code, algorithms, and data in ways that optimize runtime and memory without sacrificing readability. The following sections illustrate how to apply Bentley\u2019s rules and other performance patterns in day-to-day software design.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#fundamental-coding-rules","title":"Fundamental Coding Rules","text":"<p>The foundation of performance engineering lies in writing simpler, cleaner, and more efficient code. Bentley\u2019s rules provide timeless principles that emphasize simplicity, focus, and foresight.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#code-simplification","title":"Code Simplification","text":"<p>Simple programs are faster and easier to maintain. By removing unnecessary control structures or redundant computations, developers achieve greater clarity and speed. For example, replacing nested loops or complex conditional logic with direct list comprehensions or optimized built-in functions often yields both cleaner and faster code. (see example)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#problem-simplification","title":"Problem Simplification","text":"<p>Performance issues are often better solved by rethinking the problem rather than refining the code. Instead of sorting an entire dataset to find a median (O(n log n)), algorithms like Quickselect (O(n)) directly solve the core task. Simplifying the objective reduces complexity and computation time, enhancing scalability.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#relentless-suspicion","title":"Relentless Suspicion","text":"<p>Every line of code should justify its existence. Eliminating redundant copies, unnecessary conversions, or inefficient sorting algorithms reduces time and space usage. Replacing custom loops with optimized library functions improves reliability and performance, while streamlining the code.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#early-binding","title":"Early Binding","text":"<p>Early binding involves moving work to an earlier stage to avoid repeated effort. Precomputing constant expressions, caching repetitive results, or establishing database connections before iterative processes saves significant time. It demonstrates the trade-off between memory and time, achieving efficiency through anticipation.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#compiler-and-language-level-optimizations","title":"Compiler and Language-Level Optimizations","text":"<p>Compilers and modern programming languages incorporate various optimization strategies that developers can leverage by writing predictable and efficient code structures.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#hoisting","title":"Hoisting","text":"<p>Loop-invariant computations should be moved outside the loop to avoid repeated evaluation. For example, saving a collection\u2019s length before a loop prevents unnecessary method calls during iteration.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#loop-fusion","title":"Loop Fusion","text":"<p>Two consecutive loops traversing the same dataset can often be combined into one, minimizing loop overhead and improving cache performance.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#unswitching","title":"Unswitching","text":"<p>When an <code>if</code> condition within a loop does not depend on the iteration variable, it can be moved outside the loop. This reduces branching overhead and improves instruction pipeline efficiency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Delay computation until the value is required. This principle, common in functional programming, avoids unnecessary calculations and saves resources, particularly when results may not always be used.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#power-of-2-multiplication","title":"Power-of-2 Multiplication","text":"<p>Bitwise shifts can replace multiplication by powers of two. While modern compilers often handle this automatically, understanding it highlights the role of low-level arithmetic in performance tuning.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#memoization","title":"Memoization","text":"<p>Caching results of expensive function calls prevents redundant computation. It\u2019s especially useful for recursive algorithms or repeated queries with identical parameters.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#primitives-vs-objects","title":"Primitives vs Objects","text":"<p>Primitive types are stored on the stack and accessed faster than objects on the heap. In languages like Java, using primitives instead of boxed types can reduce memory usage and improve performance.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#immutable-vs-mutable-objects","title":"Immutable vs Mutable Objects","text":"<p>Immutable objects like <code>String</code> ensure safety but create new instances with each modification. Using mutable alternatives such as <code>StringBuilder</code> or buffers can significantly reduce overhead in iterative concatenations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#constant-folding-and-propagation","title":"Constant Folding and Propagation","text":"<p>Compilers pre-evaluate constant expressions at compile time, avoiding runtime computation. Writing code that allows such optimizations helps reduce execution cost.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#ordering-tests","title":"Ordering Tests","text":"<p>When evaluating multiple conditions, tests should be ordered by probability and cost. Frequently true conditions or cheaper checks should appear first to minimize evaluation time.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#advanced-optimization-patterns","title":"Advanced Optimization Patterns","text":"<p>Optimization patterns extend beyond code syntax, encompassing structural and algorithmic design decisions that maximize efficiency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#data-structure-optimizations","title":"Data Structure Optimizations","text":"<p>Choose data structures aligned with access patterns. Techniques like packing, encoding, and precomputation minimize memory and enable faster lookups. Cached or sparse representations reduce redundancy in large datasets.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#control-flow-optimizations","title":"Control Flow Optimizations","text":"<p>Reducing control flow complexity enhances predictability and cache performance. Techniques such as loop unrolling, hoisting, and short-circuiting improve performance by minimizing unnecessary iterations or branching.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#functional-optimizations","title":"Functional Optimizations","text":"<p>Function inlining replaces function calls with their bodies to remove call overhead, while tail-recursion elimination prevents stack overflows. Memoization and lazy loading allow functions to reuse or delay results efficiently.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#hardware-dependent-optimizations","title":"Hardware-Dependent Optimizations","text":"<p>Hardware-awareness is essential for high-performance applications. Code should exploit the architecture\u2019s strengths rather than fighting against it.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#instruction-costs","title":"Instruction Costs","text":"<p>Different operations have different CPU costs. Integer addition is faster than division, and avoiding unnecessary multiplications can improve instruction throughput.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#register-allocation","title":"Register Allocation","text":"<p>Keeping frequently used variables in CPU registers minimizes memory latency. Although modern compilers often manage this, understanding register allocation aids in writing CPU-friendly code.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#memory-layout-and-locality","title":"Memory Layout and Locality","text":"<p>Memory access patterns strongly affect performance. Traversing arrays row by row (contiguous memory) instead of by columns improves cache utilization, as adjacent elements are preloaded together.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#paging-and-storage","title":"Paging and Storage","text":"<p>Efficient data placement in memory and awareness of virtual memory paging are key to large-scale performance. Sequential access reduces page faults and cache misses, improving execution predictability.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#5-best-practices-summary","title":"5. Best Practices Summary","text":"<p>Performance optimization should never compromise clarity or correctness. Always profile and measure before attempting optimization. The best programs balance simplicity, accuracy, and efficiency.</p> <p>Key reminders: - Prioritize readability and maintainability. - Optimize only where performance matters. - Measure the impact of each change quantitatively. - Understand the hardware, but design for clarity first.</p> <p>Premature optimization is the root of all evil. \u2014 Donald Knuth</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/examples/","title":"Examples","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/examples/#examples-performance-engineering","title":"Examples \u2014 Performance Engineering","text":"<p>This document collects practical code examples that illustrate each of the fundamental rules discussed in the Applications section of Performance Engineering.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/examples/#code-simplification-example-code-simplification-example","title":"Code Simplification Example {#code-simplification-example}","text":"<p>The Code Simplification rule by Jon Bentley emphasizes that simple programs are both faster and easier to maintain. By reducing unnecessary constructs and using concise syntax, performance and readability can be improved simultaneously.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/examples/#original-code","title":"Original Code","text":"<pre><code>def sum_of_squares(nums):\n    result = 0\n    for num in nums:\n        if num % 2 == 0:\n            result += num ** 2\n    return result\n\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8]\nprint(sum_of_squares(numbers))\n</code></pre> <p>Output:</p> <pre><code>120\n</code></pre> <p>This version works correctly but is unnecessarily verbose: it creates a loop, performs conditional checks, and manually accumulates results.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/examples/#simplified-version","title":"Simplified Version","text":"<pre><code>def sum_of_squares(nums):\n    return sum(num ** 2 for num in nums if num % 2 == 0)\n</code></pre> <p>Output:</p> <pre><code>120\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/examples/#explanation","title":"Explanation","text":"<ul> <li>The expression <code>(num ** 2 for num in nums if num % 2 == 0)</code> generates squares of all even numbers.</li> <li><code>sum()</code> directly adds those values without needing an intermediate variable.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/examples/#benefits","title":"Benefits","text":"<ul> <li>Simplicity: Fewer lines of code, clearer intent.</li> <li>Performance: Python comprehensions are internally optimized in C.</li> <li>Readability: The purpose is immediately visible \u2014 sum of even squares.</li> <li>Maintainability: Less code means fewer potential errors.</li> </ul> <p>This example demonstrates Bentley\u2019s principle: simpler code is often faster code.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/","title":"Experiments \u2013 Monitoring and Performance Engineering","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#introduction","title":"Introduction","text":"<p>\"Programming is an attempt to capture the uniformity of values and processes in actors.\" \u2014 Robin Milner (1934\u20132010)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#brief-biography","title":"Brief Biography","text":"<p>Robin Milner was a British computer scientist renowned for his foundational contributions to programming language theory and formal methods. He developed the ML programming language, one of the earliest to feature type inference and functional programming constructs that influenced modern languages such as Haskell, OCaml, and F#. Milner also created the \u03c0-calculus, a formal model for describing concurrent systems, and made pioneering advances in theoretical computer science related to polymorphism and process calculi. He was a professor at the University of Edinburgh and a fellow of the Royal Society. In 1991, he received the Turing Award for \u201cthree distinct and complete achievements: LCF, ML, and CCS,\u201d which laid the groundwork for much of modern programming language design and verification.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#performance-engineering","title":"Performance Engineering","text":"<p>Performance engineering focuses on understanding and improving how programs execute. It is not only about making code work correctly, but also about ensuring it runs efficiently and scales well. This section explores experimental insights into performance monitoring, optimization, and computational efficiency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#measuring-performance","title":"Measuring Performance","text":"<p>Performance can be measured by analyzing execution time under different computational setups. Consider the case of multiplying two 1024\u00d71024 matrices:</p> <p></p> <ul> <li>Case A: Running time = 1.32 seconds  </li> <li>Case B: Running time = 7.76 seconds </li> </ul> <p>Matrix Multiply Slow </p> <p>Matrix Multiply Fast</p> <p>The only difference between the two implementations is the loop order. This affects how memory is accessed, leading to variations in performance by a factor of up to six. Such behavior arises from data locality, a key concept in performance analysis.  </p> <p>Data that is accessed consecutively tends to remain in fast memory (cache), resulting in quicker computations. When data is scattered, cache performance degrades, leading to slower execution.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#cache-optimization","title":"Cache Optimization","text":"<p>Cache memory provides rapid access to frequently used data. When a requested piece of data is already in cache, a cache hit occurs, which is fast. Otherwise, a cache miss forces the processor to retrieve data from slower main memory.</p> <p>To optimize cache usage: - Reorganize computations to reuse data already loaded into cache. - Access memory in sequential order whenever possible. - Choose data structures that map efficiently to cache lines.  </p> <p>Even small changes in loop nesting or traversal order can significantly improve performance without altering the algorithm itself.</p> <p>Matrix Multiply Blocked (or Tiling) </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#this-implementation-demonstrates-cache-blocking-or-tiling-which-improves-performance-by-maximizing-cache-reuse","title":"This implementation demonstrates cache blocking (or tiling), which improves performance by maximizing cache reuse.","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#algorithmic-optimization","title":"Algorithmic OptimizationContextoIdea b\u00e1sica del algoritmoEficienciaImplicaciones pr\u00e1cticasEjemplo num\u00e9rico simple","text":"<p>Optimization is also achieved through algorithm design. For instance, matrix multiplication can be performed using algorithms with lower theoretical complexity:</p> <ul> <li>Strassen algorithm: (O(n^{2.807}))  </li> <li>Coppersmith\u2013Winograd algorithm: (O(n^{2.376}))  </li> <li>Optimized CW-like algorithms: (O(n^{2.373}))</li> </ul> <p>These methods, often called galactic algorithms, are mainly of theoretical interest since they are rarely used in practical systems. Nonetheless, they illustrate the boundaries of what is computationally possible.</p> Algoritmo de Strassen <p>     La multiplicaci\u00f3n cl\u00e1sica de dos matrices A y B de tama\u00f1o n\u00d7n requiere:   </p> <pre><code>O(n^3)</code></pre> <p>     operaciones (exactamente <code>n^3</code> multiplicaciones y <code>n^3 \u2212 n^2</code> sumas).   </p> <p>     Strassen demostr\u00f3 que no era necesario realizar tantas multiplicaciones y que se pod\u00eda mejorar la complejidad algor\u00edtmica.   </p> <p>Para dos matrices <code>2\u00d72</code>:</p> <pre><code>A = [[a, b],\n     [c, d]]\n\nB = [[e, f],\n     [g, h]]</code></pre> <p>La multiplicaci\u00f3n cl\u00e1sica necesita 8 multiplicaciones.</p> <p>Strassen descubri\u00f3 una forma de calcular el producto usando solo 7 multiplicaciones (y m\u00e1s sumas/restas):</p> <pre><code>M1 = (a + d) (e + h)\nM2 = (c + d) e\nM3 = a (f \u2212 h)\nM4 = d (g \u2212 e)\nM5 = (a + b) h\nM6 = (c \u2212 a) (e + f)\nM7 = (b \u2212 d) (g + h)</code></pre> <p>Luego, el resultado <code>C = A \u00d7 B</code> se obtiene como:</p> <pre><code>C11 = M1 + M4 \u2212 M5 + M7\nC12 = M3 + M5\nC21 = M2 + M4\nC22 = M1 \u2212 M2 + M3 + M6</code></pre> <p>Si se aplica recursivamente a matrices grandes, dividi\u00e9ndolas en submatrices, la complejidad pasa de:</p> <pre><code>O(n^3)  \u2192  O(n^{log2(7)}) \u2248 O(n^{2.81})</code></pre> <p>Esto fue un avance enorme, porque por primera vez se demostr\u00f3 que la multiplicaci\u00f3n de matrices pod\u00eda hacerse en menos de tiempo c\u00fabico.</p> <ul> <li>Strassen es m\u00e1s r\u00e1pido para matrices grandes, pero requiere m\u00e1s memoria.</li> <li>Introduce m\u00e1s operaciones de suma/resta, lo que puede aumentar los errores num\u00e9ricos en c\u00e1lculos con coma flotante.</li> <li>En la pr\u00e1ctica, los algoritmos modernos (como Coppersmith\u2013Winograd o algoritmos h\u00edbridos) solo aplican Strassen a partir de cierto tama\u00f1o de matriz.</li> </ul> <p>     Si multiplicas dos matrices <code>4\u00d74</code>, puedes dividirlas en 8 submatrices     <code>2\u00d72</code> y aplicar Strassen recursivamente. De este modo, se reducen multiplicaciones,     aunque aumentan las sumas.   </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#density-and-sparsity","title":"Density and Sparsity","text":"<p>In many applications, data structures like matrices are mostly empty. Density measures the proportion of non-zero elements, while sparsity measures the proportion of zeros.</p> <p>For example, a 6\u00d76 matrix with 9 non-zero elements has:  </p> <p>Density = 9/36 = 25%,  Sparsity = 75%.</p> <p>Real-world matrices often have extremely low densities (e.g., 0.0003% or 2.31%). Managing such sparse data efficiently requires specialized compression techniques.</p> <p>Example running times for 1024\u00d71024 matrices:</p> Density Time (seconds) 0.1% 0.02 1% 0.08 2% 0.14 3% 0.23 <p>As density increases, the amount of computation and storage requirements grow proportionally.</p> <p>Dense vs CRS </p>  Runing times test <p>Correction: The checksums are identical and the max error is 0 \u2192 both implementations compute the same result. 100%</p> <p>Performance: With <code>n = 512</code> and density = <code>0.25</code> (25%), it\u2019s normal for the dense version (0.013 s) to outperform the sparse/CSR one (0.031 s): at such densities, the overhead of managing indices and irregular memory accesses usually outweighs the savings from skipping zeros; sparse computation is typically memory-bound and suffers from poor cache locality, whereas the dense version makes better use of the hardware.</p> <p>The complexity of sparse operations scales with <code>nnz</code> (not with <code>n\u00b2</code>), but at 25% <code>nnz</code> is already large, so the benefit is diluted. In addition, <code>B*B</code> introduces fill-in (new non-zeros appear), so the product can be much less sparse than <code>B</code> and further worsen performance.</p> Rule of thumb <p>Sparse methods tend to win when the matrix is very sparse (\u224810% or less, and often much less), and/or when <code>n</code> is large; otherwise, dense methods are usually faster.</p> What to test to \u201csee\u201d the crossover <ul> <li>Lower the density (e.g., 1%, 2%, 5%) and increase the size (<code>n \u2265 2000</code>).</li> <li>Repeat several iterations to warm up the JVM and take the best/median time.</li> <li>Confirm you\u2019re using CSR with primitive arrays (avoid autoboxing/objects).</li> <li>If you compare against a highly optimized dense implementation (BLAS/Vector API), expect it to win even at fairly low densities.</li> </ul> Conclusion <p>Your output makes sense; at 25% and <code>n = 512</code>, it\u2019s expected that the dense version is about 2\u20133\u00d7 faster than the sparse one.</p>  Runing times test <p>java DenseVsCSRSquare 1024 0.01 </p> <p>java DenseVsCSRSquare 1024 0.001</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#storage-formats-for-sparse-data","title":"Storage Formats for Sparse Data","text":"<p>In Performance Engineering, the way data is stored in memory has a major impact on computational efficiency. This is especially true for sparse matrices, where most elements are zero and storing all entries would waste both memory and processing time.</p> <p>Several specialized formats have been developed to store only the non-zero values and their positions efficiently. The choice of format depends on the access pattern (by rows, by columns, or by diagonals) and on the type of operations to be performed. Dense matrices are commonly stored as two-dimensional arrays, but sparse matrices require alternative formats to avoid wasting memory. Common representations include:</p> <ul> <li>COO (Coordinate Format) </li> <li>CRS (Compressed Row Storage) </li> <li>CCS (Compressed Column Storage) </li> <li>CDS (Compressed Diagonal Storage) </li> <li>JDS (Jagged Diagonal Storage)</li> </ul> <p>Each format balances storage efficiency and computational performance differently. Choosing the right one depends on access patterns, density level, and the algorithm used.</p> <p>Click on each format below to expand its description and example.</p> COO (Coordinate Format) <p>       The Coordinate (COO) format stores each non-zero entry of a sparse matrix       as an explicit triplet <code>(row, column, value)</code>.     </p> <p>Example: 4\u00d74 matrix A</p> 0500 0080 0003 1000 <p>COO triplets (row, col, value)</p> RowColValue 015 128 233 301 <p>Advantages</p> <ul> <li>Very easy to construct and understand.</li> <li>Good for incremental matrix assembly.</li> </ul> <p>Disadvantages</p> <ul> <li>Poor for arithmetic or repeated access (needs scanning all triplets).</li> <li>Usually converted to CSR/CCS for fast computations.</li> </ul> CRS (Compressed Row Storage) <p>       The Compressed Row Storage (CRS) format, also known as        Compressed Sparse Row (CSR), stores only the non-zero values of the matrix       row by row, together with their column indices and row pointers.     </p> <p>Example: 4\u00d74 matrix A</p> 0500 0080 0003 1000 <p>CRS Representation</p> <p>       The matrix is represented using three arrays:     </p> <ul> <li><code>values</code> \u2192 stores all non-zero elements row by row</li> <li><code>col_index</code> \u2192 stores the column index of each non-zero element</li> <li><code>row_ptr</code> \u2192 marks the starting index of each row in the <code>values</code> array</li> </ul> ArrayContentDescription <code>values</code> [5, 8, 3, 1] Non-zero values in row order <code>col_index</code> [1, 2, 3, 0] Column indices of each value <code>row_ptr</code> [0, 1, 2, 3, 4] Start of each row in <code>values</code> <p>How it works</p> <p>       Each pair (<code>values[k]</code>, <code>col_index[k]</code>) gives the value and column position        of a non-zero element. The array <code>row_ptr</code> shows where each row begins in <code>values</code>.     </p> <p>Advantages</p> <ul> <li>Efficient for row-wise operations (e.g., matrix-vector multiplication).</li> <li>Compact memory usage for sparse matrices.</li> </ul> <p>Disadvantages</p> <ul> <li>Column access is slower (requires full row scan).</li> <li>More complex to modify dynamically.</li> </ul> CCS (Compressed Column Storage) <p>       The Compressed Column Storage (CCS) format, also known as        Compressed Sparse Column (CSC), is the column-oriented version of CRS.       It stores all non-zero values column by column, together with their row indices       and column pointers.     </p> <p>Example: 4\u00d74 matrix A</p> 0500 0080 0003 1000 <p>CCS Representation</p> <p>The matrix is represented using three arrays:</p> <ul> <li><code>values</code> \u2192 stores the non-zero elements column by column</li> <li><code>row_index</code> \u2192 stores the row index of each non-zero element</li> <li><code>col_ptr</code> \u2192 marks the starting index of each column in the <code>values</code> array</li> </ul> ArrayContentDescription <code>values</code> [1, 5, 8, 3] Non-zero values in column order <code>row_index</code> [3, 0, 1, 2] Row indices corresponding to each value <code>col_ptr</code> [0, 1, 2, 3, 4] Start of each column in <code>values</code> <p>How it works</p> <p>       Each pair (<code>values[k]</code>, <code>row_index[k]</code>) represents a non-zero entry,       where <code>col_ptr</code> indicates the start of each column\u2019s data.       This structure is especially efficient for operations that proceed column by column.     </p> <p>Advantages</p> <ul> <li>Efficient for column-wise operations and solving sparse linear systems.</li> <li>Compact and easy to convert from CRS (by transposing the matrix).</li> </ul> <p>Disadvantages</p> <ul> <li>Row access is slower (requires scanning the whole column).</li> <li>Not ideal for dynamic updates or incremental builds.</li> </ul> CDS (Compressed Diagonal Storage) <p>       The Compressed Diagonal Storage (CDS) format (also called       Diagonal or Banded storage) stores only the diagonals that contain       non-zero elements. Each diagonal is identified by its <code>offset</code> <code>k = col - row</code> (main diagonal <code>k = 0</code>, upper diagonals <code>k &gt; 0</code>, lower diagonals <code>k &lt; 0</code>).     </p> <p>Example: 5\u00d75 matrix A (non-zeros on k = -1, 0, 1)</p> 14000 25700 03680 00971 00024 <p>Diagonal offsets</p> <p> <code>diag_offsets = [-1, 0, 1]</code> </p> <p>Compact diagonal data (per offset)</p> k (offset)Data (compact)Length -1[2, 3, 9, 2]4  0[1, 5, 6, 7, 4]5 +1[4, 7, 8, 1]4 <p>Padded (aligned by column index, optional)</p> kc=0c=1c=2c=3c=4 -12392  015674 +14781 <p>How it works</p> <ul> <li>Only selected diagonals are stored, identified by <code>diag_offsets</code>.</li> <li>Each diagonal is saved as a compact vector (optionally padded to length <code>n</code> for alignment).</li> <li>Multiplications and solves that access along diagonals benefit from locality and reduced storage.</li> </ul> <p>Advantages</p> <ul> <li>Excellent for banded matrices with small bandwidth (e.g., tridiagonal, pentadiagonal).</li> <li>Very compact and cache-friendly for near-diagonal sparsity patterns.</li> <li>Simplifies specialized algorithms (Thomas algorithm, banded solvers).</li> </ul> <p>Disadvantages</p> <ul> <li>Not suitable for general sparsity patterns (non-zeros far from diagonals).</li> <li>Number of stored diagonals must be known; adding arbitrary entries may require format changes.</li> <li>Conversions to/from CSR/CCS may be needed for generic sparse operations.</li> </ul> JDS (Jagged Diagonal Storage) <p>       The Jagged Diagonal Storage (JDS) format is a variant of the diagonal approach       that reorganizes the matrix rows according to the number of non-zero elements       they contain. Rows with more non-zeros appear first, producing \u201cjagged\u201d diagonals       of decreasing length.     </p> <p>       The goal of JDS is to improve cache efficiency and enable vectorization        in matrix-vector multiplications, especially on vector or SIMD architectures.     </p> <p>Example: 5\u00d75 matrix A</p> 05000 10800 20030 00004 70000 <p>Step 1: Count non-zeros per row</p> RowNon-zeros 01 12 22 31 41 <p>Step 2: Reorder rows (descending by non-zeros)</p> <p>       New row order \u2192 [1, 2, 0, 3, 4]     </p> <p>Step 3: Build jagged diagonals</p> DiagonalValuesColumn Indices 0[1, 2, 0, 0, 7][0, 0, 1, 3, 4] 1[8, 3, 5, 4][2, 3, 1, 4] <p>Additional arrays:</p> <ul> <li><code>perm</code> \u2192 stores the row permutation [1, 2, 0, 3, 4]</li> <li><code>jd_ptr</code> \u2192 starting index of each jagged diagonal [0, 5, 9]</li> <li><code>col_index</code> \u2192 flattened list of all column indices [0,0,1,3,4,2,3,1,4]</li> <li><code>values</code> \u2192 flattened list of all non-zero values [1,2,0,0,7,8,3,5,4]</li> </ul> <p>How it works</p> <ul> <li>Rows are sorted by decreasing number of non-zero elements.</li> <li>Each \u201cdiagonal\u201d stores one element per row, resulting in irregular (jagged) lengths.</li> <li>Access is highly sequential, optimizing vector and GPU execution.</li> </ul> <p>Advantages</p> <ul> <li>Optimized for vectorized matrix-vector multiplication.</li> <li>Improves cache usage due to data locality in flattened arrays.</li> <li>Good for hardware with SIMD or streaming access patterns.</li> </ul> <p>Disadvantages</p> <ul> <li>More complex to build and interpret than CSR/COO.</li> <li>Less efficient for random access or element updates.</li> <li>Row order must be stored separately for reconstruction.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/","title":"Lectures \u2013 Monitoring and Performance Engineering","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#introduction","title":"Introduction","text":"<p>Programming is the art of telling another human being what one wants the computer to do. Donald Knuth</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#brief-biography","title":"Brief Biography","text":"<p>Donald Ervin Knuth, born on January 10, 1938, in Milwaukee, Wisconsin, USA, is a computer scientist and professor emeritus at Stanford University, best known as the author of The Art of Computer Programming, one of the most influential works in the field. Knuth studied mathematics at Case Institute of Technology and earned his Ph.D. in mathematics from the California Institute of Technology (Caltech). Often referred to as the \u201cfather of algorithm analysis,\u201d he pioneered many of the foundational methods for analyzing algorithm efficiency. He is also the creator of the TeX typesetting system and the METAFONT font design system, both of which revolutionized scientific publishing. His meticulous approach to computer science has inspired generations of researchers and programmers. Throughout his career, Knuth has received numerous honors, including the Turing Award, the National Medal of Science, and the John von Neumann Medal. He continues to work on later volumes of The Art of Computer Programming and maintains his commitment to the precision and beauty of computing.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#improving-performance","title":"Improving Performance","text":"<p>The challenge of performance engineering lies in handling the increasing size and complexity of computations. Several strategies can help address this:</p> <ul> <li> <p>Faster processors: Following Moore\u2019s law, the number of transistors doubles approximately every two years, leading to faster chips. </p> </li> <li> <p>Parallelism: Use multicore processors, GPUs, or clusters to execute computations simultaneously. </p> </li> <li> <p>Software optimization: Adapt algorithms to take advantage of hardware and domain-specific structures. </p> </li> </ul> <p></p> <p>Moore's Law: The number of transistors on microchips doubles every two years</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#bentleys-rules-for-performance","title":"Bentley\u2019s Rules for Performance","text":"<p>Jon Bentley proposed four practical rules to improve program performance. Each focuses on simplifying and optimizing both problem and implementation.</p> <p>Code Simplification Fast programs are simple. Keep code minimal and clean so that it is easier to optimize and maintain. Key takeaway: Simplicity enhances speed and reliability.</p>  Code Simplification <p>Imagine cooking a recipe with twenty unnecessary steps \u2014every time you switch utensils or ingredients, you lose time and coordination.</p> <p>If you simplify the recipe to the essential steps, you finish faster and make fewer mistakes.</p> <p>Cooking well and quickly means keeping only what\u2019s truly needed.</p> <p>Problem Simplification Simplify the problem itself before optimizing the code. Reducing complexity in the problem often yields a more efficient solution. Key takeaway: A simpler problem produces a faster solution.</p>  Problem Simplification <p>Suppose you need to organize a library. Instead of sorting all the books in the world by author and date, you decide to organize only the books you actually own.</p> <p>By redefining the problem, you can move faster and get better results.</p> <p>Before solving a problem, make sure you\u2019re solving the right one \u2014 and at the right scale.</p> <p>Relentless Suspicion Question every instruction in time-critical code and every field in space-critical data structures. Key takeaway: Constantly verify the necessity of each operation.</p>  Relentless Suspicion <p>Think about packing a suitcase for a trip. Every item you pack takes up space and adds weight. If you look critically (\u201cDo I really need this?\u201d), you\u2019ll realize many things are unnecessary.</p> <p>The lighter the suitcase, the easier it is to move and the less time you spend searching through it.</p> <p>Doubt everything that doesn\u2019t add real value; lightness improves performance.</p> <p>Early Binding Perform computations earlier to avoid redundant work later. Precomputing results or decisions saves time during execution. Key takeaway: Shift work forward when possible to minimize repetition.</p>  Early Binding <p>It\u2019s like ironing your clothes right after washing them instead of every morning before leaving home.</p> <p>You put in effort once, and then you benefit for days.</p> <p>Doing work early prevents having to repeat it later and keeps things flowing smoothly.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#optimization-ethics","title":"Optimization Ethics","text":"<p>While performance optimization is crucial, it should not come prematurely. As two pioneers famously warned:</p> <p>Premature optimization is the root of all evil. \u2014 Donald Knuth The first rule of program optimization is \u201cDon\u2019t do it.\u201d The second rule (for experts only) is \u201cDon\u2019t do it yet.\u201d \u2014 Michael A. Jackson</p> <p>The goal of performance engineering is to first produce clean and correct code that behaves as expected. Only after correctness is guaranteed should optimization efforts begin.</p> <p>Performance engineering, therefore, is not only about increasing speed but about understanding, measuring, and refining computation to achieve a sustainable balance between accuracy, clarity, and efficiency.</p> <p></p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/","title":"Complexity management","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/","title":"Monitoring and performance engineering","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/","title":"Architectures for Big Data","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/","title":"Parallel programming","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/","title":"Map reduce","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/","title":"Distributed file systems","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/","title":"Development of applications for execution on Big Data clusters","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/","title":"Vector programming","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""}]}