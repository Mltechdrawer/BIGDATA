{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Big DataWelcome to Big Data","text":""},{"location":"#big-data_1","title":"\ud83d\udcca Big Data","text":"<p>Big Data is a compulsory course of the 3rd year in the Degree in Data Science and Engineering (1st semester, 6 ECTS). It provides students with the theoretical foundations and practical skills to design, implement, and analyse data-intensive applications that are reliable, scalable, and maintainable.</p> <p>The course combines theory, practical exercises, and laboratory sessions. Weekly activities are closely related to the theoretical content, reinforcing knowledge through a group project with iterative development and presentations. Students will acquire essential competences in parallel programming, distributed systems, and performance engineering \u2014 all key skills for modern data engineering and analytics.</p> <p>The learning approach emphasises individual and team work, fostering the ability to apply theoretical concepts to real-world Big Data problems. This course serves as a solid foundation for advanced subjects related to distributed systems, cloud computing, and large-scale machine learning.</p> <p>Faculty:</p> <ul> <li>Jos\u00e9 Juan Hern\u00e1ndez Cabrera. Practical Sessions Responsible  </li> <li>Jos\u00e9 \u00c9vora G\u00f3mez.  </li> <li>Mar\u00eda Dolores Afonso Su\u00e1rez. Course Coordinator  </li> </ul> <p></p> <p></p> <p> </p> <p>\u00a9 2025 Mar\u00eda Dolores Afonso Su\u00e1rez. Este material se distribuye bajo licencia Creative Commons Atribuci\u00f3n 4.0 Internacional (CC BY 4.0).</p>"},{"location":"contents/","title":"Contents","text":"<p>Big Data is structured in two blocks that guide students from the theoretical foundations of complexity and architectures, to the practical aspects of distributed and parallel programming. The aim is to provide the knowledge and skills required to understand, design, and implement scalable and efficient data-intensive applications.</p> <p>BLOCK 1. Theoretical Concepts of Big Data covers complexity management, monitoring and performance engineering, and architectures for Big Data systems. These topics provide the conceptual basis for understanding the challenges of reliability, scalability, and fault tolerance in large-scale data systems.</p> <p>BLOCK 2. Distributed and Parallel Programming introduces the fundamentals of parallel programming, MapReduce, distributed file systems, cluster-oriented application development, and vector programming. These topics consolidate the student\u2019s ability to design and implement Big Data solutions that exploit parallelism, distribution, and modern computational infrastructures.</p>"},{"location":"introduction/","title":"Introduction to Big Data","text":"<p>A new era of data-driven innovation</p>"},{"location":"introduction/#from-early-computing-to-the-digital-revolution","title":"From Early Computing to the Digital Revolution","text":"<p>The journey of Big Data starts with the evolution of computing itself:</p> <ul> <li>1950s: John von Neumann formalized computer architecture, setting the basis for digital computation. At the same time, advances in numerical algorithms and game theory influenced how data and complexity would be modeled.  </li> <li>1960s\u20131970s: Paul Eisler\u2019s printed circuit boards and John Bardeen\u2019s transistor discoveries paved the way for miniaturization. Jack Kilby\u2019s microchip and Douglas Engelbart\u2019s vision of interactive computing changed how people interacted with technology.  </li> <li>1980s\u20131990s: Ted Nelson conceptualized hypertext, Robert Metcalfe created Ethernet, and Tim Berners-Lee at CERN invented the World Wide Web, democratizing access to global information. Alan Kay and Engelbart further advanced the idea of graphical user interfaces (GUI).  </li> <li>2000s: Complex systems modeling, network growth, and ubiquitous devices fueled an explosion of data that set the stage for the Big Data era.</li> </ul> <p> Big Data era</p>"},{"location":"introduction/#the-convergence-of-statistics-and-artificial-intelligence","title":"The Convergence of Statistics and Artificial Intelligence","text":"<p>Big Data analytics is powered by advances in mathematics, statistics, and AI:</p> <ul> <li>Thomas Bayes: Introduced probabilistic reasoning, enabling decision-making under uncertainty.  </li> <li>Geoffrey Hinton: Pioneered deep learning, with neural networks that can process speech, images, and language at scale.  </li> <li>Richard Sutton: Advanced reinforcement learning, allowing systems to learn by trial and error, simulating real-world decision-making.  </li> </ul> <p>Together, these approaches transformed massive datasets into knowledge and actionable insights.</p> <p></p>"},{"location":"introduction/#what-is-big-data","title":"What Is Big Data?","text":"<p>Big Data refers to datasets that are too large, fast, or diverse to be managed by traditional methods. Its defining characteristics are often expressed as the 5Vs:</p> <ul> <li>Volume: massive quantities of data.  </li> <li>Velocity: real-time or near real-time data flows.  </li> <li>Variety: text, images, video, audio, and sensor data.  </li> <li>Veracity: ensuring accuracy and reliability.  </li> <li>Value: extracting useful insights for decision-making.  </li> </ul> <p></p>"},{"location":"introduction/#how-big-is-big","title":"How big is \u201cBig\u201d?","text":"<p>An exabyte (EB) equals one million terabytes (TB). For example: - 40 EB = 40,000 PB = 40,000,000 TB = 40,000,000,000 GB. This unimaginable scale is equivalent to millions of modern hard drives, highlighting the storage and processing challenges of Big Data.</p>"},{"location":"introduction/#big-data-architectures","title":"Big Data Architectures","text":"<ul> <li>Data Lake: Centralized repository that stores raw, unstructured, and semi-structured data. Highly flexible, but lacks governance and fast querying.  </li> <li>Data Warehouse: Structured, schema-based system optimized for analytics and business intelligence (BI). Enforces ACID transactions and strong data consistency.  </li> <li>Data Lakehouse: Hybrid architecture combining the scalability of lakes with the reliability of warehouses. Supports both raw and structured data, ACID compliance, and efficient queries.  </li> </ul> <p>The Lakehouse approach is becoming the standard in modern Big Data platforms.</p> <p></p>"},{"location":"introduction/#highlights-of-big-data-in-the-21st-century","title":"Highlights of Big Data in the 21st Century","text":"<p>Recent and emerging trends shaping Big Data include:</p> <ol> <li>Rise of cloud-native Big Data solutions.  </li> <li>Real-time data processing at scale.  </li> <li>Privacy and governance as central challenges.  </li> <li>Edge computing for localized analytics.  </li> <li>Integration of AI and machine learning into data platforms.  </li> <li>Big Data applications in healthcare and medicine.  </li> <li>Data democratization and self-service analytics.  </li> <li>Data lakes and lakehouses as dominant architectures.  </li> <li>Advances in Natural Language Processing (NLP) for text and speech.  </li> <li>Quantum computing as a future enabler of large-scale analytics.  </li> <li>Big Data for sustainability and climate change.  </li> <li>Data Fabric and Data Mesh architectures for decentralized governance.  </li> </ol>"},{"location":"introduction/#the-impact-of-big-data-on-society","title":"The Impact of Big Data on Society","text":"<p>Big Data influences multiple aspects of daily life and industry:</p> <ul> <li>Spam and fraud detection.  </li> <li>Recommender systems powering e-commerce and entertainment.  </li> <li>Emotional AI analyzing human sentiment.  </li> <li>Sensorization and IoT for smart cities, vehicles, and health.  </li> <li>Video analysis for security, transport, and medical imaging.  </li> </ul> <p> Spam</p> <p> Fraud</p> <p> Emotional Intelligence</p> <p> Sensorization</p> <p> Recommendation systems</p> <p>These applications show Big Data\u2019s power to deliver innovation, while also raising challenges of ethics, privacy, and fairness.</p>"},{"location":"introduction/#why-study-big-data","title":"Why Study Big Data?","text":"<p>By the end of this course, students will be able to:</p> <ul> <li>Design scalable and distributed systems.  </li> <li>Implement parallel programming and data-intensive algorithms.  </li> <li>Integrate machine learning and AI into Big Data workflows.  </li> <li>Address social and ethical challenges in the use of large-scale data.  </li> </ul> <p>Big Data is not only about technology \u2014 it is about shaping the future of science, business, and society in a data-driven world.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/acid/","title":"ACID","text":"<p>ACID is a set of four essential properties that guarantee reliable, consistent, and safe transactions in a database system.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/acid/#atomicity","title":"Atomicity","text":"<p>A transaction must be treated as a single, indivisible unit: either all its operations are completed, or none are. If something fails, the entire transaction is rolled back.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/acid/#consistency","title":"Consistency","text":"<p>A transaction must take the database from one valid state to another valid state, following all defined rules, constraints, and data integrity requirements.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/acid/#isolation","title":"Isolation","text":"<p>Concurrent transactions must not interfere with each other. Each transaction should behave as if it were the only one running, even when many occur simultaneously.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/acid/#durability","title":"Durability","text":"<p>Once a transaction is committed, its results are permanent, even in the event of system crashes or power failures.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/","title":"Architectures for Big Data","text":"<p>Main architectural models used in modern Big Data systems. The progression from Lambda to Kappa and finally to Lakehouse reflects the evolution in how large-scale data processing pipelines are designed, deployed, and maintained.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#lambda-architecture","title":"Lambda Architecture","text":"<p>Lambda Architecture</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#overview","title":"Overview","text":"<p>The Lambda Architecture was proposed to address workloads that require: - Low-latency processing of real-time data streams - Accurate historical analysis of large datasets - Fault-tolerant distributed storage and computation </p> <p>It splits computation into two separate layers: - A Batch Layer for comprehensive historical processing - A Speed Layer for incremental, real-time computation  </p> <p>The results from both streams are merged in a Serving Layer, which exposes data for querying.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#components","title":"Components","text":"Layer Purpose Typical Technologies Batch Layer Computes results on large historical datasets; stores master dataset HDFS, Spark, MapReduce Speed Layer Processes streaming data with low latency Kafka Streams, Apache Flink, Spark Streaming Serving Layer Exposes processed data for queries Cassandra, HBase, Elasticsearch"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#workflow","title":"Workflow","text":"<ol> <li>Raw data is ingested into distributed storage.  </li> <li>The Batch Layer performs periodic, large-scale recomputations.  </li> <li>The Speed Layer computes incremental updates from real-time streams.  </li> <li>Both views are merged to answer queries.  </li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#advantages-and-challenges","title":"Advantages and Challenges","text":"<p>Advantages - Fault-tolerant by design (historical data always recoverable). - Accurate due to full recomputation in the batch layer.  </p> <p>Challenges - Code duplication: logic must be implemented twice. - Operational complexity is high. - High cost in maintenance and monitoring.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#kappa-architecture","title":"Kappa Architecture","text":"<p>Kappa Architecture</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#21-motivation","title":"2.1 Motivation","text":"<p>Kappa Architecture was introduced as a simplification of Lambda Architecture. It is based on the idea that:  </p> <p>If all data is a stream, then batch processing becomes unnecessary. </p> <p>This allows the system to use only one processing model: streaming.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#core-principles","title":"Core Principles","text":"<ul> <li>All data is treated as an event stream.  </li> <li>A durable append-only log acts as the system of record.  </li> <li>Stream processors derive real-time and historical views by replaying the log when necessary.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#components_1","title":"Components","text":"Component Description Example Technologies Log Storage Stores all events in order Apache Kafka, Redpanda Stream Processing Engine Processes data in real time Apache Flink, Kafka Streams, Spark Structured Streaming Serving Layer Stores processed materialized views Cassandra, Elasticsearch"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#advantages-and-limitations","title":"Advantages and Limitations","text":"<p>Advantages - No duplicated code path (simpler development). - Reprocessing is possible by replaying the event log. - Lower operational overhead compared to Lambda.  </p> <p>Limitations - Requires log storage with long retention policies. - Some historical batch workloads may be less efficient than Lambda.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#lakehouse-architecture","title":"Lakehouse Architecture","text":"<p>Lakehouse</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#background","title":"Background","text":"<p>Traditional Data Warehouses enforce strong schema and  guarantees but are expensive and inflexible. Data Lakes, in contrast, store raw data cheaply but lack governance, transactional safety, and can degrade into data swamps.  </p> <p>The Lakehouse Architecture aims to combine the strengths of both.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#core-characteristics","title":"Core Characteristics","text":"Feature Description Unified Storage Uses a Data Lake as the central storage platform (e.g., S3, HDFS). ACID Transactions Enabled through metadata layers such as Delta Lake, Iceberg, or Hudi. Schema Enforcement + Governance Ensures data quality and consistency. Supports both BI and ML Workloads Enables SQL analytics and AI pipelines on the same data."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#layered-data-structure","title":"Layered Data Structure","text":"Layer Data State Purpose Bronze Raw ingested data Archival + traceability Silver Cleaned and structured data Reliable analytical base Gold Aggregated business-level data BI dashboards + ML models"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#key-technologies","title":"Key Technologies","text":"Component Technologies Storage AWS S3, Azure Blob, GCS, HDFS Table Format Delta Lake, Apache Iceberg, Apache Hudi Processing Apache Spark, Trino, Databricks, Flink"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#advantages","title":"Advantages","text":"<ul> <li>Eliminates duplicate data pipelines.  </li> <li>Reduces complexity compared to Lambda/Kappa.  </li> <li>Supports machine learning workflows natively.  </li> <li>Highly cost-effective when deployed in cloud environments.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/architectures/#evolution-summary","title":"Evolution Summary","text":"Architecture Batch Streaming Complexity Best Use Case Lambda Yes Yes High Systems requiring historical accuracy and real-time data Kappa No Yes Medium Event-driven and continuous streaming applications Lakehouse Yes Depends on engine Medium Unified environments for BI and ML with governance <p>Comparison</p> <p>Case Study</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/","title":"Case Studies for Big Data Architectures","text":"<p>Practical case studies to apply the architectural concepts learned in the course: - The first one focuses on Apache Spark in a Lakehouse context. - The second one focuses on Apache Flink and event stream processing following the Kappa Architecture model.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#case-study-1-agricultural-iot-sensor-analytics-with-apache-spark-lakehouse-model","title":"Case Study 1: Agricultural IoT Sensor Analytics with Apache Spark (Lakehouse Model)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#scenario","title":"Scenario","text":"<p>A smart agriculture company monitors environmental variables (temperature, humidity, soil moisture, sunlight levels) using thousands of IoT sensors deployed across farmlands.   Sensors send measurements every 10 seconds, generating millions of data records per day.  </p> <p>The company needs: - Real-time alerts for abnormal readings. - Daily analytics to detect patterns affecting crop yield. - A single data storage layer supporting BI dashboards and ML models.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this case, students should be able to: - Design a Lakehouse-based architecture with unified storage. - Implement data ingestion from streaming and batch sources. - Use Spark for both batch and structured streaming operations. - Apply data refinement through Bronze \u2192 Silver \u2192 Gold layers.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#proposed-architecture","title":"Proposed Architecture","text":"Layer Storage / Tool Purpose Bronze S3 / HDFS raw zone Store raw ingested sensor events Silver Delta Lake / Iceberg Clean, normalize, deduplicate data Gold Delta Lake curated tables Aggregated metrics for dashboards and ML Processing Apache Spark (batch + streaming) ETL, feature engineering, anomaly detection Serving Power BI / Superset Visualization and reporting"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#student-tasks","title":"Student Tasks","text":"<ol> <li>Data Ingestion</li> <li>Simulate streaming input using Kafka or a file source with incremental append.  </li> <li> <p>Write raw input to the Bronze table.  </p> </li> <li> <p>Data Cleaning and Normalization </p> </li> <li>Use Spark DataFrames to:  <ul> <li>Convert timestamps  </li> <li>Remove duplicated records  </li> <li>Standardize measurement units  </li> </ul> </li> <li> <p>Store cleaned data in the Silver table.  </p> </li> <li> <p>Analytical Aggregation </p> </li> <li>Compute hourly averages and anomalies per sensor.  </li> <li> <p>Save aggregated results to the Gold table.  </p> </li> <li> <p>Visualization </p> </li> <li>Create a dashboard to monitor:  <ul> <li>Temperature trends over time  </li> <li>Sensor malfunction detection  </li> <li>Real-time anomaly alerts  </li> </ul> </li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#reflection-questions","title":"Reflection Questions","text":"<ul> <li>What benefits does the Bronze \u2192 Silver \u2192 Gold model bring to maintainability?  </li> <li>How does Delta Lake\u2019s transaction log improve reliability?  </li> <li>Could this design be migrated to a cloud-native environment without architectural changes?  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#case-study-2-real-time-traffic-monitoring-with-apache-flink-kappa-architecture","title":"Case Study 2: Real-Time Traffic Monitoring with Apache Flink (Kappa Architecture)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#scenario_1","title":"Scenario","text":"<p>A transportation authority wants to analyze live vehicle traffic across a city to: - Detect traffic congestion in real time. - Trigger automatic notifications to road signage systems. - Identify long-term traffic flow patterns.  </p> <p>Data is streamed continuously from thousands of road sensors detecting: - Vehicle count - Average speed - Road segment ID - Timestamp  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#learning-objectives_1","title":"Learning Objectives","text":"<p>Students should be able to: - Apply Kappa Architecture principles. - Use Apache Flink for real-time stream processing. - Maintain stateful event processing pipelines. - Generate materialized views for real-time dashboards.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#proposed-architecture_1","title":"Proposed Architecture","text":"Component Tool Purpose Log Storage Apache Kafka Durable ordered event log (system of record) Stream Processing Engine Apache Flink Real-time computation and state management Serving Storage Cassandra / Elasticsearch Low-latency access to computed metrics Visualization Grafana Real-time analytics dashboard"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#student-tasks_1","title":"Student Tasks","text":"<ol> <li>Ingest Data Stream </li> <li>Consume JSON events from Kafka.  </li> <li> <p>Define event schema in Flink.  </p> </li> <li> <p>Real-Time Aggregation </p> </li> <li> <p>Compute:  </p> <ul> <li>Vehicles per road segment (per minute)  </li> <li>Average speed over sliding windows  </li> </ul> </li> <li> <p>Anomaly Detection </p> </li> <li>Identify abnormal congestion (speed &lt; threshold &amp;&amp; count &gt; threshold).  </li> <li> <p>Trigger notification events for downstream systems.  </p> </li> <li> <p>Materialized View Output </p> </li> <li>Write aggregated results to Cassandra or Elasticsearch.  </li> <li>Build a real-time dashboard using Grafana.  </li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#reflection-questions_1","title":"Reflection Questions","text":"<ul> <li>How does Flink maintain reliable state over long-running streaming jobs?</li> <li>Why is the event log (Kafka) considered the single source of truth in Kappa Architecture?</li> <li>What challenges arise when reprocessing events by replaying logs?</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/case_study/#final-comparison-between-the-two-cases","title":"Final Comparison Between the Two Cases","text":"Case Architecture Model Processing Engine Best for IoT Agriculture (Spark) Lakehouse Batch + Streaming (Spark) Combined historical + real-time analytics Traffic Monitoring (Flink) Kappa Streaming-only (Flink) Low-latency continuous event processing"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/","title":"Comparison of Big Data Architectures","text":"<p>Key differences between Lambda, Kappa, and Lakehouse architectures.  </p> <p>The comparison focuses on processing models, complexity, maintenance, scalability, and suitability for various workloads.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#conceptual-overview","title":"Conceptual Overview","text":"Architecture Processing Model Core Idea Typical Use Case Lambda Batch + Streaming (dual pipeline) Combine the accuracy of batch with the low latency of real-time processing. Systems needing historical analytics and real-time views. Kappa Streaming only Treat all data as a stream; reprocess by replaying logs. Event-driven systems with continuous data input. Lakehouse Unified analytical storage Merge Data Warehouse reliability with Data Lake flexibility. Mixed BI + ML workloads that require governance and scalability."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#pipeline-structure-comparison","title":"Pipeline Structure Comparison","text":"Layer / Component Lambda Kappa Lakehouse Data Ingestion Streaming + batch imports Streaming only Batch and streaming supported Storage Model Distributed file system + serving DB Distributed log + serving DB Data Lake with transactional table format Processing Engine Batch engine + stream engine Single stream engine Single unified engine (Spark, Trino, etc.) Query Access Unified Serving Layer Materialized stream views Direct table queries (ACID tables)"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#complexity-and-maintainability","title":"Complexity and Maintainability","text":"Aspect Lambda Kappa Lakehouse Implementation Complexity High (two code paths) Medium (one processing model) Medium (metadata layer required) Operational Overhead High Moderate Moderate to low (depending on platform) Code Duplication Yes No No Data Governance Weak unless explicitly implemented Weak unless added Strong (built into table formats)"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#scalability-and-performance-characteristics","title":"Scalability and Performance Characteristics","text":"Metric Lambda Kappa Lakehouse Scalability Very high (distributed batch) Very high (distributed streaming) Very high (cloud-native data lake storage) Latency Low (via speed layer) Very low Depends on query engine Historical Reprocessing Batch recomputation Event log replay ACID time travel / versioned tables Cost Efficiency Can be expensive Cost-efficient if log retention is optimized Highly cost-efficient, especially on cloud object storage"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#technology-ecosystem-examples","title":"Technology Ecosystem Examples","text":"Component Lambda Kappa Lakehouse Storage HDFS + Serving DB Kafka (log) + DB S3 / HDFS + Delta/ Iceberg / Hudi Processing Spark + Flink/Storm Kafka Streams / Flink Spark / Trino / Databricks Query Engine Presto / Hive / Elasticsearch Elasticsearch / Cassandra Spark SQL / Trino / Snowflake-style engines"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#selection-guidelines","title":"Selection Guidelines","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#choose-lambda-when","title":"Choose Lambda when:","text":"<ul> <li>Both historical accuracy and low-latency results are required.  </li> <li>Data quality and replayability are critical.  </li> <li>Operational complexity is acceptable.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#choose-kappa-when","title":"Choose Kappa when:","text":"<ul> <li>The system processes continuous event streams.  </li> <li>Real-time processing is the main requirement.    </li> <li>Reprocessing can be done through log replay.    </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#choose-lakehouse-when","title":"Choose Lakehouse when:","text":"<ul> <li>You need to support BI dashboards and ML models on the same data.  </li> <li>Data governance, schema enforcement, and auditability matter.    </li> <li>You want to avoid pipeline duplication and uncontrolled data lakes.   </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#summary-diagram-conceptual","title":"Summary Diagram (Conceptual)","text":"<pre><code>                   +------------------------+\n                   |       DATA SOURCES      |\n                   +-----------+-------------+\n                               |\n                               v\n+-------------------+   +-------------------+   +--------------------+\n|     Lambda        |   |      Kappa        |   |     Lakehouse      |\n+-------------------+   +-------------------+   +--------------------+\n| Batch + Streaming |   | Streaming Only    |   | Unified Lake + WH  |\n| Dual pipelines    |   | Single code path  |   | ACID + Governance  |\n+-------------------+   +-------------------+   +--------------------+\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/comparison/#8-practical-recommendation-for-most-modern-systems","title":"8. Practical Recommendation for Most Modern Systems","text":"<p>When building new data platforms today, Lakehouse is generally the preferred architecture because it simplifies pipelines, reduces operational burden, and supports both analytics and machine learning under a unified model.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/","title":"Fundamentals for Big Data Architectures","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#motivation-and-challenges-in-big-data","title":"Motivation and Challenges in Big Data","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#the-data-explosion","title":"The Data Explosion","text":"<p>In recent years, there has been a dramatic increase in the amount of data generated from digital services, social media, Internet of Things (IoT) devices, enterprise applications, scientific computing, and automation systems. This phenomenon, often referred to as the data deluge, has surpassed the processing, storage, and analytical capabilities of traditional computing infrastructures.</p> <p>This growth is exponential, influenced by: - Continuous digitalization of business processes. - Proliferation of connected devices and sensors. - Increased need for real-time monitoring and decision-making. - Generation of detailed logs for observability and auditing.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#the-v-characteristics-of-big-data","title":"The \u201cV\u201d Characteristics of Big Data","text":"<p>Big Data is commonly described using five core properties:</p> V Description Example Volume Massive amount of data Millions of transactions or user events Velocity Speed at which data is generated and processed Real-time analytics or fraud detection Variety Different data formats and structures Structured tables, JSON, images, logs Veracity Reliability and quality of data Noisy or incomplete sensor readings Value Ability to extract useful knowledge Predictive maintenance, business insights"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#limitations-of-traditional-systems","title":"Limitations of Traditional Systems","text":"<p>Traditional database and processing architectures rely on: </p> <ul> <li>A single centralized server.  </li> <li>Local storage on disk.  </li> <li>Sequential or limited parallel processing.  </li> </ul> <p>These systems face several issues when scaling: </p> <ul> <li>Performance bottlenecks in CPU, memory, and I/O.  </li> <li>Vertical scalability limits (hardware upgrades are costly and reach physical limits).  </li> <li>Single points of failure, reducing reliability.  </li> <li>Inability to efficiently handle unstructured data or real-time workloads.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#need-for-new-architectural-approaches","title":"Need for New Architectural Approaches","text":"<p>To address these challenges, Big Data systems are designed to: </p> <ul> <li>Distribute data across many nodes.  </li> <li>Process tasks in parallel.  </li> <li>Scale horizontally by adding more machines when demand increases.  </li> </ul> <p>This shift requires both new computational models and new storage strategies, which are covered in the next sections.  </p> Architecture Common Blocks <p>Data ingestion:The process of collecting and importing data from various sources into a system where it can be stored, processed, and analyzed.</p> <p>Storage (data lakes, distributed systems)</p> <p>Processing (batch and/or streaming)</p> <p>Access, querying, and analytics</p> <p>Governance and metadata:The set of processes and information that ensure data is properly managed, understood, secured, and used consistently across the organization.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#fundamentals-of-distributed-computing","title":"Fundamentals of Distributed Computing","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#what-is-a-distributed-architecture","title":"What Is a Distributed Architecture?","text":"<p>A distributed computing architecture is a system where multiple networked machines (nodes) cooperate to perform computational tasks as if they were a single logical system. Each node typically contributes: - CPU resources - Memory - Storage - Network communication capabilities  </p> <p>The goals are:</p> <ul> <li>Scalability </li> <li>Fault tolerance </li> <li>High throughput </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#models-of-parallel-and-distributed-computation","title":"Models of Parallel and Distributed Computation","text":"Model Description Suitability for Big Data SMP (Symmetric Multiprocessing) Multiple processors share the same memory. Limited scalability; vertical scaling only. MPP (Massively Parallel Processing) Each node has its own CPU and memory, and processes partitions in parallel. Highly suitable for distributed analytics. COW (Cluster of Workstations) Independent machines connected by a network form a compute cluster. Foundation of Hadoop and Spark clusters. DSM (Distributed Shared Memory) Distributed memory is abstracted as a single shared space. Complex to implement at large scale."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#concurrency-vs-parallelism","title":"Concurrency vs Parallelism","text":"<ul> <li>Concurrency refers to multiple tasks being managed at the same time.</li> <li>Parallelism refers to tasks being processed simultaneously.</li> </ul> <p>Big Data processing aims to maximize parallelism by distributing both data and computation across nodes.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#latency-and-throughput","title":"Latency and Throughput","text":"Metric Goal Typical Use Case Latency Reduce response time Real-time monitoring and alerts Throughput Maximize total processed data Batch analytical jobs <p>Batch systems focus on throughput; streaming systems focus on latency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#amdahls-law-and-scalability-limits","title":"Amdahl\u2019s Law and Scalability Limits","text":"<p>Amdahl\u2019s Law states that the maximum performance improvement of a system by parallelization is limited by the portion of the process that cannot be parallelized. This implies that: - Adding more nodes does not always yield proportional speedups. - Efficient Big Data systems require algorithms designed for distributed execution.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#virtualization-and-containers","title":"Virtualization and Containers","text":"Technology Virtualizes Advantages Examples Hypervisor-based virtualization Hardware Strong isolation VMware, KVM Containers Operating system Fast startup, resource efficiency Docker, Podman <p>Containers enable reproducibility, portability, and lightweight deployment.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#container-orchestration-with-kubernetes","title":"Container Orchestration with Kubernetes","text":"<p>Key concepts:</p> <ul> <li>Pod: Minimum deployable execution unit.  </li> <li>Node: A machine (virtual or physical) in the cluster.  </li> <li>Cluster: Set of coordinated nodes.  </li> <li>HPA (Horizontal Pod Autoscaler): Scales the number of Pod replicas.  </li> <li>VPA (Vertical Pod Autoscaler): Adjusts CPU/memory per Pod.  </li> </ul> <p>Kubernetes is widely used to deploy and scale distributed data platforms.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#cloud-computing-models","title":"Cloud Computing Models","text":"Model Level of Abstraction Examples IaaS (Infrastructure as a Service) Virtual machines and networks AWS EC2, Google Compute Engine PaaS (Platform as a Service) Managed execution environments AWS Lambda, Google App Engine SaaS (Software as a Service) Complete applications Salesforce, Office 365 <p>Cloud environments are now the standard for scaling Big Data workloads dynamically.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#fundamentals-of-scalable-storage-and-databases","title":"Fundamentals of Scalable Storage and Databases","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#challenges-of-traditional-storage","title":"Challenges of Traditional Storage","text":"<p>Single-server storage suffers from: - Limited capacity and performance - High expansion cost - Risk of catastrophic data loss  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#in-memory-databases","title":"In-Memory Databases","text":"<p>In-memory databases store data in RAM to reduce latency and accelerate query processing.</p> <p>Advantages - Extremely fast read and write operations.</p> <p>Disadvantages - RAM is volatile \u2192 persistent logging to disk is required.</p> <p>Examples: Redis, Hazelcast, SAP HANA.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#row-oriented-vs-column-oriented-storage","title":"Row-Oriented vs Column-Oriented Storage","text":"Model Best suited for Workload type Examples Row-store Transactional systems OLTP PostgreSQL, MySQL Column-store Analytical processing OLAP Parquet, ORC, Vertica"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#persistence-logging-and-snapshots","title":"Persistence: Logging and Snapshots","text":"<ul> <li>Snapshots periodically store a full copy of the in-memory state.</li> <li>Write-ahead logs capture operations sequentially for recovery.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#distributed-file-systems","title":"Distributed File Systems","text":"System Architecture Scalability Suitable for Big Data NFS (Network File System) Centralized Limited No HDFS (Hadoop Distributed File System) Replicated block storage High Yes Object Storage (e.g., S3, GCS, Azure Blob) Flat namespace with HTTP API Very high Yes"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/fundamentals/#fault-tolerance-and-replication","title":"Fault Tolerance and Replication","text":"<p>Distributed storage systems implement: - Data replication across multiple nodes. - Automatic rebalancing when nodes fail.  </p> <p>These mechanisms are essential for reliability in Big Data environments.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/","title":"Additive vs. Multiplicative Costs in Graphs","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#1-additive-cost","title":"1. Additive Cost","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#definition","title":"Definition","text":"<p>In an additive cost, the total value of a path is the sum of the individual costs of each edge:</p> <p>Total Cost (A \u2192 C) = A_AB + A_BC</p> <p>This model is used when each step adds an independent cost, such as: - Distance traveled - Execution time - Energy consumed - Accumulated price  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#example","title":"Example","text":"<p>Imagine a graph where nodes represent cities and edge weights are distances (in km):</p> Route Cost (km) A \u2192 B 5 B \u2192 C 7 A \u2192 C \u2014 <p>Then, the total additive cost from A to C via B is:</p> <p>Cost (A \u2192 B \u2192 C) = 5 + 7 = 12</p> <p>If there are multiple paths, we choose the one with the minimum total sum using algorithms such as Dijkstra or Floyd\u2013Warshall.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#2-multiplicative-cost","title":"2. Multiplicative Cost","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#definition_1","title":"Definition","text":"<p>In a multiplicative cost, the total value of a path is the product of the individual costs (or probabilities, or reliability factors) of each edge:</p> <p>Total Cost (A \u2192 C) = A_AB \u00d7 A_BC</p> <p>This is used when each step transforms or attenuates the effect of the previous one, such as: - Success probabilities - Transmission coefficients (signal loss or energy) - Relationship strength or affinity - Growth or decay factors  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#example_1","title":"Example","text":"<p>Suppose edge weights represent probabilities of success:</p> Route Probability A \u2192 B 0.9 B \u2192 C 0.8 A \u2192 C \u2014 <p>Then, the probability of reaching C from A through B is:</p> <p>P(A \u2192 B \u2192 C) = 0.9 \u00d7 0.8 = 0.72</p> <p>Each step reduces the overall probability of success.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#3-comparison","title":"3. Comparison","text":"Concept Additive Cost Multiplicative Cost Operation Sum (+) Product (\u00d7) Typical Context Distance, time, energy Probability, reliability, intensity Total Behavior Increases with each step Decreases or amplifies Example Algorithms Dijkstra, Floyd\u2013Warshall Markov models, probabilistic propagation"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/additive_vs_multiplicative/#4-graphical-interpretation","title":"4. Graphical Interpretation","text":"<p>Graph 1 (Additive \u2013 distances): A \u2014(5)\u2014 B \u2014(7)\u2014 C \u2192 total cost = 12  </p> <p>Graph 2 (Multiplicative \u2013 probabilities): A \u2014(0.9)\u2014 B \u2014(0.8)\u2014 C \u2192 total cost = 0.72  </p> <p>In the first case, traveling becomes more expensive with each step. In the second, success becomes less likely with each step.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/","title":"Applications - Complexity Management","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#application-of-matrix-multiplication-to-graph-theory","title":"Application of Matrix Multiplication to Graph Theory","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#introduction","title":"Introduction","text":"<p>\"Programming is usually taught by examples\" \u2014 Niklaus Wirth (1934-2024) </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#brief-biography","title":"Brief Biography","text":"<p>Niklaus Wirth was an influential computer scientist, creator of programming languages such as Pascal, Modula, and Oberon. He received the Turing Award in 1984 for his contributions to programming language design and software methodologies. His teaching approach always emphasized the use of practical examples to explain complex concepts.</p> <p>Matrix multiplication plays a crucial role in many Big Data applications, especially when data is modeled as graphs or networks. Graphs allow us to represent and analyze massive, interconnected datasets in areas such as social networks, biological systems, computer networks, and recommendation systems. By leveraging matrix multiplication, we can efficiently compute paths, connectivity, influence, and ranking in graphs \u2014 operations that are fundamental to large-scale data analysis. In Big Data, where datasets may include millions of nodes and billions of connections, matrix-based approaches provide a scalable mathematical framework for tasks such as link prediction, shortest path discovery, clustering, and random walks.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#applications-of-graphs-across-domains","title":"Applications of graphs across domains","text":"<ul> <li>Social Networks: Nodes represent users, and edges represent friendships or follower relationships.  Example</li> <li>Transportation Networks: Nodes are locations (cities, airports), and edges are routes (roads, flights). Example </li> <li>Web Pages: Nodes are web pages, and edges are hyperlinks between them. Example </li> <li>Computer Networks: Nodes are devices, and edges are communication links.  Example </li> <li>Biological Networks: Nodes are genes or proteins, and edges are interactions or regulatory relationships.  Example </li> <li>Recommendation Systems: Nodes represent users and items, with edges indicating preferences or ratings.  Example </li> <li>Knowledge Graphs: Nodes represent entities (people, places, things), and edges represent semantic relationships. Example </li> <li>Scheduling Problems: Nodes represent tasks, and edges represent dependencies. Example </li> </ul> <p>Example </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#social-networks","title":"Social Networks","text":"<p>Efficient Relationship Modelling: Graphs can naturally represent social connections and quickly identify relationships like mutual friends or groups. Pathfinding: Algorithms like Breadth-First Search (BFS) can find the shortest path between two people, useful for \"degrees of separation.\"</p> <p></p> <p>Social Network</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#transportation-networks","title":"Transportation Networks","text":"<p>Optimal Path Finding: Algorithms like Dijkstra's or A(asterisk) can find the shortest or fastest route between locations. Network Analysis: Graphs make it easy to analyze connectivity (e.g., detecting if certain cities are isolated).</p> <p></p> <p>Transportation Network</p> <p></p> <p>Transportation Network</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#web-pages","title":"Web Pages","text":"<p>Link Analysis: Algorithms such as PageRank use the graph structure of the web to rank pages based on the number of incoming and outgoing links. Crawling and Indexing: Graph traversal techniques help search engines crawl the web efficiently.</p> <p></p> <p>This visualization represents the network of web pages connected to voson.anu.edu.au obtained by a web crawl, modified from Fig. 12.9 of the NodeXL Book [27, p. 192].</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#computer-networks","title":"Computer Networks","text":"<p>Routing Algorithms: Graphs enable efficient packet routing by finding the shortest path in a network. Failure Detection: It helps identify potential points of failure and improve the robustness of the network.</p> <p></p> <p>Transportation Network</p> <p></p> <p>Transportation Network detail</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#biological-networks","title":"Biological Networks","text":"<p>Pathway Analysis: Graphs model complex biological pathways, helping to identify critical interactions in gene regulation or protein networks. Community Detection: Helps identify clusters or communities of related genes or proteins, useful in understanding functional modules.</p> <p></p> <p>Biological Network</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#recommendation-systems","title":"Recommendation Systems","text":"<p>User-Item Relationship Modelling: Graphs can represent complex relationships between users and items, enabling collaborative filtering. Graph-based Ranking: Algorithms can rank items based on their connections to other highly-rated items.</p> <p></p> <p>Recommendation System Network</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#knowledge-graphs","title":"Knowledge Graphs","text":"<p>Semantic Search: Allows for more natural language search by understanding relationships between entities. Reasoning and Inference: Graph algorithms can help infer new knowledge based on existing relationships.</p> <p></p> <p>Knowledge Graph</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#scheduling-problems","title":"Scheduling Problems","text":"<p>Dependency Resolution: Graphs can represent task dependencies, making it easier to identify the correct order of execution. Cycle Detection: Can detect circular dependencies that would prevent proper scheduling.</p> <p></p> <p>Tasks Graph representation</p>  Graph analisys <p>Besides paths and influence, graph analysis usually includes:</p> <p> Communities (natural groups)</p> <p> Centrality (ways of measuring importance)</p> <p> Anomalies (detecting unusual nodes)</p> <p> Temporal evolution (network dynamics)</p> <p> Diffusion/propagation (information, viruses, etc.)</p> <p> Patterns and motifs (recurring local structures)</p> <p> Matching (users\u2013items, resources\u2013tasks)</p> <p> Robustness (what happens if nodes/edges fail)</p> <p>Exercises</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/","title":"Length of Paths","text":"<p>In graph theory, one of the most interesting questions we can ask is: How many different ways are there to go from one node to another in exactly k steps? This concept is fundamental in Big Data applications, where we often want to explore connectivity in very large networks such as social graphs, transportation systems, or biological networks.</p> <p>The tool that allows us to answer this question is the adjacency matrix of a graph and its powers. By multiplying the adjacency matrix by itself, we can discover not just direct connections, but also paths of longer lengths.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#explanation","title":"Explanation","text":"<ul> <li>Let A be the adjacency matrix of a graph with nodes ( V = {v_1, v_2, ..., v_n} ).  </li> <li>The entry ( A_{ij} = 1 ) if there is an edge from node ( v_i ) to node ( v_j ), and 0 otherwise.  </li> <li>When we compute ( A^2 ), the entry ( (A^2)_{ij} ) gives the number of paths of length 2 between ( v_i ) and ( v_j ).  </li> <li>More generally, ( (A^k)_{ij} ) tells us the number of paths of length k from ( v_i ) to ( v_j ).  </li> </ul> <p>This makes adjacency matrix powers a very powerful tool for analyzing networks in Big Data: with a single matrix operation we can count millions of possible connections!</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#example","title":"Example","text":"<p>Consider a small graph with 6 nodes: A, B, C, D, E, F. Suppose we want to know the number of different paths from A to F.</p> <ul> <li>1-step path: directly from A to F (if the edge exists).  </li> <li>2-step path: A \u2192 B \u2192 F.  </li> <li>3-step path: A \u2192 B \u2192 D \u2192 F.  </li> <li>5-step path: A \u2192 B \u2192 C \u2192 E \u2192 B \u2192 F.  </li> </ul> <p>Mathematically, these are obtained from the adjacency matrix A: - From A itself, we see all direct edges (paths of length 1). - From A\u00b2, we get all paths of length 2. - From A\u00b3, we get all paths of length 3.  </p> <p>This illustrates how matrix multiplication helps us uncover paths of arbitrary length in a graph, which is extremely useful for analyzing reachability in large-scale networks.</p> <p> </p> \ud83d\udca1 Specific problem: <p>  Detecting users that can be influenced in a limited number of steps.</p> <p>Suppose a user A publishes an information (news, rumour or advertising product), and we want to know which users can be reached through interactions in exactly n steps. For example, we want to see who could receive the information if it is shared among friends up to three times (i.e. in paths of length 3). </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#repeated-squaring-and-complexity","title":"Repeated Squaring and Complexity","text":"<p>To compute shortest paths efficiently in large networks, we need more than a naive approach. Repeated squaring is a powerful method that reduces the number of computations while still giving us exact results.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#explanation_1","title":"Explanation","text":"<ul> <li>The idea: instead of computing powers sequentially, we square the matrix repeatedly.  </li> <li>Formally:  </li> <li>( W^2 = W \\otimes W ) (using min-plus multiplication).  </li> <li>( W^4 = W^2 \\otimes W^2 ).  </li> <li>Continue until ( W^{2^{\\lceil \\log(n-1) \\rceil}} ).  </li> <li>This algorithm has a time complexity of ( O(n^3 \\log n) ), which is much more efficient than naive approaches for large graphs.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#example_1","title":"Example","text":"<p>For a given weight matrix W, repeated squaring allows us to quickly discover shortest paths between all pairs of nodes, even in very large networks.</p> <p>ag_4, ag_5 y ag_6</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#random-walks-and-markov-chains","title":"Random Walks and Markov Chains","text":"<p>Sometimes, we are not interested in deterministic paths but in probabilistic movement across a network. This is where random walks and Markov chains come into play, both of which can be described using matrix multiplication.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#explanation_2","title":"Explanation","text":"<ul> <li>A random walk is a process where, at each step, we move to a random neighbor of the current node.  </li> <li>This can be represented by a transition matrix T, where each entry ( T_{ij} ) gives the probability of moving from node i to node j.  </li> <li>By computing ( T^n ), we find the probabilities of being in each node after n steps.  </li> <li>Over time, this leads to a stationary distribution, which tells us the long-term behavior of the system.</li> </ul> <p>This framework is fundamental in PageRank, recommendation systems, and modeling diffusion in social and biological networks.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#example_2","title":"Example","text":"<p>Consider a graph with nodes A\u2013F. The transition matrix T encodes the probability of moving between nodes. - If we start at node A and compute ( T^n ) for a large n, we obtain a vector that represents the probability of being at each node after many steps.</p> <p>ag_7</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#case-study-casino-random-walk","title":"Case Study: Casino Random Walk","text":"<p>To better understand random walks, let\u2019s look at a simple but illustrative example: the casino.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications_graphs_extended/#explanation_3","title":"Explanation","text":"<ul> <li>Imagine a player who always bets $1.  </li> <li>The probability of winning or losing is equal.  </li> <li>This can be modeled as a random walk on a line graph, where each node represents the player\u2019s wealth at a given time.  </li> <li>As the game evolves, the player\u2019s wealth moves left (loss) or right (win) along the line.  </li> </ul> <p>This model allows us to study important aspects of the game, such as the probability distribution of the player\u2019s wealth after a certain number of bets, or the risk of eventual ruin.</p> <p>ag_8</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/","title":"Exercises on Big-O Notation","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-1-mapping-the-complexity-landscape","title":"Variant 1 \u2014 Mapping the Complexity Landscape","text":"<p>Goal. Empirically identify the time complexity of several algorithms using synthetic datasets of increasing size, and contrast it with the theoretical complexity.</p> <p>Tasks. 1. Choose three different complexity classes (O(1), O(log n), O(n), O(n log n), etc.). 2. Measure execution times for increasing input sizes using <code>System.nanoTime()</code>. 3. Plot results and normalize (<code>time/n</code>, <code>time/n log n</code>, etc.) to recognize the class. 4. Write a report for each algorithm with hypothesis, evidence, and discussion.</p> <p>Deliverables. - Report (4\u20136 pages) with plots and analysis. - Code with measurement scripts.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-2-scaling-limits-in-practice","title":"Variant 2 \u2014 Scaling Limits in Practice","text":"<p>Goal. Explore the practical limits of different algorithmic complexities by running experiments until execution times become impractical, and compare results to theoretical expectations.</p> <p>Tasks. 1. Select three complexity classes (e.g., O(n), O(n log n), O(n\u00b2)). 2. Define a \u201ctime budget\u201d (e.g., 5 seconds per experiment). 3. Increase input size <code>n</code> step by step until execution exceeds the budget. 4. Record the maximum feasible <code>n</code> for each algorithm and compare with theoretical growth. 5. Discuss discrepancies between theoretical predictions and practical limits (e.g., due to JVM, caching, or hardware).  </p> <p>Deliverables. - Report (3\u20135 pages) with tables of <code>n_max</code> values and discussion. - Code with input scaling scripts.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-3-comparing-growth-rates-on-the-same-plot","title":"Variant 3 \u2014 Comparing Growth Rates on the Same Plot","text":"<p>Goal. Visually demonstrate differences in algorithmic growth rates by plotting multiple complexity classes together for the same input range.  </p> <p>Tasks. 1. Select at least four algorithms with distinct complexities (e.g., O(1), O(log n), O(n), O(n\u00b2)). 2. Measure execution times for the same sequence of input sizes. 3. Plot all results on the same chart (possibly log-log scale for clarity). 4. Highlight where one complexity \u201covertakes\u201d another (e.g., O(n\u00b2) becomes much slower than O(n log n)). 5. Provide an interpretation that connects plots to theoretical curves.  </p> <p>Deliverables. - Report (4\u20136 pages) with combined plots and written interpretation. - Annotated code to reproduce graphs.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#common-notes","title":"Common Notes","text":"<ul> <li>Use definitions of Big O notation, best/worst/average cases.</li> <li>Always explain what your analysis counts (iterations, comparisons, assignments).</li> <li>Differentiate time and space when relevant.</li> <li>(Optional) Use JMH for micro-benchmarks in Java.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/","title":"Example: Adjacency Matrix Multiplication in a Biological Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#scenario","title":"Scenario","text":"<p>Consider a biological network where nodes represent genes (or proteins) and edges represent regulatory or interaction relationships. We have four genes: A, B, C, and D, with the following known interactions:</p> <ul> <li>A \u2192 B: Gene A activates gene B  </li> <li>B \u2192 C: Gene B activates gene C  </li> <li>C \u2192 D: Gene C activates gene D  </li> <li>A \u2192 D: Gene A also activates gene D directly  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 0 C 0 0 0 1 D 0 0 0 0 <p>Each row represents the regulating gene (source) and each column represents the regulated gene (target). A value of 1 means there is a direct regulatory interaction between two genes.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The matrix A\u00b2 shows indirect (two-step) interactions \u2014 which genes influence others through an intermediate regulator.</p> A B C D A 0 0 1 0 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 Gene A indirectly influences C through B (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 Gene B indirectly influences D through C (B \u2192 C \u2192 D).  </li> <li>A\u00b2\u208dA,D\u208e = 0 \u2192 Although A directly regulates D, there is no distinct two-step pathway (already covered in A).  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#biological-insights","title":"Biological Insights","text":"<ul> <li>Indirect or cascade regulation: Detect genes that affect others through intermediate regulators (e.g., signaling pathways).  </li> <li>Signal propagation: Analyze how activation of one gene can spread through the network over multiple steps.  </li> <li>Hub genes: Identify genes with high connectivity in A or A\u00b2 \u2014 potential master regulators or essential genes.  </li> <li>Network redundancy: If A\u00b2\u208di,j\u208e &gt; 1, multiple regulatory paths exist between i and j, suggesting robustness in gene regulation.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/biological_network/#extensions","title":"Extensions","text":"<ul> <li>A\u00b3 \u2192 shows interactions involving three steps (A \u2192 B \u2192 C \u2192 D).  </li> <li>(I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 total reachability or accumulated influence of a gene across the network.  </li> <li>Weighted adjacency matrices \u2192 edge weights can represent interaction strength, affinity, or probability of binding.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/","title":"Ejemplos","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#velocidad-de-ejecucion","title":"Velocidad de ejecuci\u00f3n","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#python","title":"Python","text":"<pre><code>import random\nfrom time import *\n\nn = 1024\n\nA = [[random.random() for _ in range(n)] for _ in range(n)]\nB = [[random.random() for _ in range(n)] for _ in range(n)]\nC = [[0 for _ in range(n)] for _ in range(n)]\n\nstart = time()\nfor i in range(n):\n    for j in range(n):\n        for k in range(n):\n            C[i][j] += A[i][k] * B[k][j]\n\nend = time()\n\nprint(\"%.6f\" % (end - start))\n\n# Python. Runing time aroud: 409 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#java","title":"Java","text":"<pre><code>import java.util.Random;\n\npublic class Matrix {\n\n    static int n = 1024;\n    static double[][] a = new double[n][n];\n    static double[][] b = new double[n][n];\n    static double[][] c = new double[n][n];\n\n    public static void main(String[] args) {\n        Random random = new Random();\n        for (int i = 0; i &lt; n; i++) {\n            for (int j = 0; j &lt; n; j++) {\n                a[i][j] = random.nextDouble();\n                b[i][j] = random.nextDouble();\n                c[i][j] = 0;\n            }\n        }\n\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; n; i++) {\n            for (int j = 0; j &lt; n; j++) {\n                for (int k = 0; k &lt; n; k++) {\n                    c[i][j] += a[i][k] * b[k][j];\n                }\n            }\n        }\n        long stop = System.currentTimeMillis();\n\n        System.out.println((stop - start) * 1e-3);\n    }\n}\n\n// Java. Runing time aroud: 7.76 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#rust","title":"Rust","text":"<pre><code>use std::time::SystemTime;\nuse rand::Rng;\n\nfn main() {\n    let n: usize = 1024;\n    let mut a: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n    let mut b: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n    let mut c: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n\n    let mut rng = rand::thread_rng();\n    for i in 0..n {\n        for j in 0..n {\n            a[i][j] = rng.gen::&lt;f64&gt;();\n            b[i][j] = rng.gen::&lt;f64&gt;();\n        }\n    }\n\n    let start: SystemTime = SystemTime::now();\n    for i in 0..n {\n        for j in 0..n {\n            for k in 0..n {\n                c[i][j] += a[i][k] * b[k][j];\n            }\n        }\n    }\n    let elapsed: Result&lt;std::time::Duration, std::time::SystemTimeError&gt; = start.elapsed();\n\n    println!(\"Elapsed: {:.2?}\", elapsed);\n}\n\n// Rust. Runing time aroud: 7.91 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#c","title":"C","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;sys/time.h&gt;\n\n#define n 1024\ndouble a[n][n];\ndouble b[n][n];\ndouble c[n][n];\n\nstruct timeval start, stop;\n\nint main() {\n    for (int i = 0; i &lt; n; ++i) {\n        for (int j = 0; j &lt; n; ++j) {\n            a[i][j] = (double) rand() / RAND_MAX;\n            b[i][j] = (double) rand() / RAND_MAX;\n            c[i][j] = 0;\n        }\n    }\n\n    gettimeofday(&amp;start, NULL);\n    for (int i = 0; i &lt; n; ++i) {\n        for (int j = 0; j &lt; n; ++j) {\n            for (int k = 0; k &lt; n; ++k) {\n                c[i][j] += a[i][k] * b[k][j];\n            }\n        }\n    }\n    gettimeofday(&amp;stop, NULL);\n\n    double diff = stop.tv_sec - start.tv_sec\n                + 1e-6 * (stop.tv_usec - start.tv_usec);\n    printf(\"%0.6f\\n\", diff);\n\n    return 0;\n}\n/* C. Runing time aroud: 0.677867 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/","title":"Example: Adjacency Matrix Multiplication in a Computer Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#scenario","title":"Scenario","text":"<p>Consider a network of four devices: A (host), B (switch), C (router-1), and D (router-2). The edges are directed communication links (row = source, column = destination).</p> <p>Direct links: - A \u2192 B - B \u2192 C, B \u2192 D - C \u2192 D - D \u2192 (no outgoing links)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 0 B 0 0 1 1 C 0 0 0 1 D 0 0 0 0 <p>Each row represents the source device, and each column represents the destination device. A value of 1 indicates a direct communication link between two devices.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The matrix A\u00b2 shows how many two-hop communication paths exist between devices.</p> A B C D A 0 0 1 1 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 Device A can reach C in two hops (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dA,D\u208e = 1 \u2192 Device A can reach D in two hops (A \u2192 B \u2192 D).  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 Device B can reach D in two hops (B \u2192 C \u2192 D).  </li> <li>Rows with all zeros (like D) indicate sink nodes \u2014 devices that do not forward traffic.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#possible-analyses","title":"Possible Analyses","text":"<ul> <li>Two-hop reachability: Identify which devices are reachable within two transmissions (useful for TTL, latency estimation, or broadcast domain boundaries).  </li> <li>Critical relay nodes: Columns with nonzero entries (e.g., C and D) reveal which devices act as communication relays \u2014 their failure could break the network.  </li> <li>Routing and segmentation: Determine which destinations require multiple hops to optimize routing or add redundancy.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/computer_network/#extensions","title":"Extensions","text":"<ul> <li>A\u00b3 \u2192 identifies devices reachable in three hops (additional relays).  </li> <li>(I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 cumulative reachability up to k hops.  </li> <li>Weighted adjacency matrices \u2192 represent bandwidth, loss, or latency to find the most efficient routes.  </li> <li>Redundancy analysis \u2192 if A\u00b2\u208di,j\u208e &gt; 1, multiple two-hop paths exist between i and j.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/","title":"Experiments \u2013 Complexity Management","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#introduction","title":"Introduction","text":"<p>\u201cThere are a lot of ways known to do it wrong and which one is right is not clear.\u201d \u2014 James Gosling (1955\u2013)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#brief-biography","title":"Brief Biography","text":"<p>James Gosling, born on May 19, 1955, in Calgary, Canada, is a computer scientist widely known as the \u201cfather of Java.\u201d He studied computer science at the University of Calgary and later earned a Ph.D. in computer science from Carnegie Mellon University. Gosling joined Sun Microsystems in the 1980s, where he led the development of the Java programming language, officially released in 1995. Java quickly became one of the most influential programming languages in the world due to its portability, security, and \u201cwrite once, run anywhere\u201d philosophy. Beyond Java, Gosling has contributed to compiler design, operating systems, and software development tools. He has worked at several leading technology companies, including Sun Microsystems, Oracle, Google, and Amazon Web Services.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#the-creation-of-java","title":"The Creation of Java","text":"<p>In the early 1990s, Gosling and his team at Sun Microsystems initiated the \u201cGreen Project\u201d to develop a language suitable for consumer devices and embedded systems. Gosling designed Java with a strong emphasis on simplicity, object-oriented design, memory management, and platform independence. The language introduced features like automatic garbage collection and a virtual machine (the JVM) that allowed programs to run across different hardware and operating systems. Java\u2019s release in 1995 revolutionized programming by becoming the backbone of enterprise applications, web development, and later Android applications. Today, Java remains one of the most widely used and enduring programming languages, a testament to Gosling\u2019s vision and leadership.</p> <p>Complexity in computation is not only about algorithms but also about implementation choices, programming languages, and hardware utilization. This experiment explores matrix multiplication as a case study to analyze performance differences across programming languages and approaches.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#reminder-matrix-multiplication","title":"Reminder: Matrix Multiplication","text":"<p>If A and B are <code>n \u00d7 n</code> matrices, their product C = AB is also an <code>n \u00d7 n</code> matrix. Each element of the resulting matrix is obtained by combining a row of A with a column of B.</p> <p>Formally:</p> <p></p> <p>[ C[i,j] = \\sum_{k=1}^n A[i,k] \\cdot B[k,j] ]</p> <p>This operation is fundamental in scientific computing, graphics, and machine learning, but it is also computationally intensive, requiring O(n\u00b3) operations in its na\u00efve form.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#algorithm-naive-approach","title":"Algorithm (Na\u00efve Approach)","text":"<p>Pseudocode of the classic algorithm:</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#python","title":"Python","text":"<p>Python Example</p> <p></p> <p>Running time 409.45 seconds with 1024 x 1024 matrices</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#rust","title":"Rust","text":"<p>Rust Example</p> <p></p> <p>Running time 7.91 second with 1024 x 1024 matrices</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#java","title":"Java","text":"<p>Java Example</p> <p></p> <p>Running time 7.76 seconds with 1024 x 1024 matrices 52x faster than python</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#c","title":"C","text":"<p>C Example</p> <p></p> <p>Running time 0.677867 seconds with 1024 x 1024 matrices</p> <p>11x faster than java</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#tiobe-programming-community-index","title":"TIOBE Programming Community Index","text":"<p>The image represents the TIOBE Programming Community Index, which measures the relative popularity of programming languages over time. It does not indicate the best language but rather the most used and most visible in the global developer community, based on search engines, courses, and vendors.</p> \ud83d\udca1 Details <p>Java (green): Dominated the early 2000s with more than 25% share but has steadily declined, although it remains highly relevant.</p> <p>C (black): Extremely stable and consistently strong, often alternating with Java in the top positions.</p> <p>Python (light blue): Shows explosive growth after 2015, becoming the most popular language since 2020. This reflects the rise of data science, machine learning, and artificial intelligence.</p> <p>C++ (orange): Popular in the early 2000s, now stabilised at around 8\u201310%.</p> <p>C# (dark blue): Grew quickly with the .NET ecosystem in the 2000s and maintains a solid mid-level share.</p> <p>PHP (aqua): Very popular in web development between 2005\u20132010 but declined as JavaScript frameworks and other technologies took over.</p> <p>JavaScript (yellow): Maintains a stable share, though its dominance in web applications is not fully reflected in TIOBE\u2019s methodology.</p> <p>Other languages (SQL, Assembly, Visual Basic, etc.): Remain present in niche applications.</p> \ud83d\udca1 Conclusions <p>C and Java were the long-time leaders of the programming world.</p> <p>Python\u2019s meteoric rise illustrates how industry trends (AI, data analytics) can change the landscape of programming.</p> <p>The index shows that language popularity evolves with technological needs, and students should be aware of both long-standing and emerging languages.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#exercise-1-complexity-experiment","title":"EXERCISE 1 Complexity Experiment","text":"<p>Given the matrix multiplication algorithm, how would you optimize the storage and management of the input data to improve the efficiency of the computation? Consider both memory access patterns and the use of specialized data structures.</p> <p>Solution</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#benchmarking-in-complexity-management","title":"Benchmarking in Complexity Management","text":"<p>Benchmarking is the methodology of comparing processes with respect to specific performance measures such as execution time, memory usage, throughput, or scalability.  </p> <p>It allows us to: - Evaluate performance under different conditions. - Compare technologies and frameworks. - Identify bottlenecks in computation. - Optimize resources. - Ensure scalability as systems and data grow.  </p> <p>The benchmarking process generally follows three key steps:</p> <ol> <li>Setup the experiment: Define datasets, algorithms, and parameters.  </li> <li>Execution: Run tests under controlled conditions.  </li> <li>Analysis: Interpret results, compare metrics, and extract insights.  </li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#case-study-examples","title":"Case Study Examples","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#1-performance-evaluation","title":"1. Performance Evaluation","text":"<p>Imagine a company processing large datasets in real time for fraud detection. Using a framework such as Apache Spark, benchmarking helps measure how many transactions per second the system can handle before latency increases. This determines the optimal configuration to maintain performance while minimizing delays.</p> \ud83d\udca1 Proposal <p>Here\u2019s a real-world case where benchmarking helps: fraud detection with Spark.</p> <p>If the system starts to show latency at 50,000 transactions per second, what changes could you propose to improve performance?</p> \ud83d\udca1 Reflexion <p>If the system shows latency at 50,000 transactions per second, you could improve performance by:</p> <p> Scaling horizontally \u2192 add more nodes or executors to increase parallelism.</p> <p> Optimizing resources \u2192 tune memory and CPU allocation.</p> <p> Partitioning the data stream \u2192 add more Kafka/Spark partitions for better distribution.</p> <p> Reducing state size \u2192 use watermarks or windowing to avoid unbounded memory growth.</p> <p> Optimizing the code \u2192 avoid unnecessary shuffles and use efficient libraries.</p> <p>In summary, the goal is to increase parallelism, make better use of resources, and control state growth, so the system can process more transactions without adding latency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#2-technology-comparison","title":"2. Technology Comparison","text":"<p>A research team compares Apache Hadoop and Apache Flink for batch processing. - Hadoop: More efficient for massive, distributed storage and processing. - Flink: Better for real-time analytics. Benchmarking on workloads like social media data helps them select the best tool.</p> \ud83d\udca1 Proposal <p>This is an example of how benchmarking guides technology choice: Hadoop vs Flink.</p> <p>If you had to process social media streams, which system would you benchmark and why?</p> \ud83d\udca1 Reflexion If I had to process social media streams, I would benchmark Apache Flink because it is optimized for real-time and low-latency stream processing. Hadoop is more efficient for large-scale batch processing, but social media data requires continuous analysis, so Flink would likely perform better for this use case."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#3-resource-optimization","title":"3. Resource Optimization","text":"<p>In cloud environments, resources are billed by usage. Benchmarking reveals how different CPU/memory configurations affect the runtime of machine learning tasks. For example, doubling memory but keeping CPU constant may cut processing time by half \u2014 optimizing cost-performance balance.</p> \ud83d\udca1 Proposal <p>This is how benchmarking can guide resource allocation in cloud environments.</p> <p>If doubling memory halves the processing time, would you consider it cost-effective even if memory is twice as expensive as CPU?</p> \ud83d\udca1 Reflexion In order to process social media streams, maybe benchmark Apache Flink is a good choice because it is optimized for real-time and low-latency stream processing. Hadoop is more efficient for large-scale batch processing, but social media data requires continuous analysis, so Flink would likely perform better for this use case."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#4-identifying-bottlenecks","title":"4. Identifying Bottlenecks","text":"<p>In ETL (Extract, Transform, Load) workflows, benchmarking can reveal network transfer speed as the bottleneck between Amazon S3 and Hadoop clusters. This insight guides investment in better infrastructure or alternative transfer methods.</p> \ud83d\udca1 Proposal <p>Benchmarking can reveal whether the bottleneck lies in the network rather than in storage or compute.</p> <p>If your ETL pipeline is slow, how would you use benchmarking to determine whether the problem is CPU, storage, or network?</p> \ud83d\udca1 Reflexion To identify bottlenecks in an ETL pipeline, I would run benchmarks that measure the performance of each stage separately\u2014data transfer, storage, and processing. If the results show that network transfer between storage (e.g., S3) and the processing engine (e.g., Hadoop) is significantly slower than computation or storage access, then the network is the bottleneck. This insight helps decide whether to improve network capacity or adjust data transfer methods."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#5-ensuring-scalability","title":"5. Ensuring Scalability","text":"<p>An online retailer analyzing customer behavior starts with 1M transactions but expects exponential growth. Scalability benchmarks test workloads of 10M, 50M, or 100M transactions to ensure the system scales without performance degradation.</p> \ud83d\udca1 Proposal <p>Scalability benchmarking helps companies anticipate growth and ensure that performance remains stable as data volume increases.</p> <p>If your dataset grew 100 times larger, how would you benchmark your system to check whether it can scale without performance degradation?</p> \ud83d\udca1 Reflexion To benchmark scalability, the dataset size can be gradually increased (e.g., 10x, 50x, 100x) while measuring how the system\u2019s performance changes. If the system maintains acceptable response times and throughput as the data grows, then it scales effectively. Otherwise, the benchmark highlights where improvements in infrastructure or algorithms are required."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#exercise-2-complexity-experiment","title":"EXERCISE 2 Complexity Experiment","text":"<p>Execute the algorithms associated to different methods or programming languages with different datasets (size of the matrix) and extract measures (execution time).</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#optimizing-matrix-multiplication","title":"Optimizing Matrix Multiplication","text":"<p>Students are expected to: 1. Implement algorithms for matrix multiplication (na\u00efve and optimized). 2. Execute them in different languages or frameworks (e.g., Python, Java, C, Rust). 3. Vary dataset size (matrix dimension) and record metrics (execution time, memory usage). 4. Compare results across methods. 5. Draw conclusions about efficiency, scalability, and hardware utilization.</p> <p></p> <p>Comparison of common time complexities (Big-O). The graph shows how algorithm performance scales with input size, from constant time O(1) to exponential O(2^n), highlighting the dramatic differences in growth rates.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#documenting-benchmarks","title":"Documenting Benchmarks","text":"<p>When reporting experiments, follow a structured format:</p> <p>Title - Clear and concise, e.g. Benchmarking Matrix Multiplication Algorithms on Big Data Platforms.  </p> <p>Abstract - Briefly state the task, algorithms compared, performance measures, and conclusions.  </p> <p>Body - Describe setup, methodology, datasets, and tools. - Provide tables or charts (execution times, speedups, scalability curves).  </p> <p>Conclusions - Summarize key findings. - State recommendations for future research or practice.  </p> <p>Example Abstract: We study the behavior of several matrix multiplication algorithms used for large-scale computation. We analyze execution time, memory usage, and scalability. Our experiments provide a reproducible benchmark across a variety of datasets to guide future research in Big Data performance engineering. Based on our results, we recommend combining optimized libraries (BLAS, MKL) with GPU acceleration for the most efficient solutions.</p> <p>Scientific Papers</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/","title":"Exercises on Graphs in Big Data Context","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-1-exploring-graph-representations","title":"Variant 1 \u2014 Exploring Graph Representations","text":"<p>Goal. Compare adjacency matrix and adjacency list representations in terms of memory usage and query efficiency.  </p> <p>Tasks. 1. Build a synthetic social network graph with different densities (sparse vs dense). 2. Store the graph both as an adjacency list and as an adjacency matrix. 3. Measure memory consumption and query time for operations like:    - Checking if two nodes are connected.    - Finding neighbors of a node. 4. Plot results as graph size increases.  </p> <p>Deliverables. - Report (4\u20136 pages) with plots and analysis. - Code that builds both representations and measures performance.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-2-degrees-of-separation-in-social-networks","title":"Variant 2 \u2014 Degrees of Separation in Social Networks","text":"<p>Goal. Empirically measure how the average path length grows in social network graphs, and relate it to the \u201csmall-world\u201d phenomenon.  </p> <p>Tasks. 1. Generate synthetic social networks with increasing numbers of nodes (Erd\u0151s\u2013R\u00e9nyi and Barab\u00e1si\u2013Albert models). 2. Compute the average shortest path length using BFS or matrix multiplications. 3. Plot how path length grows with graph size for each model. 4. Discuss implications for real-world social networks.  </p> <p>Deliverables. - Report (3\u20135 pages) with plots and interpretation. - Code to generate graphs and compute path lengths.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-3-shortest-path-performance-in-large-graphs","title":"Variant 3 \u2014 Shortest Path Performance in Large Graphs","text":"<p>Goal. Empirically compare Dijkstra\u2019s algorithm and BFS (for unweighted graphs) in terms of runtime and scalability.  </p> <p>Tasks. 1. Generate random graphs with varying sizes and edge weights. 2. Run BFS for unweighted graphs and Dijkstra for weighted graphs. 3. Measure execution time and number of operations as input size grows. 4. Plot results and analyze when Dijkstra\u2019s overhead becomes significant.  </p> <p>Deliverables. - Report (4\u20136 pages) with analysis and plots. - Annotated code to reproduce experiments.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-4-ranking-web-pages-with-pagerank","title":"Variant 4 \u2014 Ranking Web Pages with PageRank","text":"<p>Goal. Explore how PageRank values evolve with graph size and structure.  </p> <p>Tasks. 1. Generate directed graphs simulating web pages and hyperlinks. 2. Implement the PageRank algorithm using iterative matrix multiplications. 3. Measure convergence speed (iterations until stable) as graph size increases. 4. Compare rankings across different graph topologies (chain, star, random).  </p> <p>Deliverables. - Report (4\u20136 pages) with convergence plots and discussion. - Code implementing PageRank.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#variant-5-task-scheduling-and-dependency-graphs","title":"Variant 5 \u2014 Task Scheduling and Dependency Graphs","text":"<p>Goal. Analyze project scheduling using Directed Acyclic Graphs (DAGs).  </p> <p>Tasks. 1. Generate random DAGs representing task dependencies. 2. Implement topological sorting to find valid execution orders. 3. Measure execution time and memory usage as graph size increases. 4. Extend the experiment by adding random cycles to test cycle detection.  </p> <p>Deliverables. - Report (4\u20136 pages) with examples and analysis. - Code for DAG generation, sorting, and cycle detection.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/graphs_exercises/#common-notes","title":"Common Notes","text":"<ul> <li>Clearly distinguish between time complexity (theoretical) and observed runtime.  </li> <li>Vary graph density to show how sparse vs dense cases differ.  </li> <li>Optionally use existing graph libraries (e.g., NetworkX, SNAP, GraphX) to scale experiments.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/","title":"Example: Adjacency Matrix Multiplication in a Knowledge Graph","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#scenario","title":"Scenario","text":"<p>Consider a small knowledge graph where nodes represent entities and edges represent semantic relationships.</p> <p>Entities: - A: \"Albert Einstein\" - B: \"Physics\" - C: \"Nobel Prize\" - D: \"Switzerland\"</p> <p>Relationships: - A \u2192 B: Einstein is associated with Physics - A \u2192 D: Einstein lived in Switzerland - B \u2192 C: Physics is related to the Nobel Prize - D \u2192 C: Switzerland hosts Nobel Prize institutions  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 0 C 0 0 0 0 D 0 0 1 0 <p>Each row represents the source entity, and each column represents the target entity. A value of 1 means there is a direct semantic relationship between the entities.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The result A\u00b2 represents indirect (two-step) relationships, i.e., entities connected through an intermediate node.</p> A B C D A 0 0 2 0 B 0 0 0 0 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 2 \u2192   Albert Einstein (A) is semantically connected to the Nobel Prize (C) through two distinct paths:</li> <li>A \u2192 B \u2192 C (Einstein \u2192 Physics \u2192 Nobel Prize)  </li> <li>A \u2192 D \u2192 C (Einstein \u2192 Switzerland \u2192 Nobel Prize)</li> </ul> <p>This indicates a reinforced semantic relationship: multiple paths in the graph link Einstein to the Nobel Prize.</p> <ul> <li>All other entries are 0 \u2192 no indirect two-step relationships exist among the other entities.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#insights-and-applications","title":"Insights and Applications","text":"<ul> <li>Implicit relationship inference: Identify hidden or indirect semantic links (e.g., inferring that \"Einstein is related to the Nobel Prize\" even if not explicitly stated).  </li> <li>Knowledge discovery: Find entities that share common semantic intermediaries (useful in semantic search and reasoning systems).  </li> <li>Semantic strength analysis: Higher values in A\u00b2 or A\u00b3 indicate multiple reinforcing semantic paths between entities.  </li> <li>Query expansion: Use A\u00b2 or A\u00b3 to retrieve contextually related entities in knowledge retrieval systems.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/knowledge_graph/#extensions","title":"Extensions","text":"<ul> <li>A\u00b3 \u2192 captures relationships through three-step paths (longer semantic chains).  </li> <li>(I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 measures total semantic connectivity for an entity.  </li> <li>Weighted adjacency matrices \u2192 edges can represent relationship confidence, semantic strength, or co-occurrence frequency.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/","title":"Lectures - Complexity Management","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#introduction","title":"Introduction","text":"<p>Programmers are always surrounded by complexity; we cannot avoid it. \u2014 Tony Hoare (1934\u2013)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#brief-biography","title":"Brief Biography","text":"<p>Charles Antony Richard Hoare, known as Tony Hoare, was born in Colombo (Sri Lanka, then Ceylon) in 1934. Originally studying Classics at Oxford, he later turned to mathematics and computer science, becoming one of the most influential figures in programming languages and algorithms. In 1960, he created Quicksort, one of the most widely used and efficient sorting algorithms. Hoare worked on compiler design, programming language theory, and software verification. He was a professor at the University of Oxford and later a principal researcher at Microsoft Research. In recognition of his contributions, he received the ACM Turing Award in 1980.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#null-references-the-billion-dollar-mistake","title":"Null References \u2013 The \"Billion-Dollar Mistake\"","text":"<p>In 1965, while designing the programming language ALGOL W, Tony Hoare introduced the concept of null references (or null pointers) as a way to indicate the absence of a value in reference variables. Decades later, he admitted this design decision was a serious error, calling it his \"billion-dollar mistake\", since null references have caused countless errors, system crashes, vulnerabilities, and financial losses across the software industry. Hoare discussed this in a 2009 talk at QCon London, reflecting on the immense cost of null-related bugs. Modern programming languages such as Kotlin, Swift, and Rust have since introduced explicit type systems that differentiate between nullable and non-nullable values, reducing the risk of null pointer exceptions and improving software reliability.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o","title":"BiG O","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o_1","title":"Big O","text":"<p>The order of complexity in algorithms is a way to measure an algorithm's efficiency in terms of time and/or space as the input size grows. This measure is expressed using Big O notation, which provides a way to describe the asymptotic behaviour of the algorithm, meaning how it behaves when the input becomes very large.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#types-of-complexity","title":"Types of Complexity","text":"<ul> <li>Time Complexity: Measures the execution time of the algorithm based on the input size n. It focuses on how many operations the algorithm performs as the input size increases.  </li> <li>Space Complexity: Measures the amount of memory an algorithm needs as a function of the input size n.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o-notation","title":"Big O Notation","text":"<p>Big O notation describes the worst-case scenario, representing the upper limit of time or space required as n grows. </p> <p></p>  Details <p>Also called Landau's symbol, is a symbolism used in complexity theory, computer science, and mathematics to describe the asymptotic behavior of functions.</p> <p>German mathematician Edmund Landau, who was one of the first to use this type of notation in mathematical analysis, to describe the behaviour of mathematical functions when they tend to infinity.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o1","title":"O(1)","text":"<p>O(1): Constant. The execution time does not change with the size of the input. Example: Print a message. </p> <p></p>  Details <p>Constant time complexity (O(1)) because the execution time of the System.out.println statement is independent of the input size.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o1_1","title":"O(1)","text":"<p>O(1): Constant. Why is it \u201cprint a message\u201d O(1)? Because the execution time of the <code>System.out.println</code> statement is independent of the input size.  </p> <p></p>  Details <p>Constant time complexity (O(1)) because the execution time of the System.out.println statement is independent of the input size.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#olog-n","title":"O(log n)","text":"<p>O(log n): Logarithmic. The execution time grows logarithmically as the input size increases. Example: Binary search. </p> <p></p>  Details <p>For an array of size n = 1,000,000, binary search will take roughly log\u20612(1,000,000)\u224820 comparisons to find the target.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#olog-n_1","title":"O(log n)","text":"<p>O(log n): Logarithmic. Why is \u201cbinary search\u201d O(log n)? Because at each step, it halves the search space. So, if you double the size of the array, the number of steps required to find an element increases only by 1 (since log\u2082(2n) = log\u2082(n) + 1).  </p> <p></p>  Details <p>Logarithmic Growth: Binary search is logarithmic because at each step, it halves the search space. So if you double the size of the array, the number of steps required to find an element increases only by 1 (since log\u20612(2n)=log\u20612(n)+1\\log_2(2n) = \\log_2(n) + 1log2\u200b(2n)=log2\u200b(n)+1).</p> <p>For example:</p> <p> - For an array of size 1,000,000, binary search requires about 20 comparisons.</p> <p> - For an array of size 2,000,000, it requires 21 comparisons.</p> <p> - Even though the input size doubled, the time to search increased by only a single step, which is characteristic of logarithmic growth.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on","title":"O(n)","text":"<p>O(n): Linear. The execution time grows proportionally with the input size. Example: Iterating through an array.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_1","title":"O(n)","text":"<p>O(n): Linear. Why is \u201citerating through an array\u201d O(n)? The for loop iterates exactly n times. The number of operations (iterations) grows linearly with the input size. If n = 1,000,000 (or 2,000,000) the loop runs 1,000,000 (2,000,000) times.  </p> <p></p>  Details <p> Why is This O(n) Linear Time Complexity?</p> <p>In this example, the for loop runs exactly n times, which means the number of iterations grows linearly with the input size n.</p> <p>For example:</p> <p> - If n = 10, the loop runs 10 times.</p> <p> - If n = 1000, the loop runs 1000 times.</p> <p> - If n = 1,000,000, the loop runs 1,000,000 times.</p> <p>O(n) means that the execution time of the algorithm is proportional to the size of the input. If you double the size of the input (n), the time to complete the loop will approximately double as well.</p> <p>So, if it takes 1 second to run 1,000,000 iterations, it will take roughly 2 seconds to run 2,000,000 iterations (assuming constant factors like hardware performance stay the same).</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on-log-n","title":"O(n log n)","text":"<p>O(n log n): Linear-logarithmic. Common in sorting algorithms like Merge Sort and Heap Sort. Example: Sorting (Merge Sort).  </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on-log-n_1","title":"O(n log n)","text":"<p>O(n log n): Linear-logarithmic. Why is sorting (merge sort) O(n log n)?  </p> <p></p>  Details <p> Why is this O(n log n)?</p> <p>Sorting an array of n elements using efficient algorithms like Merge Sort or Timsort takes O(n log n) time:</p> <p> - The array is divided into smaller subarrays recursively (halving the array size at each step), which takes logarithmic time O(log\u2061n)O(\\log n)O(logn).</p> <p> - Merging the sorted subarrays takes linear time O(n)O(n)O(n) because every element needs to be checked at least once.</p> <p> - Therefore, the overall time complexity is O(n \\log n).</p> <p>Example:</p> <p>For an array of size 100,000, the time complexity would be proportional to:</p> <p> - n=100,000n = 100,000n=100,000.</p> <p> - log\u2061n=log\u20612100,000\u224816.61\\log n = \\log_2 100,000 \\approx 16.61logn=log2\u200b100,000\u224816.61</p> <p> - So, the number of operations would be proportional to 100,000\u00d716.61\u22481,661,000100,000 \\times 16.61 \\approx 1,661,000100,000\u00d716.61\u22481,661,000 operations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on2","title":"O(n\u00b2)","text":"<p>O(n\u00b2): Quadratic. The execution time is proportional to the square of the input size (e.g., sorting algorithms like Bubble Sort or Selection Sort). Example: Bubble sort. </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on2_1","title":"O(n\u00b2)","text":"<p>O(n\u00b2): Quadratic. Why is Bubble Sort O(n\u00b2)?  </p> <p></p>  Details <p> Why is Bubble sort O(n\u00b2)?</p> <p>The Bubble Sort algorithm has two nested loops: one that iterates over the array and another that iterates over the unsorted part of the array to compare and swap adjacent elements.</p> <p> The outer loop runs n times, and for each iteration of the outer loop, the inner loop also runs n times. This leads to a total of n\u00d7n=n\u00b2 operations. In simple terms, as the input size n grows, the time it takes to sort the array grows quadratically. </p> <p>For example:</p> <p> - If n = 1000, the algorithm performs approximately 1000\u00d71000=1,000,0001000 \\times 1000 = 1,000,0001000\u00d71000=1,000,000 operations.</p> <p> If n = 2000, the algorithm performs approximately 2000\u00d72000=4,000,0002000 \\times 2000 = 4,000,0002000\u00d72000=4,000,000 operations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on3","title":"O(n\u00b3)","text":"<p>O(n\u00b3): Polynomial. The runtime of the algorithm increases cubically as the input size grows. Example: Matrix Multiplication (Triple Nested Loops).</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on3_1","title":"O(n\u00b3)","text":"<p>O(n\u00b3): Polynomial. Why is Matrix Multiplication O(n\u00b3)? </p> <p></p>  Details <p> Why is This O(n\u00b3)?</p> <p>Triple Nested Loops: The three nested loops each iterate n times:</p> <p> - The outer loop iterates over the rows of matrix AAA. </p> <p> - The middle loop iterates over the columns of matrix BBB.</p> <p> - The inner loop computes the dot product for each element of the result matrix.</p> <p>The total number of iterations is n\u00d7n\u00d7n=n^3, which is why the algorithm has O(n\u00b3) time complexity.</p> <p>Example:</p> <p> - If n = 100, the algorithm performs approximately 100\u00d7100\u00d7100=1,000,000100 \\times 100 \\times 100 = 1,000,000100\u00d7100\u00d7100=1,000,000 operations.</p> <p> - If n = 200, the algorithm performs approximately 200\u00d7200\u00d7200=8,000,000200 \\times 200 \\times 200 = 8,000,000200\u00d7200\u00d7200=8,000,000 operations.</p> <p> As n increases, the number of operations grows very quickly, which reflects the cubic nature of the time complexity.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o2n","title":"O(2\u207f)","text":"<p>O(2\u207f): Exponential. The execution time doubles with each increase in the input size, common in brute force problems. Example: Fibonacci Sequence (recursive). </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o2n_1","title":"O(2\u207f)","text":"<p>O(2\u207f): Exponential. Why is Fibonacci Sequence O(2\u207f)? </p> <p></p>  Details <p> Why is this O(2\u207f)?</p> <p>The total number of calls for fib(n) is close to 2^n, which is why the time complexity is O(2\u207f).</p> <p>Formal Reason:</p> <p> The recurrence relation that describes the time complexity is: </p> <p> T(n)=T(n\u22121)+T(n\u22122)+O(1) </p> <p>This recurrence reflects the fact that each call to fib(n) involves two recursive calls: fib(n - 1) and fib(n - 2). The solution to this recurrence is O(2^n), which means the number of recursive calls grows exponentially with n.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_2","title":"O(n!)","text":"<p>O(n!): Factorial. Extremely inefficient, grows very quickly, seen in permutation and combinatorial problems. Example: Permutation Generation. </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_3","title":"O(n!)","text":"<p>O(n!): Factorial. Why is Permutation Generation O(n!)? </p>  Details <p> Why is Permutation Generation O(n!)?</p> <p> - For every element in the list, we generate all permutations of the remaining elements.</p> <p> - This results in a recursive process where the time complexity grows factorially as the size of the list increases.</p> <p> - The number of possible permutations of a list of n elements is n!, which means that the time complexity of generating all permutations is O(n!).</p> <p>Example:</p> <p> For a list of 5 elements ({1, 2, 3, 4, 5}), there are 5!=1205! = 1205!=120 possible permutations. The function generates all 120 permutations and measures how long this process takes.</p> <p> If you increase the number of elements to 6, the number of permutations increases to 6!=7206! = 7206!=720. For 7 elements, it becomes 7!=5,0407! = 5,0407!=5,040, and so on. The execution time grows factorially with the size of the input list.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o_2","title":"Big O","text":"<p>Big O notation describes the worst-case scenario, representing the upper limit of time or space required as n grows.</p> <p></p> <p>Exercises</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/","title":"Example: Adjacency Matrix Multiplication in a Recommendation System","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#scenario","title":"Scenario","text":"<p>Consider a simple recommendation system with two types of nodes: - Users: U\u2081, U\u2082 - Items: I\u2081, I\u2082, I\u2083  </p> <p>Edges indicate that a user has positively rated an item (for instance, a rating \u2265 4 stars).</p> <p>Relationships: - U\u2081 \u2192 I\u2081, I\u2082 - U\u2082 \u2192 I\u2082, I\u2083  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"<p>We arrange the nodes as [U\u2081, U\u2082, I\u2081, I\u2082, I\u2083]. The adjacency matrix is bipartite, meaning users connect only to items.</p> U\u2081 U\u2082 I\u2081 I\u2082 I\u2083 U\u2081 0 0 1 1 0 U\u2082 0 0 0 1 1 I\u2081 0 0 0 0 0 I\u2082 0 0 0 0 0 I\u2083 0 0 0 0 0 <p>Each row represents a user or item, and each column represents a possible connection (rating or preference).</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#operation-1-a-at-user-similarity","title":"Operation 1: A \u00d7 A\u1d40 (User Similarity)","text":"<p>Multiplying A by its transpose yields user\u2013user similarity, based on shared items.</p> U\u2081 U\u2082 I\u2081 I\u2082 I\u2083 U\u2081 2 1 0 0 0 U\u2082 1 2 0 0 0 I\u2081 0 0 1 0 0 I\u2082 0 0 0 1 0 I\u2083 0 0 0 0 1"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#interpretation","title":"Interpretation","text":"<ul> <li>The top-left block (users \u00d7 users) measures similarity:</li> <li>(U\u2081, U\u2082) = 1 \u2192 They both liked item I\u2082.  </li> <li>(U\u2081, U\u2081) = 2 \u2192 U\u2081 rated two items in total.  </li> <li>Higher values indicate stronger preference overlap.</li> <li>The system can recommend item I\u2083 to U\u2081, since U\u2082, a similar user, rated it positively.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#operation-2-at-a-item-similarity","title":"Operation 2: A\u1d40 \u00d7 A (Item Similarity)","text":"<p>This multiplication yields item\u2013item similarity, based on the users who rated them.</p> U\u2081 U\u2082 I\u2081 I\u2082 I\u2083 I\u2081 0 0 1 1 0 I\u2082 0 0 1 2 1 I\u2083 0 0 0 1 1"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#interpretation_1","title":"Interpretation","text":"<ul> <li>(I\u2081, I\u2082) = 1 \u2192 Both rated by U\u2081.  </li> <li>(I\u2082, I\u2083) = 1 \u2192 Both rated by U\u2082.  </li> <li>Items with higher co-ratings can be recommended together (e.g., recommend I\u2083 to someone who liked I\u2082).</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/recommendation_system/#analytical-insights","title":"Analytical Insights","text":"<ul> <li>User similarity: Identify users with shared preferences (collaborative filtering).  </li> <li>Item similarity: Detect related or substitutable items (content-based filtering).  </li> <li>Community detection: Group users with similar behavior patterns.  </li> <li>Preference prediction: Estimate missing ratings via matrix operations (e.g., A\u00b2 or matrix factorization).  </li> <li>Recommendation propagation: Use higher powers (A + A\u00b2 + \u2026) to infer multi-hop preference relations.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/","title":"Example: Adjacency Matrix Multiplication in a Scheduling Problem","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#scenario","title":"Scenario","text":"<p>Consider a set of tasks with dependencies, meaning some tasks cannot start until others are completed.</p> <p>Tasks: - A: Design the architecture - B: Implement the code - C: Test the system - D: Deploy the application  </p> <p>Dependencies (A \u2192 B means \u201cB depends on A\u201d): - A \u2192 B (coding requires the design) - B \u2192 C (testing requires code) - C \u2192 D (deployment requires tests) - A \u2192 C (some tests depend directly on the design)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 1 0 B 0 0 1 0 C 0 0 0 1 D 0 0 0 0 <p>Each row represents a preceding task, and each column represents a dependent task. A value of 1 indicates that a task must be completed before another can start.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>This operation shows indirect second-level dependencies \u2014 tasks that depend on others through an intermediate task.</p> A B C D A 0 0 1 1 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 Task A indirectly influences C (A \u2192 B \u2192 C), in addition to the direct A \u2192 C relation.  </li> <li>A\u00b2\u208dA,D\u208e = 1 \u2192 Task A indirectly influences D through A \u2192 B \u2192 C \u2192 D.  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 Task B indirectly influences D through B \u2192 C \u2192 D.  </li> <li>Task D has no outgoing dependencies (it\u2019s the final task in the workflow).</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#analytical-insights","title":"Analytical Insights","text":"<ol> <li> <p>Indirect dependency detection:    Identifies which tasks are connected by multiple dependency levels (useful for critical path analysis).</p> </li> <li> <p>Critical path analysis:    Reveals which initial tasks have the greatest downstream impact on the overall schedule.</p> </li> <li> <p>Delay propagation:    If one task is delayed, A\u00b2 shows which tasks will be affected within two dependency levels.</p> </li> <li> <p>Planning optimization:    Highlights redundant or unnecessary dependencies that may create scheduling bottlenecks.</p> </li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scheduling_problem/#extensions","title":"Extensions","text":"<ul> <li>A\u00b3 \u2192 shows third-level dependencies (longer paths in the scheduling graph).  </li> <li>(I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 measures total task reachability (direct and indirect dependencies).  </li> <li>Weighted adjacency matrices \u2192 edges can represent task duration, priority, or risk, allowing cumulative impact analysis.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/","title":"scientificpapers.md","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#how-to-write-a-scientific-paper-academic-style-guide-with-examples","title":"How to Write a Scientific Paper (Academic Style Guide with Examples)","text":"<p>Writing a scientific article requires following internationally accepted conventions that ensure clarity, reproducibility, and rigor. A scientific paper must follow a clear and structured format to effectively communicate research findings. Below is a detailed guide to the main sections, including style notes and sample phrases commonly used in academic writing.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#1-title-and-authors","title":"1. Title and Authors","text":"<ul> <li>The title should be concise, informative, and reflect the central contribution.  </li> <li>Avoid vague terms such as \"A Study on...\"; instead, use specific keywords.  </li> <li>Include all authors with institutional affiliations and a corresponding author email.  </li> </ul> \ud83d\udca1 Example titles <p>- Optimizing Neural Network Training with Hybrid Gradient Techniques</p> <p>- A Comparative Study of Quantum Algorithms for Linear Systems</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#2-abstract","title":"2. Abstract","text":"<ul> <li>Length: typically 150\u2013250 words.  </li> <li>Provides a complete but concise overview of the study.  </li> <li>Must include:  </li> <li>Problem or challenge.  </li> <li>Methods and approach.  </li> <li>Key results.  </li> <li>Main conclusion.  </li> </ul> \ud83d\udca1 Example titles <p>- This paper addresses the challenge of...</p> <p>- The proposed method was evaluated on...</p> <p>- Results show a significant improvement in...</p> <p>- These findings suggest that...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#3-keywords","title":"3. Keywords","text":"<ul> <li>Include 4\u20136 specific terms.  </li> <li>Facilitate search and indexing.  </li> </ul> \ud83d\udca1 Examples <p>- \"graph neural networks,\" \"computational linguistics,\" \"data privacy,\" \"blockchain security.\"</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#4-introduction","title":"4. Introduction","text":"<ul> <li>Establishes the context and significance of the research.  </li> <li>Summarizes existing work and identifies the gap.  </li> <li>Ends with objectives and contributions.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Recent studies have demonstrated that...</p> <p>- However, little attention has been paid to...</p> <p>- The aim of this paper is to...</p> <p>- Our contributions are as follows...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#5-problem-statement","title":"5. Problem Statement","text":"<ul> <li>Defines clearly what problem is being addressed.  </li> <li>May include formal definitions, hypotheses, or models.  </li> </ul> \ud83d\udca1 Problem Statement <p>- The problem can be formally defined as...</p> <p>- We hypothesize that...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#6-methodology-method-proposal-solution","title":"6. Methodology (Method / Proposal / Solution)","text":"<ul> <li>Explains how the research was conducted.  </li> <li>Must allow replication by other researchers.  </li> <li>Include: algorithms, models, datasets, tools, hardware/software.  </li> </ul> \ud83d\udca1 Sample phrases <p>- We implemented the proposed approach using...</p> <p>- Experiments were carried out on...</p> <p>- The parameters were set to...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Present experiments systematically and report results clearly.  </li> <li>Use consistent formatting for tables and figures.  </li> <li>Always provide units and ensure charts are readable.  </li> <li>Discuss results in relation to prior work.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Table 1 summarizes the performance of...</p> <p>- As shown in Figure 2, the proposed method outperforms...</p> <p>- These findings are consistent with...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#8-conclusions","title":"8. Conclusions","text":"<ul> <li>Recap the main findings without repeating details.  </li> <li>Emphasize originality and significance.  </li> </ul> \ud83d\udca1 Sample phrases <p>- In conclusion, this work demonstrates that...</p> <p>- The results confirm that...</p> <p>- The study contributes to the field by...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#9-future-work","title":"9. Future Work","text":"<ul> <li>Outline possible directions for extending the study.  </li> <li>Acknowledge current limitations.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Future research could explore...</p> <p>- One limitation of this study is...</p> <p>- Further validation is required in...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#10-references","title":"10. References","text":"<ul> <li>Use a recognized citation style (IEEE, APA, etc.).  </li> <li>Ensure every citation in text has a corresponding reference.  </li> </ul> \ud83d\udca1 Sample phrases <p>- As discussed by Smith et al. [3]...</p> <p>- Following the method in [12]...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#11-general-style-and-recommendations","title":"11. General Style and Recommendations","text":"<ul> <li>Use precise, formal, and objective language.  </li> <li>Prefer passive or impersonal voice (e.g., \"was conducted\" instead of \"we conducted\").  </li> <li>Proofread for clarity and coherence.  </li> </ul> \ud83d\udca1 Sample academic transitions <p>- Moreover, </p> <p>- In contrast, </p> <p>- Therefore, </p> <p>- It is worth noting that... </p> <p>This academic-style guide not only explains the structure of a paper but also provides language templates that students can adapt in their own writing.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/","title":"Example: Adjacency Matrix Multiplication in a Social Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#scenario","title":"Scenario","text":"<p>We have a small social network with four users: A, B, C, and D. Connections are directed \u2014 for instance, like Twitter follows (A \u2192 B means A follows B).</p> Relationship Description A \u2192 B A follows B B \u2192 C B follows C C \u2192 D C follows D A \u2192 D A also follows D"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 0 C 0 0 0 1 D 0 0 0 0 <p>Each row shows who follows whom.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>This multiplication tells us the number of paths of length 2 between users \u2014 i.e., \u201cfriend of a friend\u201d or \u201cfollower of a followed user.\u201d</p> A B C D A 0 0 1 0 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 A can reach C in two steps (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 B can reach D in two steps (B \u2192 C \u2192 D).  </li> <li>Other entries are 0 \u2192 no two-step paths exist.</li> </ul> <p>This helps answer questions such as: - \u201cWho can I reach in two hops?\u201d - \u201cHow many intermediaries are between two users?\u201d - \u201cWho could be suggested as a new friend (indirectly connected users)?\u201d</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/social_network/#possible-extensions","title":"Possible Extensions","text":"<ul> <li>A\u00b3 \u2192 paths of length 3 (\u201cfriend of a friend of a friend\u201d).  </li> <li>(A + A\u00b2 + A\u00b3 + \u2026) \u2192 overall reachability or connectivity-based centrality.  </li> <li>Weighted networks: non-binary values can represent interaction strength or frequency.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/","title":"Example: Adjacency Matrix Multiplication in a Transportation Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#scenario","title":"Scenario","text":"<p>We have a transportation network connecting four cities: A (Madrid), B (Barcelona), C (Valencia), and D (Seville).</p> <p>The direct routes (by road, rail, or air) are:</p> Route Description A \u2192 B Direct route from Madrid to Barcelona B \u2192 C Direct route from Barcelona to Valencia C \u2192 D Direct route from Valencia to Seville A \u2192 D Direct route from Madrid to Seville"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 0 C 0 0 0 1 D 0 0 0 0 <p>Each row represents the origin city, and each column represents the destination city. A value of 1 means there is a direct connection between the two cities.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The result A\u00b2 shows how many indirect routes (two-leg connections) exist between cities.</p> A B C D A 0 0 1 0 B 0 0 0 1 C 0 0 0 0 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 There is an indirect route from Madrid to Valencia through Barcelona (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dB,D\u208e = 1 \u2192 There is an indirect route from Barcelona to Seville through Valencia (B \u2192 C \u2192 D).  </li> <li>All other values are 0 \u2192 no two-leg routes exist between those cities.</li> </ul> <p>This analysis can be used to: - Identify reachable cities in two steps (e.g., flights with one layover). - Measure network efficiency by counting how many destinations can be reached in a few steps. - Detect bottlenecks or critical connections in the transport system.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/transportation_network/#possible-extensions","title":"Possible Extensions","text":"<ul> <li>A\u00b3 \u2192 identifies routes with two layovers (three legs).  </li> <li>(A + A\u00b2 + A\u00b3 + \u2026) \u2192 measures total reachability or overall connectivity.  </li> <li>Weighted networks \u2192 values represent distance, time, or cost, enabling optimization of routes.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/","title":"Other Uses of Matrix Multiplication in Big Data (Beyond Graphs)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#machine-learning-and-deep-learning","title":"Machine Learning and Deep Learning","text":"<ul> <li>Neural network training: each layer performs matrix multiplication operations (weights \u00d7 inputs).  </li> <li>Example: in an embedding-based recommendation model, products and users are represented as vectors, and similarity is computed through matrix multiplications. </li> </ul> Machine Learning and Deep Learning <p>Neural network training: each layer performs matrix multiplication between weights and inputs to compute the activations.   For example, in a layer with weight matrix W and input vector X, the output is Y = W \u00d7 X + b, where b is the bias term.</p> <p>Example: In an embedding-based recommendation model, users and products are represented as vectors in a latent space.   If U is the user embedding matrix (each row represents a user) and P is the product embedding matrix (each row a product),   the similarity or affinity between users and products is computed as S = U \u00d7 P\u1d40,   where S<sub>ij</sub> expresses how much user i is predicted to like product j.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<ul> <li>Vector representations (word embeddings, BERT, transformers): require millions of matrix multiplication operations to compute attention and contextual relationships.  </li> <li>Example: the self-attention mechanism in transformers is based on matrix multiplications between queries, keys, and values.  </li> </ul> Natural Language Processing (NLP) <p>Vector representations: (word embeddings, BERT, transformers) require millions of matrix multiplication operations to compute attention and contextual relationships between tokens. Each word or token is represented as a numerical vector in a high-dimensional space.</p> <p>Example: In the self-attention mechanism of transformers, three matrices are computed: Queries (Q), Keys (K), and Values (V).   The attention scores are calculated through matrix multiplications as follows:   Attention(Q, K, V) = softmax((Q \u00d7 K\u1d40) / \u221ad<sub>k</sub>) \u00d7 V   This allows each token to attend to others according to their contextual relevance, forming the core of models like BERT and GPT.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#image-and-signal-processing","title":"Image and Signal Processing","text":"<ul> <li>In computer vision, convolutions can be represented as matrix multiplications between an image and filters.  </li> <li>In Big Data, this is applied to process large-scale image collections (e.g., facial recognition in massive datasets).</li> </ul> Image and Signal Processing <p>In computer vision, convolutions can be represented as matrix multiplications between an image (a matrix of pixel values) and small filters or kernels.   Each convolution operation slides the filter across the image, performing local matrix multiplications to extract features such as edges, colors, or textures.</p> <p>In Big Data, this approach is used to process large-scale image collections \u2014 for instance, in facial recognition systems or automatic image classification across massive datasets.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#matrix-factorization-in-recommendation-systems","title":"Matrix Factorization in Recommendation Systems","text":"<ul> <li>Techniques such as Singular Value Decomposition (SVD) or Matrix Factorization decompose large matrices (users \u00d7 items) into products of smaller matrices.  </li> <li>This allows for uncovering hidden patterns and predicting ratings.  </li> </ul> Matrix Factorization in Recommendation Systems <p>Techniques such as Singular Value Decomposition (SVD) or Matrix Factorization decompose large rating matrices (users \u00d7 items) into the product of smaller matrices that represent latent features of users and items.</p> <p>This allows for uncovering hidden patterns and predicting missing ratings, forming the foundation of modern collaborative filtering methods used in platforms like Netflix and Spotify.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/usesmatrixmultiplication/#scientific-simulations-and-bioinformatics","title":"Scientific Simulations and Bioinformatics","text":"<ul> <li>In molecular dynamics, climatology, or genomics, huge matrices are used to represent interactions.  </li> <li>The system\u2019s evolution is simulated through successive matrix multiplications. </li> </ul> Scientific Simulations and Bioinformatics <p>In fields such as molecular dynamics, climatology, or genomics, large matrices are used to represent complex interactions among system components \u2014 such as forces between atoms, correlations between genes, or environmental variables.</p> <p>The system\u2019s evolution is simulated through successive matrix multiplications, which update the state of the model over time.   This enables the study of complex phenomena such as protein folding, mutation propagation, or large-scale climate patterns.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/","title":"Example: Adjacency Matrix Multiplication in a Web Page Network","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#scenario","title":"Scenario","text":"<p>Consider a network of four web pages: A, B, C, and D. The edges represent hyperlinks between pages (directed links, where the row is the source and the column is the destination).</p> <p>Direct links: - A \u2192 B, A \u2192 D - B \u2192 C, B \u2192 D - C \u2192 A - D \u2192 (no outgoing links)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#adjacency-matrix-a","title":"Adjacency Matrix (A)","text":"A B C D A 0 1 0 1 B 0 0 1 1 C 1 0 0 0 D 0 0 0 0 <p>Each row represents the source page and each column represents the target page. A value of 1 indicates a direct hyperlink between two pages.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#matrix-multiplication-a2-a-a","title":"Matrix Multiplication: A\u00b2 = A \u00d7 A","text":"<p>The matrix A\u00b2 shows how many two-click paths exist between web pages.</p> A B C D A 0 0 1 1 B 1 0 0 0 C 0 1 0 1 D 0 0 0 0"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#interpretation","title":"Interpretation","text":"<ul> <li>A\u00b2\u208dA,C\u208e = 1 \u2192 From page A, you can reach C in two clicks (A \u2192 B \u2192 C).  </li> <li>A\u00b2\u208dA,D\u208e = 1 \u2192 From page A, you can reach D in two clicks (A \u2192 B \u2192 D).  </li> <li>A\u00b2\u208dB,A\u208e = 1 \u2192 From page B, you can reach A in two clicks (B \u2192 C \u2192 A).  </li> <li>A\u00b2\u208dC,B\u208e = 1 and A\u00b2\u208dC,D\u208e = 1 \u2192 From page C, you can reach B (C \u2192 A \u2192 B) and D (C \u2192 A \u2192 D).  </li> <li>The entire row for D is zeros \u2192 D is a sink node (no outgoing links).</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/web_pages/#possible-analyses","title":"Possible Analyses","text":"<ul> <li>Two-click reachability: Determine what percentage of the site is accessible within two clicks from a landing page.  </li> <li>Sink or dead-end detection: Identify pages with no outgoing links to improve internal navigation.  </li> <li>Conversion path design: Ensure that key pages (e.g., checkout or signup) are reachable within two clicks.  </li> <li>Internal SEO prioritization: Pages with many indirect incoming links (high column sums in A\u00b2) may attract more navigation flow.</li> </ul> <p>Further extensions: - A\u00b3 \u2192 paths reachable in three clicks. - (I + A + A\u00b2 + \u2026 + A\u1d4f) \u2192 cumulative reachability across multiple navigation steps. - Weighted adjacency matrices can model link importance or visibility.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/","title":"Applications \u2013 Monitoring and Performance Engineering","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#introduction","title":"Introduction","text":"<p>The purpose of software engineering is to control complexity, not to create it. \u2014 Jon Louis Bentley (1953\u2013)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#brief-biography","title":"Brief Biography","text":"<p>Jon Louis Bentley is an American computer scientist best known for his influential work in algorithm design, data structures, and software engineering principles. He is the creator of the k-d tree, a widely used data structure for multidimensional searching, and the author of the classic book Programming Pearls, which explores the art of writing efficient and elegant programs. Bentley\u2019s research has shaped modern approaches to algorithmic efficiency and code simplicity, emphasizing that clarity and maintainability are central to performance. Throughout his career, he has worked at Carnegie Mellon University and Bell Labs, collaborating with leading figures such as Brian Kernighan. His teachings and writings continue to inspire generations of programmers to view software engineering as both a scientific and creative discipline. </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#applications","title":"Applications","text":"<p>Performance engineering in real-world applications focuses on translating theory into actionable coding techniques. It involves structuring code, algorithms, and data in ways that optimize runtime and memory without sacrificing readability. The following sections illustrate how to apply Bentley\u2019s rules and other performance patterns in day-to-day software design.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#fundamental-coding-rules","title":"Fundamental Coding Rules","text":"<p>The foundation of performance engineering lies in writing simpler, cleaner, and more efficient code. Bentley\u2019s rules provide timeless principles that emphasize simplicity, focus, and foresight.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#code-simplification","title":"Code Simplification","text":"<p>Simple programs are faster and easier to maintain. By removing unnecessary control structures or redundant computations, developers achieve greater clarity and speed. For example, replacing nested loops or complex conditional logic with direct list comprehensions or optimized built-in functions often yields both cleaner and faster code. (see example)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#problem-simplification","title":"Problem Simplification","text":"<p>Performance issues are often better solved by rethinking the problem rather than refining the code. Instead of sorting an entire dataset to find a median (O(n log n)), algorithms like Quickselect (O(n)) directly solve the core task. Simplifying the objective reduces complexity and computation time, enhancing scalability. (see example)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#relentless-suspicion","title":"Relentless Suspicion","text":"<p>Every line of code should justify its existence. Eliminating redundant copies, unnecessary conversions, or inefficient sorting algorithms reduces time and space usage. Replacing custom loops with optimized library functions improves reliability and performance, while streamlining the code. (see example)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#early-binding","title":"Early Binding","text":"<p>Early binding involves moving work to an earlier stage to avoid repeated effort. Precomputing constant expressions, caching repetitive results, or establishing database connections before iterative processes saves significant time. It demonstrates the trade-off between memory and time, achieving efficiency through anticipation. (see example)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#compiler-and-language-level-optimizations","title":"Compiler and Language-Level Optimizations","text":"<p>Compilers and modern programming languages incorporate various optimization strategies that developers can leverage by writing predictable and efficient code structures.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#hoisting","title":"Hoisting","text":"<p>Loop-invariant computations should be moved outside the loop to avoid repeated evaluation. For example, saving a collection\u2019s length before a loop prevents unnecessary method calls during iteration.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#loop-fusion","title":"Loop Fusion","text":"<p>Two consecutive loops traversing the same dataset can often be combined into one, minimizing loop overhead and improving cache performance.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#unswitching","title":"Unswitching","text":"<p>When an <code>if</code> condition within a loop does not depend on the iteration variable, it can be moved outside the loop. This reduces branching overhead and improves instruction pipeline efficiency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Delay computation until the value is required. This principle, common in functional programming, avoids unnecessary calculations and saves resources, particularly when results may not always be used.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#power-of-2-multiplication","title":"Power-of-2 Multiplication","text":"<p>Bitwise shifts can replace multiplication by powers of two. While modern compilers often handle this automatically, understanding it highlights the role of low-level arithmetic in performance tuning.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#memoization","title":"Memoization","text":"<p>Caching results of expensive function calls prevents redundant computation. It\u2019s especially useful for recursive algorithms or repeated queries with identical parameters.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#primitives-vs-objects","title":"Primitives vs Objects","text":"<p>Primitive types are stored on the stack and accessed faster than objects on the heap. In languages like Java, using primitives instead of boxed types can reduce memory usage and improve performance.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#immutable-vs-mutable-objects","title":"Immutable vs Mutable Objects","text":"<p>Immutable objects like <code>String</code> ensure safety but create new instances with each modification. Using mutable alternatives such as <code>StringBuilder</code> or buffers can significantly reduce overhead in iterative concatenations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#constant-folding-and-propagation","title":"Constant Folding and Propagation","text":"<p>Compilers pre-evaluate constant expressions at compile time, avoiding runtime computation. Writing code that allows such optimizations helps reduce execution cost.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#ordering-tests","title":"Ordering Tests","text":"<p>When evaluating multiple conditions, tests should be ordered by probability and cost. Frequently true conditions or cheaper checks should appear first to minimize evaluation time.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#advanced-optimization-patterns","title":"Advanced Optimization Patterns","text":"<p>Optimization patterns extend beyond code syntax, encompassing structural and algorithmic design decisions that maximize efficiency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#data-structure-optimizations","title":"Data Structure Optimizations","text":"<p>Choose data structures aligned with access patterns. Techniques like packing, encoding, and precomputation minimize memory and enable faster lookups. Cached or sparse representations reduce redundancy in large datasets.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#control-flow-optimizations","title":"Control Flow Optimizations","text":"<p>Reducing control flow complexity enhances predictability and cache performance. Techniques such as loop unrolling, hoisting, and short-circuiting improve performance by minimizing unnecessary iterations or branching.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#functional-optimizations","title":"Functional Optimizations","text":"<p>Function inlining replaces function calls with their bodies to remove call overhead, while tail-recursion elimination prevents stack overflows. Memoization and lazy loading allow functions to reuse or delay results efficiently.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#hardware-dependent-optimizations","title":"Hardware-Dependent Optimizations","text":"<p>Hardware-awareness is essential for high-performance applications. Code should exploit the architecture\u2019s strengths rather than fighting against it.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#instruction-costs","title":"Instruction Costs","text":"<p>Different operations have different CPU costs. Integer addition is faster than division, and avoiding unnecessary multiplications can improve instruction throughput.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#register-allocation","title":"Register Allocation","text":"<p>Keeping frequently used variables in CPU registers minimizes memory latency. Although modern compilers often manage this, understanding register allocation aids in writing CPU-friendly code.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#memory-layout-and-locality","title":"Memory Layout and Locality","text":"<p>Memory access patterns strongly affect performance. Traversing arrays row by row (contiguous memory) instead of by columns improves cache utilization, as adjacent elements are preloaded together.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#paging-and-storage","title":"Paging and Storage","text":"<p>Efficient data placement in memory and awareness of virtual memory paging are key to large-scale performance. Sequential access reduces page faults and cache misses, improving execution predictability.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#5-best-practices-summary","title":"5. Best Practices Summary","text":"<p>Performance optimization should never compromise clarity or correctness. Always profile and measure before attempting optimization. The best programs balance simplicity, accuracy, and efficiency.</p> <p>Key reminders: - Prioritize readability and maintainability. - Optimize only where performance matters. - Measure the impact of each change quantitatively. - Understand the hardware, but design for clarity first.</p> <p>Best Practices: Clean Code</p> <p>Exercises 1</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/","title":"Best Practices for Writing Maintainable and Efficient Code","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#introduction","title":"Introduction","text":"<p>\u201cThe only way to go fast, is to go well.\u201d \u2014 Robert C. Martin (\"Uncle Bob\")</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#brief-biography","title":"Brief Biography","text":"<p>Robert C. Martin, commonly known as \u201cUncle Bob,\u201d is an American software engineer, author, and educator recognized for his profound influence on modern software development practices. With decades of experience in the industry, Martin has been a leading advocate for clean, maintainable, and testable code. He is one of the original authors of the Agile Manifesto and the creator of the SOLID principles, which have become fundamental guidelines for object-oriented design. Throughout his career, Martin has worked as a consultant and trainer, helping teams and organizations improve software quality through disciplined engineering practices. He has written several seminal books, including Clean Code, The Clean Coder, and Clean Architecture, which have shaped the philosophy of professional software craftsmanship. His teachings emphasize responsibility, simplicity, and clarity in design, inspiring developers worldwide to view programming as both a technical and ethical craft.</p> <p>Software systems evolve continuously: new features, new developers, new environments. Code that is easy to read, modify, and reason about is essential for sustainable development and long-term performance. This chapter presents best practices inspired by Clean Code principles and Performance Engineering techniques for writing robust, maintainable, and efficient software.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#why-best-practices-matter","title":"Why Best Practices Matter","text":"<p>Poorly written code becomes expensive to understand, extend, and debug. Good code is: - Readable - Predictable - Safe to modify - Efficient enough without premature complexity  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#clean-code-philosophy","title":"Clean Code Philosophy","text":"<p>Clean Code emphasizes clarity over cleverness. It states that code should be written for humans first, and machines second.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#performance-engineering-and-code-quality","title":"Performance Engineering and Code Quality","text":"<p>Performance is not just about speed \u2014 it is about avoiding unnecessary complexity. Fast code is usually simple code.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#understanding-dirty-code","title":"Understanding \u201cDirty Code\u201d","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#characteristics-of-poor-code","title":"Characteristics of Poor Code","text":"Symptom Description Rigidity Hard to change; small change triggers a chain of unexpected changes. Fragility Code breaks in multiple places after a small modification. Immobility Code cannot be reused due to tangled dependencies. Needless Complexity Abstractions or patterns introduced without purpose. Needless Repetition Duplicate logic scattered across the codebase. Opacity Hard to read or interpret; intent is unclear."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#consequences","title":"Consequences","text":"<ul> <li>Reduced development velocity</li> <li>Higher maintenance cost</li> <li>Frequent regression bugs</li> <li>Team reluctance to modify code</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#clean-code-principles-general-rules","title":"Clean Code Principles (General Rules)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#code-should-read-like-prose","title":"Code Should Read Like Prose","text":"<p>If someone can understand code by reading it naturally, it is well-written.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#kiss-keep-it-simple","title":"KISS: Keep It Simple","text":"<p>Prefer simple solutions. Complexity must be justified.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#be-small","title":"Be Small","text":"<p>Small classes. Small methods. Small modules.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#the-boy-scout-rule","title":"The Boy Scout Rule","text":"<p>\"Leave the campground cleaner than you found it.\" Improve the code whenever you touch it.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#always-identify-the-root-cause","title":"Always Identify the Root Cause","text":"<p>Fixing symptoms creates technical debt. Fixing causes prevents it.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#naming-conventions","title":"Naming Conventions","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#names-should-be-descriptive","title":"Names Should Be Descriptive","text":"<p>Prefer clarity over brevity.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#classes-as-nouns-methods-as-verbs","title":"Classes as Nouns, Methods as Verbs","text":"<ul> <li><code>UserRepository</code>, not <code>UserDB</code></li> <li><code>calculateTotal()</code>, not <code>total()</code></li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#avoid","title":"Avoid","text":"<ul> <li>Generic names: <code>data</code>, <code>temp</code>, <code>obj</code></li> <li>Negative conditionals (<code>if !isEmpty \u2192 if hasElements</code>)</li> <li>Unpronounceable names (<code>xqz</code>, <code>tmpStr2elm</code>)</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#use-word-pairs","title":"Use Word Pairs","text":"<p>Examples: begin / end open / close min / max add / remove  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#replace-magic-numbers-with-constants","title":"Replace Magic Numbers with Constants","text":"<pre><code>final int MAX_RETRIES = 3;\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#writing-methods-well","title":"Writing Methods Well","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#stepdown-rule-top-to-bottom-readability","title":"Stepdown Rule (Top-to-Bottom Readability)","text":"<p>A function should read like a narrative: each line should lower the level of abstraction smoothly.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#prefer-fewer-parameters","title":"Prefer Fewer Parameters","text":"<p>Functions with many parameters are harder to understand and test. More than three parameters is typically a design smell.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#reduce-cyclomatic-complexity","title":"Reduce Cyclomatic Complexity","text":"<p>Avoid deep nesting (<code>if</code> inside <code>if</code> inside <code>for</code>). Prefer: - early returns, - guard clauses, - decomposition into smaller functions.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#do-one-thing","title":"Do One Thing","text":"<p>A method should have a single, well-defined responsibility.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#avoid-side-effects","title":"Avoid Side Effects","text":"<p>A function should not change external state unless explicitly expected.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#avoid-boolean-flag-parameters","title":"Avoid Boolean Flag Parameters","text":"<p>Boolean flags indicate that the function is doing more than one thing. Split it into multiple functions instead.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#commenting-practices","title":"Commenting Practices","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#explain-yourself-in-code-first","title":"Explain Yourself in Code First","text":"<p>Use meaningful names and method extraction instead of comments.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#remove-zombie-code","title":"Remove Zombie Code","text":"<p>Do not comment out unused logic \u2014 delete it. Version control keeps history.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#avoid-noise-comments","title":"Avoid Noise Comments","text":"<p>Avoid comments that repeat what code already expresses.</p> <p>Example of a noise comment:</p> <pre><code>// increment counter\ni++;\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#use-comments-only-when-necessary","title":"Use Comments Only When Necessary","text":"<p>Use comments to: - Explain the intent behind a decision - Clarify logic that is not self-evident - Warn about potential side effects or performance implications</p> <p>Avoid comments that restate what the code already expresses.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#understandability-and-design-practices","title":"Understandability and Design Practices","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#be-consistent","title":"Be Consistent","text":"<p>Consistency in naming, formatting, and structure reduces cognitive load and improves maintainability. When conventions are predictable, new developers onboard more easily.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#use-explanatory-variables","title":"Use Explanatory Variables","text":"<p>Prefer variables that make the code intention clear.</p> <pre><code># Clear\ndistance = speed * time\n\n# Unclear\nd = s * t\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#encapsulate-boundary-conditions","title":"Encapsulate Boundary Conditions","text":"<p>Boundary logic (limits, ranges, null checks) should be contained in one place rather than repeated across the codebase.</p> <pre><code>if (index &gt;= 0 &amp;&amp; index &lt; list.size()) {\n    return list.get(index);\n}\n</code></pre> <p>This ensures stability and consistency when constraints change.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#prefer-value-objects-over-primitive-types","title":"Prefer Value Objects Over Primitive Types","text":"<p>Wrap domain values into objects to avoid duplicated logic and to express intent more clearly.</p> <pre><code>class Temperature {\n    private final double celsius;\n\n    public Temperature(double celsius) {\n        this.celsius = celsius;\n    }\n\n    public boolean isAboveFreezing() {\n        return celsius &gt; 0;\n    }\n}\n</code></pre> <p>This avoids passing raw primitives with unclear meaning across the system and centralizes domain logic.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#avoid-hidden-logical-dependencies","title":"Avoid Hidden Logical Dependencies","text":"<p>A method should not rely on assumptions that are not visible in its signature.</p> <p>Make requirements explicit using:</p> <ul> <li>Parameters</li> <li>Constructor injection</li> <li>Explicit preconditions</li> </ul> <p>Hidden dependencies make code fragile and hard to maintain.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#prefer-polymorphism-over-conditionals","title":"Prefer Polymorphism Over Conditionals","text":"<p>Instead of writing branching logic based on types:</p> <pre><code>if (type.equals(\"SAVINGS\")) ...\nelse if (type.equals(\"CHECKING\")) ...\n</code></pre> <p>Use polymorphism to encapsulate behavior:</p> <pre><code>interface Account {\n    double calculateInterest();\n}\n\nclass SavingsAccount implements Account {\n    public double calculateInterest() { return 0.05; }\n}\n\nclass CheckingAccount implements Account {\n    public double calculateInterest() { return 0.01; }\n}\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#dependency-injection-and-the-law-of-demeter","title":"Dependency Injection and the Law of Demeter","text":"<p>A class should only interact with:</p> <ul> <li>Its own fields</li> <li>Objects passed as parameters</li> <li>Objects it creates directly</li> </ul> <p>Avoid chained calls such as:</p> <pre><code>order.getCustomer().getAddress().getZipCode();\n</code></pre> <p>Such patterns create tight coupling and fragile dependencies.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#objects-and-data-structures","title":"Objects and Data Structures","text":"<ul> <li>Favor encapsulation: expose behavior, not internal data.</li> <li>Prefer many small, cohesive classes rather than large multipurpose ones.</li> <li>Avoid unnecessary use of <code>static</code>, which hinders flexibility, substitution, and testing.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#writing-good-tests","title":"Writing Good Tests","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#one-assertion-per-test","title":"One Assertion per Test","text":"<p>Each test should verify one behavior to make the cause of failure clear.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#characteristics-of-good-tests","title":"Characteristics of Good Tests","text":"Property Description Readable The intent of the test should be clear. Fast Tests should run quickly to encourage frequent use. Independent Tests must not depend on shared or external state. Repeatable The same test must always produce the same result."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#performance-engineering-best-practices","title":"Performance Engineering Best Practices","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#simplify-the-code-first","title":"Simplify the Code First","text":"<p>Readable code is naturally easier to optimize and reason about.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#simplify-the-problem","title":"Simplify the Problem","text":"<p>Most optimization opportunities arise from removing unnecessary work rather than micro-tuning operations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#question-every-instruction","title":"Question Every Instruction","text":"<p>Assume no line is sacred \u2014 remove or simplify when possible.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#precomputation-early-binding","title":"Precomputation (Early Binding)","text":"<p>Compute expensive values once if they will be reused.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#hoisting-loop-invariant-expressions","title":"Hoisting Loop-Invariant Expressions","text":"<p>Move expressions that do not depend on the loop index outside the loop body.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#loop-fusion","title":"Loop Fusion","text":"<p>Combine loops that iterate over the same data range to reduce overhead.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#loop-unswitching","title":"Loop Unswitching","text":"<p>Move conditional tests outside the loop when possible.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Compute values only when actually needed.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#memoization-and-caching","title":"Memoization and Caching","text":"<p>Store results of expensive function calls to avoid recomputation.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#primitive-types-vs-objects","title":"Primitive Types vs Objects","text":"<p>Be aware of memory footprint and allocation overhead when using objects heavily.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#immutable-vs-mutable-structures","title":"Immutable vs Mutable Structures","text":"<p>Immutable structures simplify reasoning and concurrency but may require additional allocations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#constant-folding-and-short-circuit-logic","title":"Constant Folding and Short-Circuit Logic","text":"<p>Rely on compiler/runtime optimizations to skip unnecessary evaluation.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/cleancode/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<ul> <li>Clean code is about clarity, simplicity, and maintainability.  </li> <li>Performance is achieved primarily by reducing unnecessary complexity, not by clever optimizations.  </li> <li>Good code should be:     -- Simple     -- Intentional     -- Consistent     -- Predictable </li> </ul> <p>Clean code leads to systems that are easier to extend, test, and evolve.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/codesimplification/","title":"Code Simplification Example 1","text":"<p>The Code Simplification rule by Jon Bentley emphasizes that simple programs are both faster and easier to maintain. By reducing unnecessary constructs and using concise syntax, performance and readability can be improved simultaneously.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/codesimplification/#original-code","title":"Original Code","text":"<pre><code>def sum_of_squares(nums):\n    result = 0\n    for num in nums:\n        if num % 2 == 0:\n            result += num ** 2\n    return result\n\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8]\nprint(sum_of_squares(numbers))\n</code></pre> <p>Output:</p> <pre><code>120\n</code></pre> <p>This version works correctly but is unnecessarily verbose: it creates a loop, performs conditional checks, and manually accumulates results.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/codesimplification/#simplified-version","title":"Simplified Version","text":"<pre><code>def sum_of_squares(nums):\n    return sum(num ** 2 for num in nums if num % 2 == 0)\n</code></pre> <p>Output:</p> <pre><code>120\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/codesimplification/#explanation","title":"Explanation","text":"<ul> <li>The expression <code>(num ** 2 for num in nums if num % 2 == 0)</code> generates squares of all even numbers.</li> <li><code>sum()</code> directly adds those values without needing an intermediate variable.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/codesimplification/#benefits","title":"Benefits","text":"<ul> <li>Simplicity: Fewer lines of code, clearer intent.</li> <li>Performance: Python comprehensions are internally optimized in C.</li> <li>Readability: The purpose is immediately visible \u2014 sum of even squares.</li> <li>Maintainability: Less code means fewer potential errors.</li> </ul> <p>Thess examples demonstrate Bentley\u2019s principle: simpler code is often faster code.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/","title":"Early Binding \u2014 Examples","text":"<p>Principle. Move work forward in time. Do work once now to avoid doing it many times later. Precompute constants, cache repeated values, and lift loop\u2011invariant work out of hot paths.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#example-1-building-a-connection-string-repeatedly-no-early-binding","title":"Example 1 \u2014 Building a Connection String Repeatedly (No Early Binding)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#code","title":"Code","text":"<pre><code>import time\n\ndef connect_to_db():\n    db_host = \"localhost\"\n    db_user = \"user\"\n    db_password = \"password\"\n\n    # Built every call \u2014 repeated work\n    connection_string = f\"host={db_host};user={db_user};password={db_password}\"\n\n    # Simulate connecting\n    time.sleep(0.1)\n    print(f\"Connecting with: {connection_string}\")\n\n# Connect multiple times (repeats construction)\nfor _ in range(5):\n    connect_to_db()\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#issues","title":"Issues","text":"<ul> <li>Repeated Work: The <code>connection_string</code> is rebuilt on every call although the data is constant.</li> <li>Inefficiency: Unnecessary string construction adds overhead inside the loop.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#example-1-improved-precompute-the-connection-string-early-binding","title":"Example 1 (Improved) \u2014 Precompute the Connection String (Early Binding)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#code_1","title":"Code","text":"<pre><code>import time\n\n# Pre-calculate once (outside the loop / hot path)\ndb_host = \"localhost\"\ndb_user = \"user\"\ndb_password = \"password\"\nconnection_string = f\"host={db_host};user={db_user};password={db_password}\"\n\ndef connect_to_db(connection_string: str):\n    time.sleep(0.1)  # Simulate I/O\n    print(f\"Connecting with: {connection_string}\")\n\nfor _ in range(5):\n    connect_to_db(connection_string)\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#benefits","title":"Benefits","text":"<ol> <li>Efficiency: Build once, reuse many times.</li> <li>Reduced Overhead: Less work inside the loop or hot path.</li> <li>Readability: The intent is clearer; constants are defined up front.</li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#example-2-recomputing-square-roots-in-a-loop-no-early-binding","title":"Example 2 \u2014 Recomputing Square Roots in a Loop (No Early Binding)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#code_2","title":"Code","text":"<pre><code>import math\n\ndef calculate_square_roots(nums):\n    results = []\n    for num in nums:\n        results.append(math.sqrt(num))  # recalculated every time\n    return results\n\nnumbers = list(range(1, 11))\nprint(calculate_square_roots(numbers))\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#issue","title":"Issue","text":"<ul> <li>Repeated Computation: <code>math.sqrt(n)</code> is recomputed for the same inputs across iterations or calls.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#example-2-improved-precompute-cache-with-a-lookup-table-early-binding","title":"Example 2 (Improved) \u2014 Precompute / Cache with a Lookup Table (Early Binding)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#code_3","title":"Code","text":"<pre><code>import math\n\n# Precompute once (e.g., at startup)\nsquare_roots_lookup = {num: math.sqrt(num) for num in range(1, 11)}\n\ndef get_square_root(num):\n    return square_roots_lookup.get(num, None)\n\nnumbers = list(range(1, 11))\nprint([get_square_root(num) for num in numbers])\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/earlybinding/#benefits_1","title":"Benefits","text":"<ol> <li>Efficiency: Avoids recalculating; lookups are O(1).</li> <li>Faster Execution: Accessing a cached value is typically faster than recomputing it.</li> <li>Reduced Computation: Work is moved to an earlier, one\u2011time phase.</li> </ol> <p>Overall insight. Early binding trades a small amount of memory (for constants or caches) to remove repeated computation from hot paths, improving performance and predictability.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/","title":"Case Studies","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#case-1-emergency-response-system-with-autonomous-drones","title":"Case 1: Emergency Response System with Autonomous Drones","text":"<p>A team has developed an autonomous drone system to assist in natural disaster situations. The drones collect environmental data (wind, temperature, humidity), and their main function is to deliver essential supplies. The data is stored in a relational database, which is shared with other emergency systems in real time.</p> <p>During a mission in a mountainous area, one of the drones lost control and crashed into a temporary facility where refugees were sheltered. After investigation, it was discovered that the drone's navigation system did not properly assess rapid changes in wind in mountainous regions.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#aspects-to-consider","title":"Aspects to Consider","text":"<p>Storage: The environmental data collected by the drones is stored in a centralized relational database that allows for quick queries.</p> <p>Optimization: Should have been applied, questioning the necessity of each instruction in the navigation algorithm and verifying if the wind data was sufficient to ensure a safe flight.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#questions-for-students","title":"Questions for Students","text":"<ul> <li>Is it ethical to deploy an autonomous system without considering all possible critical navigation scenarios?</li> <li>Should more extensive testing have been conducted on the stored data to cover situations with extreme winds? How could the databases be improved to store more useful information?</li> <li>What type of data structure or format could be used to improve the real-time processing of this environmental data?</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#case-2-real-time-medical-evaluation-platform","title":"Case 2: Real-Time Medical Evaluation Platform","text":"<p>A clinic has implemented a platform for the real-time monitoring of vital signs of elderly patients. Data is continuously generated from wearable sensors and stored in a real-time database (a streaming data system). The platform should alert relatives or medical staff when values outside normal parameters are detected.</p> <p>Recently, one of the devices failed, and the system did not issue the necessary alert in time. The reason was that the system constantly recalculated each value, and due to the large amount of data, delays occurred.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#aspects-to-consider_1","title":"Aspects to Consider","text":"<p>Storage: The data is stored in a real-time database, where it is processed as it is received.</p> <p>Optimization: To allow pre-computation of certain critical indicators, storing them for reuse, which would have reduced processing times and ensured a better response during emergencies.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#questions-for-students_1","title":"Questions for Students","text":"<ul> <li>How can the data flow be optimized so that the system responds more efficiently during an emergency?</li> <li>What are the advantages and disadvantages of applying Early Binding in a real-time monitoring context?</li> <li>How could pre-computed values be stored to facilitate access and retrieval in critical situations?</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#case-3-university-exam-evaluation-system","title":"Case 3: University Exam Evaluation System","text":"<p>The university has developed an automated system to evaluate student exams. These exams consist of multiple-choice and open-ended questions. Student responses are stored in a matrix data structure for mass evaluations.</p> <p>In a recent incident, the system assigned incorrect grades due to the way responses were evaluated: all questions were evaluated in a fixed order without considering their type or complexity, which led to errors.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#aspects-to-consider_2","title":"Aspects to Consider","text":"<p>Storage: The students' responses are stored in a matrix, where each row represents a student, and each column represents a specific response.</p> <p>Optimization: To order the evaluations by probability of being correct or by type, optimizing the evaluation sequence and reducing errors.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#questions-for-students_2","title":"Questions for Students","text":"<ul> <li>How could the evaluation system be optimized so that the most probable answers are evaluated first?</li> <li>What improvements could be made to the matrix data structure to enhance the efficiency of the evaluation system?</li> <li>Should more extensive testing with different response patterns have been conducted to ensure system accuracy?</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#case-4-autonomous-delivery-robot-in-a-city","title":"Case 4: Autonomous Delivery Robot in a City","text":"<p>A logistics company has developed an autonomous robot to deliver packages in the city. The robot uses massive data collected by sensors and cameras to avoid obstacles and optimize its routes. This data is stored in a graph that models the city's street network.</p> <p>Recently, the robot hit a pedestrian while trying to avoid a parked car. After an investigation, it was discovered that the robot was overloaded with unnecessary data, which affected its ability to react quickly.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#aspects-to-consider_3","title":"Aspects to Consider","text":"<p>Storage: The data is stored in a graph, where nodes represent intersections and edges represent streets.</p> <p>Optimization: Simplify the navigation code and eliminate unnecessary complexity, optimizing the robot's response to unexpected situations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#questions-for-students_3","title":"Questions for Students","text":"<ul> <li>How could navigation algorithms be optimized to improve the robot's real-time reaction?</li> <li>Which parts of the graph could be simplified to avoid data overload and improve efficiency?</li> <li>How could storing data in a graph influence the robot's efficiency, and what adjustments could be made to optimize it?</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#case-5-energy-demand-prediction-system","title":"Case 5: Energy Demand Prediction System","text":"<p>An electric company has developed a system to predict energy demand using historical consumption data. The data is stored in a distributed database due to the large volume of information from millions of households. Recently, however, the system issued incorrect predictions during a heatwave, leading to unexpected blackouts.</p> <p>Upon investigation, it was found that the system's algorithms were based on complex models that tried to capture too many variables, including some that were not relevant to the prediction.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#aspects-to-consider_4","title":"Aspects to Consider","text":"<p>Storage: The historical data is stored in a distributed database, which allows for massive analysis and querying of consumption patterns.</p> <p>Optimization: Reduce the model's complexity by focusing only on the most relevant variables for prediction, thus increasing accuracy and reducing computational overload.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1/#questions-for-students_4","title":"Questions for Students","text":"<ul> <li>What variables could have been eliminated from the model to reduce complexity without sacrificing accuracy?</li> <li>How could the distributed database be improved to store only truly useful and relevant data?</li> <li>What strategies could be implemented to simplify the demand prediction problem and optimize the system's responsiveness?</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1_solutions/","title":"Case Studies - Solutions","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1_solutions/#case-1-emergency-response-system-with-autonomous-drones","title":"Case 1: Emergency Response System with Autonomous Drones","text":"<p>A team has developed an autonomous drone system to assist in natural disaster situations. The drones collect environmental data (wind, temperature, humidity), and their main function is to deliver essential supplies. The data is stored in a relational database, which is shared with other emergency systems in real time.</p> <p>During a mission in a mountainous area, one of the drones lost control and crashed into a temporary facility where refugees were sheltered. After investigation, it was discovered that the drone's navigation system did not properly assess rapid changes in wind in mountainous regions.</p> <p>Aspects to Consider: Relentless Suspicion</p> <p>Storage: The environmental data collected by the drones is stored in a centralized relational database that allows for quick queries.</p> <p>Optimization: Relentless Suspicion should have been applied, questioning the necessity of each instruction in the navigation algorithm and verifying if the wind data was sufficient to ensure a safe flight.</p> <p>Questions for Students:</p> <p>Is it ethical to deploy an autonomous system without considering all possible critical navigation scenarios? Should more extensive testing have been conducted on the stored data to cover situations with extreme winds? How could the databases be improved to store more useful information? What type of data structure or format could be used to improve the real-time processing of this environmental data?</p> <p>Answers for Case 1:</p> <p>Is it ethical to deploy an autonomous system without considering all possible critical navigation scenarios? Answer: No, it is not ethical. Launching an autonomous system that interacts with people must include a thorough risk analysis. Considering all possible critical scenarios, especially those that could endanger human lives, is essential. To minimize these risks, ethical engineering should be applied, along with methods such as risk modeling and simulated environment testing to ensure coverage of the most critical cases.</p> <p>Should more extensive testing have been conducted on the stored data to cover situations with extreme winds? How could the databases be improved to store more useful information? Answer: Yes, more exhaustive testing should have been conducted, particularly regarding wind data and extreme conditions in the region. To improve the database, tags and metadata related to extreme weather conditions could be added. Additionally, a hierarchical data structure could be used to prioritize access to critical information during emergencies.</p> <p>What type of data structure or format could be used to improve the real-time processing of this environmental data? Answer: A time series database would be ideal for storing environmental data, as it allows for efficient querying of time-related information. Moreover, this data could be complemented with spatial indexing techniques to facilitate quick access to critical data in specific locations, improving the system's ability to react in real time.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1_solutions/#case-2-real-time-medical-evaluation-platform","title":"Case 2: Real-Time Medical Evaluation Platform","text":"<p>A clinic has implemented a platform for the real-time monitoring of vital signs of elderly patients. Data is continuously generated from wearable sensors and stored in a real-time database (a streaming data system). The platform should alert relatives or medical staff when values outside normal parameters are detected.</p> <p>Recently, one of the devices failed, and the system did not issue the necessary alert in time. The reason was that the system constantly recalculated each value, and due to the large amount of data, delays occurred.</p> <p>Aspects to Consider: Early Binding</p> <p>Storage: The data is stored in a real-time database, where it is processed as it is received.</p> <p>Optimization: Applying Early Binding would have allowed pre-computation of certain critical indicators, storing them for reuse, which would have reduced processing times and ensured a better response during emergencies.</p> <p>Questions for Students:</p> <p>How can the data flow be optimized so that the system responds more efficiently during an emergency? What are the advantages and disadvantages of applying Early Binding in a real-time monitoring context? How could pre-computed values be stored to facilitate access and retrieval in critical situations?</p> <p>Answers for Case 2:</p> <p>How can the data flow be optimized so that the system responds more efficiently during an emergency? Answer: Data flow can be optimized by applying pre-processing of critical data (Early Binding), storing key values such as average heart rate or alert thresholds in advance. Additionally, utilizing a streaming system that applies real-time filtering to ensure only relevant data reaches emergency services would further improve response efficiency.</p> <p>What are the advantages and disadvantages of applying Early Binding in a real-time monitoring context? Answer: The main advantage is the reduction in response time since critical calculations are already pre-computed. This is crucial for timely responses in medical emergencies. The main disadvantage is that more memory is required to store these pre-computed values, and problems could arise if the health status changes rapidly, making pre-computed values obsolete.</p> <p>How could pre-computed values be stored to facilitate access and retrieval in critical situations? Answer: Pre-computed values could be stored in an in-memory database such as Redis, which allows ultra-fast access to information. A cache could also be used for the most frequently queried values, reducing latency in emergency situations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1_solutions/#case-3-university-exam-evaluation-system","title":"Case 3: University Exam Evaluation System","text":"<p>The university has developed an automated system to evaluate student exams. These exams consist of multiple-choice and open-ended questions. Student responses are stored in a matrix data structure for mass evaluations.</p> <p>In a recent incident, the system assigned incorrect grades due to the way responses were evaluated: all questions were evaluated in a fixed order without considering their type or complexity, which led to errors.</p> <p>Aspects to Consider: Ordering Tests</p> <p>Storage: The students' responses are stored in a matrix, where each row represents a student and each column represents a specific response.</p> <p>Optimization: Applying Ordering Tests would have allowed the evaluations to be ordered by probability of being correct or by type, optimizing the evaluation sequence and reducing errors.</p> <p>Questions for Students:</p> <p>How could the evaluation system be optimized so that the most probable answers are evaluated first? What improvements could be made to the matrix data structure to enhance the efficiency of the evaluation system? Should more extensive testing with different response patterns have been conducted to ensure system accuracy?</p> <p>Answers for Case 3:</p> <p>How could the evaluation system be optimized so that the most probable answers are evaluated first? Answer: The Ordering Tests technique could be applied, analyzing historical patterns to determine which responses are most frequently correct and processing these first. A machine learning system could also be used to learn these patterns and continuously optimize the evaluation order based on previous responses.</p> <p>What improvements could be made to the matrix data structure to enhance the efficiency of the evaluation system? Answer: The matrix could be re-ordered to place the most common questions first, facilitating faster access. Additionally, a sparse matrix could be used to store only non-null responses, reducing the amount of data to process and improving efficiency.</p> <p>Should more extensive testing with different response patterns have been conducted to ensure system accuracy? Answer: Yes, more exhaustive testing with various response patterns is essential to ensure system robustness. This includes simulating correct and incorrect responses and using random testing techniques to verify how the system behaves in different situations, ensuring no biases in evaluation.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1_solutions/#case-4-autonomous-delivery-robot-in-a-city","title":"Case 4: Autonomous Delivery Robot in a City","text":"<p>A logistics company has developed an autonomous robot to deliver packages in the city. The robot uses massive data collected by sensors and cameras to avoid obstacles and optimize its routes. This data is stored in a graph that models the city's street network.</p> <p>Recently, the robot hit a pedestrian while trying to avoid a parked car. After an investigation, it was discovered that the robot was overloaded with unnecessary data, which affected its ability to react quickly.</p> <p>Aspects to Consider: Code Simplification</p> <p>Storage: The data is stored in a graph, where nodes represent intersections and edges represent streets.</p> <p>Optimization: Applying Code Simplification would have simplified the navigation code and eliminated unnecessary complexity, optimizing the robot's response to unexpected situations.</p> <p>Questions for Students:</p> <p>How could navigation algorithms be optimized to improve the robot's real-time reaction? Which parts of the graph could be simplified to avoid data overload and improve efficiency? How could storing data in a graph influence the robot's efficiency, and what adjustments could be made to optimize it?</p> <p>Answers for Case 4:</p> <p>How could navigation algorithms be optimized to improve the robot's real-time reaction? Answer: Navigation algorithms could be optimized by applying Code Simplification, eliminating redundant or unnecessary calculations, and focusing on critical decision-making functions. Implementing a Kalman filter would also help smooth sensor measurements, allowing the robot to make quicker and more accurate decisions.</p> <p>Which parts of the graph could be simplified to avoid data overload and improve efficiency? Answer: Irrelevant nodes (such as dead-end streets or restricted-access routes) could be removed from the graph to reduce route complexity. Additionally, grouping similar streets into supernodes could help simplify the network and reduce the computational cost associated with route searching.</p> <p>How could storing data in a graph influence the robot's efficiency, and what adjustments could be made to optimize it? Answer: Storing data in a graph allows for structured navigation but can be inefficient if there are too many unnecessary nodes and edges. Using a hierarchical graph representation, where major routes are evaluated first, could improve the robot's efficiency. Implementing an A* search algorithm would also be a significant improvement for quickly finding optimal routes.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/exercisesapplications1_solutions/#case-5-energy-demand-prediction-system","title":"Case 5: Energy Demand Prediction System","text":"<p>An electric company has developed a system to predict energy demand using historical consumption data. The data is stored in a distributed database due to the large volume of information from millions of households. Recently, however, the system issued incorrect predictions during a heatwave, leading to unexpected blackouts.</p> <p>Upon investigation, it was found that the system's algorithms were based on complex models that tried to capture too many variables, including some that were not relevant to the prediction.</p> <p>Aspects to Consider: Problem Simplification</p> <p>Storage: The historical data is stored in a distributed database, which allows for massive analysis and querying of consumption patterns.</p> <p>Optimization: Applying Problem Simplification would have reduced the model's complexity by focusing only on the most relevant variables for prediction, thus increasing accuracy and reducing computational overload.</p> <p>Questions for Students:</p> <p>What variables could have been eliminated from the model to reduce complexity without sacrificing accuracy? How could the distributed database be improved to store only truly useful and relevant data? What strategies could be implemented to simplify the demand prediction problem and optimize the system's responsiveness?</p> <p>Answers for Case 5:</p> <p>What variables could have been eliminated from the model to reduce complexity without sacrificing accuracy? Answer: Variables that do not have a significant correlation with energy demand, such as irrelevant weather factors, could be eliminated. Dimensionality reduction techniques like Principal Component Analysis (PCA) can help identify less relevant variables and remove them from the model.</p> <p>How could the distributed database be improved to store only truly useful and relevant data? Answer: Implementing data cleaning algorithms that filter and discard obsolete or redundant data would help maintain only relevant information. Using advanced indexing would also enable more efficient storage of key data, improving query and analysis capabilities.</p> <p>What strategies could be implemented to simplify the demand prediction problem and optimize the system's responsiveness? Answer: Applying Problem Simplification involves reducing the number of factors to only those strictly necessary for prediction. Additionally, a reinforcement learning-based model could be applied to optimize predictions by adapting to changing demand patterns, which would reduce complexity and increase efficiency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/","title":"Experiments \u2013 Monitoring and Performance Engineering","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#introduction","title":"Introduction","text":"<p>\"Programming is an attempt to capture the uniformity of values and processes in actors.\" \u2014 Robin Milner (1934\u20132010)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#brief-biography","title":"Brief Biography","text":"<p>Robin Milner was a British computer scientist renowned for his foundational contributions to programming language theory and formal methods. He developed the ML programming language, one of the earliest to feature type inference and functional programming constructs that influenced modern languages such as Haskell, OCaml, and F#. Milner also created the \u03c0-calculus, a formal model for describing concurrent systems, and made pioneering advances in theoretical computer science related to polymorphism and process calculi. He was a professor at the University of Edinburgh and a fellow of the Royal Society. In 1991, he received the Turing Award for \u201cthree distinct and complete achievements: LCF, ML, and CCS,\u201d which laid the groundwork for much of modern programming language design and verification.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#performance-engineering","title":"Performance Engineering","text":"<p>Performance engineering focuses on understanding and improving how programs execute. It is not only about making code work correctly, but also about ensuring it runs efficiently and scales well. This section explores experimental insights into performance monitoring, optimization, and computational efficiency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#measuring-performance","title":"Measuring Performance","text":"<p>Performance can be measured by analyzing execution time under different computational setups. Consider the case of multiplying two 1024\u00d71024 matrices:</p> <p></p> <ul> <li>Case A: Running time = 1.32 seconds  </li> <li>Case B: Running time = 7.76 seconds </li> </ul> <p>Matrix Multiply Slow </p> <p>Matrix Multiply Fast</p> <p>The only difference between the two implementations is the loop order. This affects how memory is accessed, leading to variations in performance by a factor of up to six. Such behavior arises from data locality, a key concept in performance analysis.  </p> <p>Data that is accessed consecutively tends to remain in fast memory (cache), resulting in quicker computations. When data is scattered, cache performance degrades, leading to slower execution.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#cache-optimization","title":"Cache Optimization","text":"<p>Cache memory provides rapid access to frequently used data. When a requested piece of data is already in cache, a cache hit occurs, which is fast. Otherwise, a cache miss forces the processor to retrieve data from slower main memory.</p> <p>To optimize cache usage: - Reorganize computations to reuse data already loaded into cache. - Access memory in sequential order whenever possible. - Choose data structures that map efficiently to cache lines.  </p> <p>Even small changes in loop nesting or traversal order can significantly improve performance without altering the algorithm itself.</p> <p>Matrix Multiply Blocked (or Tiling) </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#this-implementation-demonstrates-cache-blocking-or-tiling-which-improves-performance-by-maximizing-cache-reuse","title":"This implementation demonstrates cache blocking (or tiling), which improves performance by maximizing cache reuse.","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#algorithmic-optimization","title":"Algorithmic OptimizationContextoIdea b\u00e1sica del algoritmoEficienciaImplicaciones pr\u00e1cticasEjemplo num\u00e9rico simple","text":"<p>Optimization is also achieved through algorithm design. For instance, matrix multiplication can be performed using algorithms with lower theoretical complexity:</p> <ul> <li>Strassen algorithm: (O(n^{2.807}))  </li> <li>Coppersmith\u2013Winograd algorithm: (O(n^{2.376}))  </li> <li>Optimized CW-like algorithms: (O(n^{2.373}))</li> </ul> <p>These methods, often called galactic algorithms, are mainly of theoretical interest since they are rarely used in practical systems. Nonetheless, they illustrate the boundaries of what is computationally possible.</p> Algoritmo de Strassen <p>     La multiplicaci\u00f3n cl\u00e1sica de dos matrices A y B de tama\u00f1o n\u00d7n requiere:   </p> <pre><code>O(n^3)</code></pre> <p>     operaciones (exactamente <code>n^3</code> multiplicaciones y <code>n^3 \u2212 n^2</code> sumas).   </p> <p>     Strassen demostr\u00f3 que no era necesario realizar tantas multiplicaciones y que se pod\u00eda mejorar la complejidad algor\u00edtmica.   </p> <p>Para dos matrices <code>2\u00d72</code>:</p> <pre><code>A = [[a, b],\n     [c, d]]\n\nB = [[e, f],\n     [g, h]]</code></pre> <p>La multiplicaci\u00f3n cl\u00e1sica necesita 8 multiplicaciones.</p> <p>Strassen descubri\u00f3 una forma de calcular el producto usando solo 7 multiplicaciones (y m\u00e1s sumas/restas):</p> <pre><code>M1 = (a + d) (e + h)\nM2 = (c + d) e\nM3 = a (f \u2212 h)\nM4 = d (g \u2212 e)\nM5 = (a + b) h\nM6 = (c \u2212 a) (e + f)\nM7 = (b \u2212 d) (g + h)</code></pre> <p>Luego, el resultado <code>C = A \u00d7 B</code> se obtiene como:</p> <pre><code>C11 = M1 + M4 \u2212 M5 + M7\nC12 = M3 + M5\nC21 = M2 + M4\nC22 = M1 \u2212 M2 + M3 + M6</code></pre> <p>Si se aplica recursivamente a matrices grandes, dividi\u00e9ndolas en submatrices, la complejidad pasa de:</p> <pre><code>O(n^3)  \u2192  O(n^{log2(7)}) \u2248 O(n^{2.81})</code></pre> <p>Esto fue un avance enorme, porque por primera vez se demostr\u00f3 que la multiplicaci\u00f3n de matrices pod\u00eda hacerse en menos de tiempo c\u00fabico.</p> <ul> <li>Strassen es m\u00e1s r\u00e1pido para matrices grandes, pero requiere m\u00e1s memoria.</li> <li>Introduce m\u00e1s operaciones de suma/resta, lo que puede aumentar los errores num\u00e9ricos en c\u00e1lculos con coma flotante.</li> <li>En la pr\u00e1ctica, los algoritmos modernos (como Coppersmith\u2013Winograd o algoritmos h\u00edbridos) solo aplican Strassen a partir de cierto tama\u00f1o de matriz.</li> </ul> <p>     Si multiplicas dos matrices <code>4\u00d74</code>, puedes dividirlas en 8 submatrices     <code>2\u00d72</code> y aplicar Strassen recursivamente. De este modo, se reducen multiplicaciones,     aunque aumentan las sumas.   </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#density-and-sparsity","title":"Density and Sparsity","text":"<p>In many applications, data structures like matrices are mostly empty. Density measures the proportion of non-zero elements, while sparsity measures the proportion of zeros.</p> <p>For example, a 6\u00d76 matrix with 9 non-zero elements has:  </p> <p>Density = 9/36 = 25%,  Sparsity = 75%.</p> <p>Real-world matrices often have extremely low densities (e.g., 0.0003% or 2.31%). Managing such sparse data efficiently requires specialized compression techniques.</p> <p>Example running times for 1024\u00d71024 matrices:</p> Density Time (seconds) 0.1% 0.02 1% 0.08 2% 0.14 3% 0.23 <p>As density increases, the amount of computation and storage requirements grow proportionally.</p> <p>Dense vs CRS </p>  Runing times test <p>Correction: The checksums are identical and the max error is 0 \u2192 both implementations compute the same result. 100%</p> <p>Performance: With <code>n = 512</code> and density = <code>0.25</code> (25%), it\u2019s normal for the dense version (0.013 s) to outperform the sparse/CSR one (0.031 s): at such densities, the overhead of managing indices and irregular memory accesses usually outweighs the savings from skipping zeros; sparse computation is typically memory-bound and suffers from poor cache locality, whereas the dense version makes better use of the hardware.</p> <p>The complexity of sparse operations scales with <code>nnz</code> (not with <code>n\u00b2</code>), but at 25% <code>nnz</code> is already large, so the benefit is diluted. In addition, <code>B*B</code> introduces fill-in (new non-zeros appear), so the product can be much less sparse than <code>B</code> and further worsen performance.</p> Rule of thumb <p>Sparse methods tend to win when the matrix is very sparse (\u224810% or less, and often much less), and/or when <code>n</code> is large; otherwise, dense methods are usually faster.</p> What to test to \u201csee\u201d the crossover <ul> <li>Lower the density (e.g., 1%, 2%, 5%) and increase the size (<code>n \u2265 2000</code>).</li> <li>Repeat several iterations to warm up the JVM and take the best/median time.</li> <li>Confirm you\u2019re using CSR with primitive arrays (avoid autoboxing/objects).</li> <li>If you compare against a highly optimized dense implementation (BLAS/Vector API), expect it to win even at fairly low densities.</li> </ul> Conclusion <p>Your output makes sense; at 25% and <code>n = 512</code>, it\u2019s expected that the dense version is about 2\u20133\u00d7 faster than the sparse one.</p>  Runing times test <p>java DenseVsCSRSquare 1024 0.01 </p> <p>java DenseVsCSRSquare 1024 0.001</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#storage-formats-for-sparse-data","title":"Storage Formats for Sparse Data","text":"<p>In Performance Engineering, the way data is stored in memory has a major impact on computational efficiency. This is especially true for sparse matrices, where most elements are zero and storing all entries would waste both memory and processing time.</p> <p>Several specialized formats have been developed to store only the non-zero values and their positions efficiently. The choice of format depends on the access pattern (by rows, by columns, or by diagonals) and on the type of operations to be performed. Dense matrices are commonly stored as two-dimensional arrays, but sparse matrices require alternative formats to avoid wasting memory. Common representations include:</p> <ul> <li>COO (Coordinate Format) </li> <li>CRS (Compressed Row Storage) </li> <li>CCS (Compressed Column Storage) </li> <li>CDS (Compressed Diagonal Storage) </li> <li>JDS (Jagged Diagonal Storage)</li> </ul> <p>Each format balances storage efficiency and computational performance differently. Choosing the right one depends on access patterns, density level, and the algorithm used.</p> <p>Click on each format below to expand its description and example.</p> COO (Coordinate Format) <p>       The Coordinate (COO) format stores each non-zero entry of a sparse matrix       as an explicit triplet <code>(row, column, value)</code>.     </p> <p>Example: 4\u00d74 matrix A</p> 0500 0080 0003 1000 <p>COO triplets (row, col, value)</p> RowColValue 015 128 233 301 <p>Advantages</p> <ul> <li>Very easy to construct and understand.</li> <li>Good for incremental matrix assembly.</li> </ul> <p>Disadvantages</p> <ul> <li>Poor for arithmetic or repeated access (needs scanning all triplets).</li> <li>Usually converted to CSR/CCS for fast computations.</li> </ul> CRS (Compressed Row Storage) <p>       The Compressed Row Storage (CRS) format, also known as        Compressed Sparse Row (CSR), stores only the non-zero values of the matrix       row by row, together with their column indices and row pointers.     </p> <p>Example: 4\u00d74 matrix A</p> 0500 0080 0003 1000 <p>CRS Representation</p> <p>       The matrix is represented using three arrays:     </p> <ul> <li><code>values</code> \u2192 stores all non-zero elements row by row</li> <li><code>col_index</code> \u2192 stores the column index of each non-zero element</li> <li><code>row_ptr</code> \u2192 marks the starting index of each row in the <code>values</code> array</li> </ul> ArrayContentDescription <code>values</code> [5, 8, 3, 1] Non-zero values in row order <code>col_index</code> [1, 2, 3, 0] Column indices of each value <code>row_ptr</code> [0, 1, 2, 3, 4] Start of each row in <code>values</code> <p>How it works</p> <p>       Each pair (<code>values[k]</code>, <code>col_index[k]</code>) gives the value and column position        of a non-zero element. The array <code>row_ptr</code> shows where each row begins in <code>values</code>.     </p> <p>Advantages</p> <ul> <li>Efficient for row-wise operations (e.g., matrix-vector multiplication).</li> <li>Compact memory usage for sparse matrices.</li> </ul> <p>Disadvantages</p> <ul> <li>Column access is slower (requires full row scan).</li> <li>More complex to modify dynamically.</li> </ul> CCS (Compressed Column Storage) <p>       The Compressed Column Storage (CCS) format, also known as        Compressed Sparse Column (CSC), is the column-oriented version of CRS.       It stores all non-zero values column by column, together with their row indices       and column pointers.     </p> <p>Example: 4\u00d74 matrix A</p> 0500 0080 0003 1000 <p>CCS Representation</p> <p>The matrix is represented using three arrays:</p> <ul> <li><code>values</code> \u2192 stores the non-zero elements column by column</li> <li><code>row_index</code> \u2192 stores the row index of each non-zero element</li> <li><code>col_ptr</code> \u2192 marks the starting index of each column in the <code>values</code> array</li> </ul> ArrayContentDescription <code>values</code> [1, 5, 8, 3] Non-zero values in column order <code>row_index</code> [3, 0, 1, 2] Row indices corresponding to each value <code>col_ptr</code> [0, 1, 2, 3, 4] Start of each column in <code>values</code> <p>How it works</p> <p>       Each pair (<code>values[k]</code>, <code>row_index[k]</code>) represents a non-zero entry,       where <code>col_ptr</code> indicates the start of each column\u2019s data.       This structure is especially efficient for operations that proceed column by column.     </p> <p>Advantages</p> <ul> <li>Efficient for column-wise operations and solving sparse linear systems.</li> <li>Compact and easy to convert from CRS (by transposing the matrix).</li> </ul> <p>Disadvantages</p> <ul> <li>Row access is slower (requires scanning the whole column).</li> <li>Not ideal for dynamic updates or incremental builds.</li> </ul> CDS (Compressed Diagonal Storage) <p>       The Compressed Diagonal Storage (CDS) format (also called       Diagonal or Banded storage) stores only the diagonals that contain       non-zero elements. Each diagonal is identified by its <code>offset</code> <code>k = col - row</code> (main diagonal <code>k = 0</code>, upper diagonals <code>k &gt; 0</code>, lower diagonals <code>k &lt; 0</code>).     </p> <p>Example: 5\u00d75 matrix A (non-zeros on k = -1, 0, 1)</p> 14000 25700 03680 00971 00024 <p>Diagonal offsets</p> <p> <code>diag_offsets = [-1, 0, 1]</code> </p> <p>Compact diagonal data (per offset)</p> k (offset)Data (compact)Length -1[2, 3, 9, 2]4  0[1, 5, 6, 7, 4]5 +1[4, 7, 8, 1]4 <p>Padded (aligned by column index, optional)</p> kc=0c=1c=2c=3c=4 -12392  015674 +14781 <p>How it works</p> <ul> <li>Only selected diagonals are stored, identified by <code>diag_offsets</code>.</li> <li>Each diagonal is saved as a compact vector (optionally padded to length <code>n</code> for alignment).</li> <li>Multiplications and solves that access along diagonals benefit from locality and reduced storage.</li> </ul> <p>Advantages</p> <ul> <li>Excellent for banded matrices with small bandwidth (e.g., tridiagonal, pentadiagonal).</li> <li>Very compact and cache-friendly for near-diagonal sparsity patterns.</li> <li>Simplifies specialized algorithms (Thomas algorithm, banded solvers).</li> </ul> <p>Disadvantages</p> <ul> <li>Not suitable for general sparsity patterns (non-zeros far from diagonals).</li> <li>Number of stored diagonals must be known; adding arbitrary entries may require format changes.</li> <li>Conversions to/from CSR/CCS may be needed for generic sparse operations.</li> </ul> JDS (Jagged Diagonal Storage) <p>       The Jagged Diagonal Storage (JDS) format is a variant of the diagonal approach       that reorganizes the matrix rows according to the number of non-zero elements       they contain. Rows with more non-zeros appear first, producing \u201cjagged\u201d diagonals       of decreasing length.     </p> <p>       The goal of JDS is to improve cache efficiency and enable vectorization        in matrix-vector multiplications, especially on vector or SIMD architectures.     </p> <p>Example: 5\u00d75 matrix A</p> 05000 10800 20030 00004 70000 <p>Step 1: Count non-zeros per row</p> RowNon-zeros 01 12 22 31 41 <p>Step 2: Reorder rows (descending by non-zeros)</p> <p>       New row order \u2192 [1, 2, 0, 3, 4]     </p> <p>Step 3: Build jagged diagonals</p> DiagonalValuesColumn Indices 0[1, 2, 0, 0, 7][0, 0, 1, 3, 4] 1[8, 3, 5, 4][2, 3, 1, 4] <p>Additional arrays:</p> <ul> <li><code>perm</code> \u2192 stores the row permutation [1, 2, 0, 3, 4]</li> <li><code>jd_ptr</code> \u2192 starting index of each jagged diagonal [0, 5, 9]</li> <li><code>col_index</code> \u2192 flattened list of all column indices [0,0,1,3,4,2,3,1,4]</li> <li><code>values</code> \u2192 flattened list of all non-zero values [1,2,0,0,7,8,3,5,4]</li> </ul> <p>How it works</p> <ul> <li>Rows are sorted by decreasing number of non-zero elements.</li> <li>Each \u201cdiagonal\u201d stores one element per row, resulting in irregular (jagged) lengths.</li> <li>Access is highly sequential, optimizing vector and GPU execution.</li> </ul> <p>Advantages</p> <ul> <li>Optimized for vectorized matrix-vector multiplication.</li> <li>Improves cache usage due to data locality in flattened arrays.</li> <li>Good for hardware with SIMD or streaming access patterns.</li> </ul> <p>Disadvantages</p> <ul> <li>More complex to build and interpret than CSR/COO.</li> <li>Less efficient for random access or element updates.</li> <li>Row order must be stored separately for reconstruction.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/","title":"Lectures \u2013 Monitoring and Performance Engineering","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#introduction","title":"Introduction","text":"<p>Programming is the art of telling another human being what one wants the computer to do. Donald Knuth</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#brief-biography","title":"Brief Biography","text":"<p>Donald Ervin Knuth, born on January 10, 1938, in Milwaukee, Wisconsin, USA, is a computer scientist and professor emeritus at Stanford University, best known as the author of The Art of Computer Programming, one of the most influential works in the field. Knuth studied mathematics at Case Institute of Technology and earned his Ph.D. in mathematics from the California Institute of Technology (Caltech). Often referred to as the \u201cfather of algorithm analysis,\u201d he pioneered many of the foundational methods for analyzing algorithm efficiency. He is also the creator of the TeX typesetting system and the METAFONT font design system, both of which revolutionized scientific publishing. His meticulous approach to computer science has inspired generations of researchers and programmers. Throughout his career, Knuth has received numerous honors, including the Turing Award, the National Medal of Science, and the John von Neumann Medal. He continues to work on later volumes of The Art of Computer Programming and maintains his commitment to the precision and beauty of computing.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#improving-performance","title":"Improving Performance","text":"<p>The challenge of performance engineering lies in handling the increasing size and complexity of computations. Several strategies can help address this:</p> <ul> <li> <p>Faster processors: Following Moore\u2019s law, the number of transistors doubles approximately every two years, leading to faster chips. </p> </li> <li> <p>Parallelism: Use multicore processors, GPUs, or clusters to execute computations simultaneously. </p> </li> <li> <p>Software optimization: Adapt algorithms to take advantage of hardware and domain-specific structures. </p> </li> </ul> <p></p> <p>Moore's Law: The number of transistors on microchips doubles every two years</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#bentleys-rules-for-performance","title":"Bentley\u2019s Rules for Performance","text":"<p>Jon Bentley proposed four practical rules to improve program performance. Each focuses on simplifying and optimizing both problem and implementation.</p> <p>Code Simplification Fast programs are simple. Keep code minimal and clean so that it is easier to optimize and maintain. Key takeaway: Simplicity enhances speed and reliability.</p> <p>Don't do it The fastest way to execute an operation is not to do it at all. Often, part of the code performs unnecessary or repetitive tasks that can be eliminated.</p> <p>Example: avoid redundant calculations inside loops, remove dead code, or prevent unnecessary disk or network accesses.</p>  Code Simplification <p>Imagine cooking a recipe with twenty unnecessary steps \u2014every time you switch utensils or ingredients, you lose time and coordination.</p> <p>If you simplify the recipe to the essential steps, you finish faster and make fewer mistakes.</p> <p>Cooking well and quickly means keeping only what\u2019s truly needed.</p> <p>Code Simplification</p> <p>Problem Simplification Simplify the problem itself before optimizing the code. Reducing complexity in the problem often yields a more efficient solution. Key takeaway: A simpler problem produces a faster solution.</p> <p>Do it, but don\u2019t do it again</p> <p>If an operation must be done, store the result so it doesn\u2019t need to be recalculated. This is known as memoization or caching. Example: store the results of an API query or an expensive function in memory.</p>  Problem Simplification <p>Suppose you need to organize a library. Instead of sorting all the books in the world by author and date, you decide to organize only the books you actually own.</p> <p>By redefining the problem, you can move faster and get better results.</p> <p>Before solving a problem, make sure you\u2019re solving the right one \u2014 and at the right scale.</p> <p>Problem Simplification</p> <p>Relentless Suspicion Question every instruction in time-critical code and every field in space-critical data structures. Key takeaway: Constantly verify the necessity of each operation.</p> <p>Do it less</p> <p>Reduce the amount of work performed. Optimize loops, divide the problem, or use more efficient data structures. Example: replace a linear search (O(n)) with a binary search (O(log n)).</p>  Relentless Suspicion <p>Think about packing a suitcase for a trip. Every item you pack takes up space and adds weight. If you look critically (\u201cDo I really need this?\u201d), you\u2019ll realize many things are unnecessary.</p> <p>The lighter the suitcase, the easier it is to move and the less time you spend searching through it.</p> <p>Doubt everything that doesn\u2019t add real value; lightness improves performance.</p> <p>Relentless Suspicion</p> <p>Early Binding Perform computations earlier to avoid redundant work later. Precomputing results or decisions saves time during execution. Key takeaway: Shift work forward when possible to minimize repetition.</p> <p>Do it better</p> <p>If something cannot be removed or reduced, do it more efficiently. This involves using better algorithms or data structures, taking advantage of parallelism, or adapting the code to the hardware.  </p> <p>Example: use a faster sorting algorithm, parallelize tasks, or vectorize operations.</p>  Early Binding <p>It\u2019s like ironing your clothes right after washing them instead of every morning before leaving home.</p> <p>You put in effort once, and then you benefit for days.</p> <p>Doing work early prevents having to repeat it later and keeps things flowing smoothly.</p> <p>Early Binding</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#optimization-ethics","title":"Optimization Ethics","text":"<p>While performance optimization is crucial, it should not come prematurely. As two pioneers famously warned:</p> <p>Premature optimization is the root of all evil. \u2014 Donald Knuth The first rule of program optimization is \u201cDon\u2019t do it.\u201d The second rule (for experts only) is \u201cDon\u2019t do it yet.\u201d \u2014 Michael A. Jackson</p> <p>The goal of performance engineering is to first produce clean and correct code that behaves as expected. Only after correctness is guaranteed should optimization efforts begin.</p> <p>Performance engineering, therefore, is not only about increasing speed but about understanding, measuring, and refining computation to achieve a sustainable balance between accuracy, clarity, and efficiency.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures_code/","title":"Lectures \u2013 Monitoring and Performance Engineering - Code examples","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures_code/#code-simplification-dont-do-it","title":"Code Simplification -- Don't do it","text":"<pre><code>\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class DemoDontDoItBad {\n    public static int sumEven(int[] data) {\n        // Convertir a lista (innecesario)\n        List&lt;Integer&gt; asList = Arrays.stream(data).boxed().collect(Collectors.toList());\n\n        // Ordenar todo el array (no afecta a la suma)\n        Arrays.sort(data);\n\n        // Calcular una suma total que nunca se usa\n        int unusedTotal = 0;\n        for (int x : data) {\n            unusedTotal += x; // &lt;-- resultado no utilizado\n        }\n\n        // Lo \u00fanico que queremos: sumar pares (pero ya hemos hecho trabajo in\u00fatil arriba)\n        int evenSum = 0;\n        for (int x : data) {\n            if (x % 2 == 0) {\n                evenSum += x;\n            }\n        }\n        return evenSum;\n    }\n\n    public static void main(String[] args) {\n        int[] nums = {5, 2, 7, 8, 4, 3};\n        System.out.println(sumEven(nums)); // 14\n    }\n}\n</code></pre> <pre><code>public class DemoDontDoItGood {\n    public static int sumEven(int[] data) {\n        int evenSum = 0;\n        for (int x : data) {\n            if ((x &amp; 1) == 0) { // equivalente a x % 2 == 0, m\u00e1s barato\n                evenSum += x;\n            }\n        }\n        return evenSum;\n    }\n\n    public static void main(String[] args) {\n        int[] nums = {5, 2, 7, 8, 4, 3};\n        System.out.println(sumEven(nums)); // 14\n    }\n}\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures_code/#problem-simplification","title":"Problem Simplification","text":"<pre><code>public class DemoDoItAgainBad {\n\n    // Implementaci\u00f3n ineficiente: recalcula todo una y otra vez\n    public static long fibonacci(int n) {\n        if (n &lt;= 1) return n;\n        return fibonacci(n - 1) + fibonacci(n - 2);\n    }\n\n    public static void main(String[] args) {\n        long start = System.currentTimeMillis();\n        System.out.println(\"F(40) = \" + fibonacci(40));\n        System.out.println(\"Tiempo: \" + (System.currentTimeMillis() - start) + \" ms\");\n    }\n}\n</code></pre> <pre><code>import java.util.HashMap;\nimport java.util.Map;\n\npublic class DemoDoItAgainGood {\n\n    private static final Map&lt;Integer, Long&gt; memo = new HashMap&lt;&gt;();\n\n    public static long fibonacci(int n) {\n        if (n &lt;= 1) return (long) n;\n\n        // Si ya lo calculamos antes, lo devolvemos directamente\n        if (memo.containsKey(n)) {\n            return memo.get(n);\n        }\n\n        // Calculamos solo si es nuevo\n        long result = fibonacci(n - 1) + fibonacci(n - 2);\n        memo.put(n, result); // Guardamos el resultado\n        return result;\n    }\n\n    public static void main(String[] args) {\n        long start = System.currentTimeMillis();\n        System.out.println(\"F(40) = \" + fibonacci(40));\n        System.out.println(\"Tiempo: \" + (System.currentTimeMillis() - start) + \" ms\");\n    }\n}\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures_code/#relentless-suspicion","title":"Relentless Suspicion","text":"<pre><code>public class DemoDoItLessBad {\n    public static int countLetter(String text, char target) {\n        int count = 0;\n        for (int i = 0; i &lt; text.length(); i++) {  // correcto\n            // cada vez creamos una subcadena innecesaria\n            String letter = text.substring(i, i + 1);\n            if (letter.charAt(0) == target) {\n                count++;\n            }\n\n            // Comprobaci\u00f3n redundante en cada iteraci\u00f3n\n            if (i == text.length() - 1) {\n                System.out.println(\"Fin del bucle.\");  // in\u00fatil dentro del bucle\n            }\n        }\n        return count;\n    }\n\n    public static void main(String[] args) {\n        String phrase = \"banana\";\n        System.out.println(\"Cantidad de 'a': \" + countLetter(phrase, 'a'));\n    }\n}\n</code></pre> <pre><code>public class DemoDoItLessGood {\n    public static int countLetter(String text, char target) {\n        int count = 0;\n        int len = text.length(); // calculado una sola vez\n\n        for (int i = 0; i &lt; len; i++) {\n            if (text.charAt(i) == target) {\n                count++;\n            }\n        }\n\n        System.out.println(\"Fin del bucle.\"); // fuera del bucle\n        return count;\n    }\n\n    public static void main(String[] args) {\n        String phrase = \"banana\";\n        System.out.println(\"Cantidad de 'a': \" + countLetter(phrase, 'a'));\n    }\n}\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures_code/#early-binding","title":"Early Binding","text":"<pre><code>public class DemoDoItBetterBad {\n    // Busca linealmente: O(n)\n    public static boolean containsLinear(int[] sorted, int target) {\n        for (int x : sorted) {\n            if (x == target) return true;\n            // Incluso pudiendo cortar si x &gt; target, no lo hacemos\n        }\n        return false;\n    }\n\n    public static void main(String[] args) {\n        int[] data = {1, 4, 7, 9, 12, 15, 21, 34, 55, 89}; // ya ordenado\n        System.out.println(containsLinear(data, 21)); // true\n        System.out.println(containsLinear(data, 22)); // false\n    }\n}\n</code></pre> <pre><code>import java.util.Arrays;\n\npublic class DemoDoItBetterGood {\n    // Implementaci\u00f3n manual: O(log n)\n    public static boolean containsBinary(int[] sorted, int target) {\n        int lo = 0, hi = sorted.length - 1;\n        while (lo &lt;= hi) {\n            int mid = lo + ((hi - lo) &gt;&gt;&gt; 1); // evita overflow\n            if (sorted[mid] == target) return true;\n            if (sorted[mid] &lt; target) {\n                lo = mid + 1;\n            } else {\n                hi = mid - 1;\n            }\n        }\n        return false;\n    }\n\n    // Alternativa con la librer\u00eda est\u00e1ndar\n    public static boolean containsWithJdk(int[] sorted, int target) {\n        return Arrays.binarySearch(sorted, target) &gt;= 0;\n    }\n\n    public static void main(String[] args) {\n        int[] data = {1, 4, 7, 9, 12, 15, 21, 34, 55, 89};\n        System.out.println(containsBinary(data, 21));   // true\n        System.out.println(containsBinary(data, 22));   // false\n\n        System.out.println(containsWithJdk(data, 21));  // true\n        System.out.println(containsWithJdk(data, 22));  // false\n    }\n}\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/problemsimplification/","title":"Problem Simplification Examples","text":"<p>The Problem Simplification rule encourages developers to improve performance not just by optimizing code, but by simplifying the problem itself. Rather than performing unnecessary work, focus only on what the program truly needs to accomplish.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/problemsimplification/#example-1-sorting-the-entire-list-to-find-the-median","title":"Example 1 \u2014 Sorting the Entire List to Find the Median","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/problemsimplification/#code","title":"Code","text":"<pre><code>def find_median(numbers):\n    numbers.sort()  # Sort the entire list\n    n = len(numbers)\n    if n % 2 == 1:\n        return numbers[n // 2]  # If odd, return middle element\n    else:\n        return (numbers[n // 2 - 1] + numbers[n // 2]) / 2  # If even, average of middle elements\n\nnumbers = [5, 3, 8, 9, 1, 7, 4, 6, 2]\nprint(find_median(numbers))\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/problemsimplification/#issues","title":"Issues","text":"<ul> <li>Time Complexity: Sorting the entire list requires O(n log n) time, which is inefficient for large datasets.</li> <li>Unnecessary Work: Sorting all elements is excessive when we only need the median.</li> </ul> <p>This version works correctly but performs more computation than necessary, as it sorts the whole list to find a single value.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/problemsimplification/#example-2-simplifying-the-problem-using-quickselect","title":"Example 2 \u2014 Simplifying the Problem Using Quickselect","text":"<p>Instead of sorting the entire list, we can use the Quickselect algorithm, which finds the k-th smallest element in average time O(n). This approach directly targets what we need\u2014the median\u2014without doing extra work.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/problemsimplification/#code_1","title":"Code","text":"<pre><code>import random\n\ndef quickselect(nums, k):\n    if len(nums) == 1:\n        return nums[0]\n\n    pivot = random.choice(nums)\n    lows  = [el for el in nums if el &lt; pivot]\n    highs = [el for el in nums if el &gt; pivot]\n    pivots = [el for el in nums if el == pivot]\n\n    if k &lt; len(lows):\n        return quickselect(lows, k)\n    elif k &lt; len(lows) + len(pivots):\n        return pivots[0]\n    else:\n        return quickselect(highs, k - len(lows) - len(pivots))\n\ndef find_median(numbers):\n    n = len(numbers)\n    if n % 2 == 1:\n        return quickselect(numbers, n // 2)\n    else:\n        left = quickselect(numbers, n // 2 - 1)\n        right = quickselect(numbers, n // 2)\n        return (left + right) / 2\n\nnumbers = [5, 3, 8, 9, 1, 7, 4, 6, 2]\nprint(find_median(numbers))\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/problemsimplification/#benefits-of-problem-simplification","title":"Benefits of Problem Simplification","text":"<ol> <li>Reduced Complexity: The Quickselect algorithm achieves average-case O(n) time, improving upon the O(n log n) complexity of sorting.</li> <li>Efficiency: By simplifying the problem\u2014finding only the median instead of fully sorting\u2014the program eliminates unnecessary computation.</li> <li>Scalability: This approach scales better with large datasets, maintaining fast performance even when n is large.</li> </ol> <p>These examples demonstrate Bentley\u2019s principle: simplifying the problem itself often yields greater performance improvements than micro-optimizing code.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/","title":"Relentless Suspicion \u2014 Examples","text":"<p>Principle. Question the necessity of each instruction in a time\u2011critical piece of code and each field in a space\u2011critical data structure. Remove redundant work, use optimized primitives, and keep data minimal.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#example-1-inefficient-sorting-with-redundant-operations","title":"Example 1 \u2014 Inefficient Sorting with Redundant Operations","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#code","title":"Code","text":"<pre><code>def time_critical_sort(nums):\n    result = nums.copy()  # Unnecessary copy\n    for i in range(len(result)):\n        for j in range(i + 1, len(result)):\n            if result[i] &gt; result[j]:\n                # Swap elements (bubble/selection-like)\n                result[i], result[j] = result[j], result[i]\n    return result\n\nnumbers = [5, 3, 8, 4, 2, 9, 1]\nprint(time_critical_sort(numbers))\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#problems","title":"Problems","text":"<ul> <li>Unnecessary Copy (<code>nums.copy()</code>): Adds O(n) time and extra memory traffic.</li> <li>Inefficient Sorting (O(n\u00b2)): Nested loops are too slow for large lists.</li> <li>Redundant Operations: Many swaps; ignores better built\u2011in algorithms and cache behaviour.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#example-2-use-the-optimized-builtin-sort-timsort","title":"Example 2 \u2014 Use the Optimized Built\u2011in Sort (Timsort)","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#code_1","title":"Code","text":"<pre><code>def time_critical_sort(nums):\n    nums.sort()  # In\u2011place sorting, Timsort (average/worst O(n log n))\n    return nums\n\nnumbers = [5, 3, 8, 4, 2, 9, 1]\nprint(time_critical_sort(numbers))\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#benefits","title":"Benefits","text":"<ol> <li>Reduced Complexity: Eliminates the extra O(n) copy and unnecessary swaps.</li> <li>Efficient Sorting: Leverages Python\u2019s highly optimized Timsort for real\u2011world data.</li> <li>Improved Readability: The code states the intent directly \u2014 sort the list.</li> </ol> <p>Takeaway: prefer a single well\u2011tuned library call over custom O(n\u00b2) loops.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#example-3-spacecritical-structure-with-redundant-fields","title":"Example 3 \u2014 Space\u2011Critical Structure with Redundant Fields","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#code_2","title":"Code","text":"<pre><code>class UserData:\n    def __init__(self, username, email, age, address, phone, preferences, bio, membership):\n        self.username = username\n        self.email = email\n        self.age = age\n        self.address = address\n        self.phone = phone\n        self.preferences = preferences\n        self.bio = bio\n        self.membership = membership\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#problems_1","title":"Problems","text":"<ul> <li>Redundant Fields: Not all attributes are required for core features.</li> <li>High Memory Usage: Large fields (e.g., <code>address</code>, <code>bio</code>, <code>preferences</code>) bloat RAM/serialization.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#example-4-minimal-schema-load-on-demand","title":"Example 4 \u2014 Minimal Schema + Load on Demand","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#code_3","title":"Code","text":"<pre><code>class UserData:\n    def __init__(self, username, email, membership):\n        self.username = username\n        self.email = email\n        self.membership = membership\n\n    # Additional details (address, bio, preferences) can be fetched on demand\n    # from a database or profile service only when needed.\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/relentlesssuspicion/#benefits_1","title":"Benefits","text":"<ol> <li>Reduced Memory Usage: Store only essential data in memory-critical paths.</li> <li>Load On Demand: Expensive/large fields are retrieved only when required, improving footprint and cache locality.</li> </ol> <p>Takeaway: question each field; keep the in\u2011memory representation minimal and fetch the rest lazily.</p> <p>Overall insight. Relentless suspicion is about removing every instruction and field that does not measurably contribute to the result. Measure, simplify, and rely on optimized primitives wherever possible.</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/","title":"Complexity management","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/","title":"Monitoring and performance engineering","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/","title":"Architectures for Big Data","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/","title":"Parallel programming","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/","title":"Map reduce","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/","title":"Distributed file systems","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/","title":"Development of applications for execution on Big Data clusters","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/","title":"Vector programming","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""}]}