{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Big DataWelcome to Big Data","text":""},{"location":"#big-data_1","title":"\ud83d\udcca Big Data","text":"<p>Big Data is a compulsory course of the 3rd year in the Degree in Data Science and Engineering (1st semester, 6 ECTS). It provides students with the theoretical foundations and practical skills to design, implement, and analyse data-intensive applications that are reliable, scalable, and maintainable.</p> <p>The course combines theory, practical exercises, and laboratory sessions. Weekly activities are closely related to the theoretical content, reinforcing knowledge through a group project with iterative development and presentations. Students will acquire essential competences in parallel programming, distributed systems, and performance engineering \u2014 all key skills for modern data engineering and analytics.</p> <p>The learning approach emphasises individual and team work, fostering the ability to apply theoretical concepts to real-world Big Data problems. This course serves as a solid foundation for advanced subjects related to distributed systems, cloud computing, and large-scale machine learning.</p> <p>Faculty:</p> <ul> <li>Jos\u00e9 Juan Hern\u00e1ndez Cabrera. Practical Sessions Responsible  </li> <li>Jos\u00e9 \u00c9vora G\u00f3mez.  </li> <li>Mar\u00eda Dolores Afonso Su\u00e1rez. Course Coordinator  </li> </ul> <p></p> <p></p> <p> </p> <p>\u00a9 2025 Mar\u00eda Dolores Afonso Su\u00e1rez. Este material se distribuye bajo licencia Creative Commons Atribuci\u00f3n 4.0 Internacional (CC BY 4.0).</p>"},{"location":"contents/","title":"Contents","text":"<p>Big Data is structured in two blocks that guide students from the theoretical foundations of complexity and architectures, to the practical aspects of distributed and parallel programming. The aim is to provide the knowledge and skills required to understand, design, and implement scalable and efficient data-intensive applications.</p> <p>BLOCK 1. Theoretical Concepts of Big Data covers complexity management, monitoring and performance engineering, and architectures for Big Data systems. These topics provide the conceptual basis for understanding the challenges of reliability, scalability, and fault tolerance in large-scale data systems.</p> <p>BLOCK 2. Distributed and Parallel Programming introduces the fundamentals of parallel programming, MapReduce, distributed file systems, cluster-oriented application development, and vector programming. These topics consolidate the student\u2019s ability to design and implement Big Data solutions that exploit parallelism, distribution, and modern computational infrastructures.</p>"},{"location":"introduction/","title":"Introduction to Big Data","text":"<p>A new era of data-driven innovation</p>"},{"location":"introduction/#from-early-computing-to-the-digital-revolution","title":"From Early Computing to the Digital Revolution","text":"<p>The journey of Big Data starts with the evolution of computing itself:</p> <ul> <li>1950s: John von Neumann formalized computer architecture, setting the basis for digital computation. At the same time, advances in numerical algorithms and game theory influenced how data and complexity would be modeled.  </li> <li>1960s\u20131970s: Paul Eisler\u2019s printed circuit boards and John Bardeen\u2019s transistor discoveries paved the way for miniaturization. Jack Kilby\u2019s microchip and Douglas Engelbart\u2019s vision of interactive computing changed how people interacted with technology.  </li> <li>1980s\u20131990s: Ted Nelson conceptualized hypertext, Robert Metcalfe created Ethernet, and Tim Berners-Lee at CERN invented the World Wide Web, democratizing access to global information. Alan Kay and Engelbart further advanced the idea of graphical user interfaces (GUI).  </li> <li>2000s: Complex systems modeling, network growth, and ubiquitous devices fueled an explosion of data that set the stage for the Big Data era.</li> </ul> <p> Big Data era</p>"},{"location":"introduction/#the-convergence-of-statistics-and-artificial-intelligence","title":"The Convergence of Statistics and Artificial Intelligence","text":"<p>Big Data analytics is powered by advances in mathematics, statistics, and AI:</p> <ul> <li>Thomas Bayes: Introduced probabilistic reasoning, enabling decision-making under uncertainty.  </li> <li>Geoffrey Hinton: Pioneered deep learning, with neural networks that can process speech, images, and language at scale.  </li> <li>Richard Sutton: Advanced reinforcement learning, allowing systems to learn by trial and error, simulating real-world decision-making.  </li> </ul> <p>Together, these approaches transformed massive datasets into knowledge and actionable insights.</p> <p></p>"},{"location":"introduction/#what-is-big-data","title":"What Is Big Data?","text":"<p>Big Data refers to datasets that are too large, fast, or diverse to be managed by traditional methods. Its defining characteristics are often expressed as the 5Vs:</p> <ul> <li>Volume: massive quantities of data.  </li> <li>Velocity: real-time or near real-time data flows.  </li> <li>Variety: text, images, video, audio, and sensor data.  </li> <li>Veracity: ensuring accuracy and reliability.  </li> <li>Value: extracting useful insights for decision-making.  </li> </ul> <p></p>"},{"location":"introduction/#how-big-is-big","title":"How big is \u201cBig\u201d?","text":"<p>An exabyte (EB) equals one million terabytes (TB). For example: - 40 EB = 40,000 PB = 40,000,000 TB = 40,000,000,000 GB. This unimaginable scale is equivalent to millions of modern hard drives, highlighting the storage and processing challenges of Big Data.</p>"},{"location":"introduction/#big-data-architectures","title":"Big Data Architectures","text":"<ul> <li>Data Lake: Centralized repository that stores raw, unstructured, and semi-structured data. Highly flexible, but lacks governance and fast querying.  </li> <li>Data Warehouse: Structured, schema-based system optimized for analytics and business intelligence (BI). Enforces ACID transactions and strong data consistency.  </li> <li>Data Lakehouse: Hybrid architecture combining the scalability of lakes with the reliability of warehouses. Supports both raw and structured data, ACID compliance, and efficient queries.  </li> </ul> <p>The Lakehouse approach is becoming the standard in modern Big Data platforms.</p> <p></p>"},{"location":"introduction/#highlights-of-big-data-in-the-21st-century","title":"Highlights of Big Data in the 21st Century","text":"<p>Recent and emerging trends shaping Big Data include:</p> <ol> <li>Rise of cloud-native Big Data solutions.  </li> <li>Real-time data processing at scale.  </li> <li>Privacy and governance as central challenges.  </li> <li>Edge computing for localized analytics.  </li> <li>Integration of AI and machine learning into data platforms.  </li> <li>Big Data applications in healthcare and medicine.  </li> <li>Data democratization and self-service analytics.  </li> <li>Data lakes and lakehouses as dominant architectures.  </li> <li>Advances in Natural Language Processing (NLP) for text and speech.  </li> <li>Quantum computing as a future enabler of large-scale analytics.  </li> <li>Big Data for sustainability and climate change.  </li> <li>Data Fabric and Data Mesh architectures for decentralized governance.  </li> </ol>"},{"location":"introduction/#the-impact-of-big-data-on-society","title":"The Impact of Big Data on Society","text":"<p>Big Data influences multiple aspects of daily life and industry:</p> <ul> <li>Spam and fraud detection.  </li> <li>Recommender systems powering e-commerce and entertainment.  </li> <li>Emotional AI analyzing human sentiment.  </li> <li>Sensorization and IoT for smart cities, vehicles, and health.  </li> <li>Video analysis for security, transport, and medical imaging.  </li> </ul> <p> Spam</p> <p> Fraud</p> <p> Emotional Intelligence</p> <p> Sensorization</p> <p> Recommendation systems</p> <p>These applications show Big Data\u2019s power to deliver innovation, while also raising challenges of ethics, privacy, and fairness.</p>"},{"location":"introduction/#why-study-big-data","title":"Why Study Big Data?","text":"<p>By the end of this course, students will be able to:</p> <ul> <li>Design scalable and distributed systems.  </li> <li>Implement parallel programming and data-intensive algorithms.  </li> <li>Integrate machine learning and AI into Big Data workflows.  </li> <li>Address social and ethical challenges in the use of large-scale data.  </li> </ul> <p>Big Data is not only about technology \u2014 it is about shaping the future of science, business, and society in a data-driven world.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Architectures_for_Big_Data/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/","title":"Exercises on Big-O Notation","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-1-mapping-the-complexity-landscape","title":"Variant 1 \u2014 Mapping the Complexity Landscape","text":"<p>Goal. Empirically identify the time complexity of several algorithms using synthetic datasets of increasing size, and contrast it with the theoretical complexity.</p> <p>Tasks. 1. Choose three different complexity classes (O(1), O(log n), O(n), O(n log n), etc.). 2. Measure execution times for increasing input sizes using <code>System.nanoTime()</code>. 3. Plot results and normalize (<code>time/n</code>, <code>time/n log n</code>, etc.) to recognize the class. 4. Write a report for each algorithm with hypothesis, evidence, and discussion.</p> <p>Deliverables. - Report (4\u20136 pages) with plots and analysis. - Code with measurement scripts.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-2-scaling-limits-in-practice","title":"Variant 2 \u2014 Scaling Limits in Practice","text":"<p>Goal. Explore the practical limits of different algorithmic complexities by running experiments until execution times become impractical, and compare results to theoretical expectations.</p> <p>Tasks. 1. Select three complexity classes (e.g., O(n), O(n log n), O(n\u00b2)). 2. Define a \u201ctime budget\u201d (e.g., 5 seconds per experiment). 3. Increase input size <code>n</code> step by step until execution exceeds the budget. 4. Record the maximum feasible <code>n</code> for each algorithm and compare with theoretical growth. 5. Discuss discrepancies between theoretical predictions and practical limits (e.g., due to JVM, caching, or hardware).  </p> <p>Deliverables. - Report (3\u20135 pages) with tables of <code>n_max</code> values and discussion. - Code with input scaling scripts.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#variant-3-comparing-growth-rates-on-the-same-plot","title":"Variant 3 \u2014 Comparing Growth Rates on the Same Plot","text":"<p>Goal. Visually demonstrate differences in algorithmic growth rates by plotting multiple complexity classes together for the same input range.  </p> <p>Tasks. 1. Select at least four algorithms with distinct complexities (e.g., O(1), O(log n), O(n), O(n\u00b2)). 2. Measure execution times for the same sequence of input sizes. 3. Plot all results on the same chart (possibly log-log scale for clarity). 4. Highlight where one complexity \u201covertakes\u201d another (e.g., O(n\u00b2) becomes much slower than O(n log n)). 5. Provide an interpretation that connects plots to theoretical curves.  </p> <p>Deliverables. - Report (4\u20136 pages) with combined plots and written interpretation. - Annotated code to reproduce graphs.  </p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/bigO_exercises/#common-notes","title":"Common Notes","text":"<ul> <li>Use definitions of Big O notation, best/worst/average cases.</li> <li>Always explain what your analysis counts (iterations, comparisons, assignments).</li> <li>Differentiate time and space when relevant.</li> <li>(Optional) Use JMH for micro-benchmarks in Java.</li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/","title":"Ejemplos","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#velocidad-de-ejecucion","title":"Velocidad de ejecuci\u00f3n","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#python","title":"Python","text":"<pre><code>import random\nfrom time import *\n\nn = 1024\n\nA = [[random.random() for _ in range(n)] for _ in range(n)]\nB = [[random.random() for _ in range(n)] for _ in range(n)]\nC = [[0 for _ in range(n)] for _ in range(n)]\n\nstart = time()\nfor i in range(n):\n    for j in range(n):\n        for k in range(n):\n            C[i][j] += A[i][k] * B[k][j]\n\nend = time()\n\nprint(\"%.6f\" % (end - start))\n\n# Python. Runing time aroud: 409 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#java","title":"Java","text":"<pre><code>import java.util.Random;\n\npublic class Matrix {\n\n    static int n = 1024;\n    static double[][] a = new double[n][n];\n    static double[][] b = new double[n][n];\n    static double[][] c = new double[n][n];\n\n    public static void main(String[] args) {\n        Random random = new Random();\n        for (int i = 0; i &lt; n; i++) {\n            for (int j = 0; j &lt; n; j++) {\n                a[i][j] = random.nextDouble();\n                b[i][j] = random.nextDouble();\n                c[i][j] = 0;\n            }\n        }\n\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; n; i++) {\n            for (int j = 0; j &lt; n; j++) {\n                for (int k = 0; k &lt; n; k++) {\n                    c[i][j] += a[i][k] * b[k][j];\n                }\n            }\n        }\n        long stop = System.currentTimeMillis();\n\n        System.out.println((stop - start) * 1e-3);\n    }\n}\n\n// Java. Runing time aroud: 7.76 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#rust","title":"Rust","text":"<pre><code>use std::time::SystemTime;\nuse rand::Rng;\n\nfn main() {\n    let n: usize = 1024;\n    let mut a: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n    let mut b: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n    let mut c: Vec&lt;Vec&lt;f64&gt;&gt; = vec![vec![0.0_f64; n]; n];\n\n    let mut rng = rand::thread_rng();\n    for i in 0..n {\n        for j in 0..n {\n            a[i][j] = rng.gen::&lt;f64&gt;();\n            b[i][j] = rng.gen::&lt;f64&gt;();\n        }\n    }\n\n    let start: SystemTime = SystemTime::now();\n    for i in 0..n {\n        for j in 0..n {\n            for k in 0..n {\n                c[i][j] += a[i][k] * b[k][j];\n            }\n        }\n    }\n    let elapsed: Result&lt;std::time::Duration, std::time::SystemTimeError&gt; = start.elapsed();\n\n    println!(\"Elapsed: {:.2?}\", elapsed);\n}\n\n// Rust. Runing time aroud: 7.91 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/codigos/#c","title":"C","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;sys/time.h&gt;\n\n#define n 1024\ndouble a[n][n];\ndouble b[n][n];\ndouble c[n][n];\n\nstruct timeval start, stop;\n\nint main() {\n    for (int i = 0; i &lt; n; ++i) {\n        for (int j = 0; j &lt; n; ++j) {\n            a[i][j] = (double) rand() / RAND_MAX;\n            b[i][j] = (double) rand() / RAND_MAX;\n            c[i][j] = 0;\n        }\n    }\n\n    gettimeofday(&amp;start, NULL);\n    for (int i = 0; i &lt; n; ++i) {\n        for (int j = 0; j &lt; n; ++j) {\n            for (int k = 0; k &lt; n; ++k) {\n                c[i][j] += a[i][k] * b[k][j];\n            }\n        }\n    }\n    gettimeofday(&amp;stop, NULL);\n\n    double diff = stop.tv_sec - start.tv_sec\n                + 1e-6 * (stop.tv_usec - start.tv_usec);\n    printf(\"%0.6f\\n\", diff);\n\n    return 0;\n}\n/* C. Runing time aroud: 0.677867 seconds with 1024x1024 matrices\n</code></pre>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/","title":"Experiments \u2013 Complexity Management","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#introduction","title":"Introduction","text":"<p>\u201cThere are a lot of ways known to do it wrong and which one is right is not clear.\u201d \u2014 James Gosling (1955\u2013)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#brief-biography","title":"Brief Biography","text":"<p>James Gosling, born on May 19, 1955, in Calgary, Canada, is a computer scientist widely known as the \u201cfather of Java.\u201d He studied computer science at the University of Calgary and later earned a Ph.D. in computer science from Carnegie Mellon University. Gosling joined Sun Microsystems in the 1980s, where he led the development of the Java programming language, officially released in 1995. Java quickly became one of the most influential programming languages in the world due to its portability, security, and \u201cwrite once, run anywhere\u201d philosophy. Beyond Java, Gosling has contributed to compiler design, operating systems, and software development tools. He has worked at several leading technology companies, including Sun Microsystems, Oracle, Google, and Amazon Web Services.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#the-creation-of-java","title":"The Creation of Java","text":"<p>In the early 1990s, Gosling and his team at Sun Microsystems initiated the \u201cGreen Project\u201d to develop a language suitable for consumer devices and embedded systems. Gosling designed Java with a strong emphasis on simplicity, object-oriented design, memory management, and platform independence. The language introduced features like automatic garbage collection and a virtual machine (the JVM) that allowed programs to run across different hardware and operating systems. Java\u2019s release in 1995 revolutionized programming by becoming the backbone of enterprise applications, web development, and later Android applications. Today, Java remains one of the most widely used and enduring programming languages, a testament to Gosling\u2019s vision and leadership.</p> <p>Complexity in computation is not only about algorithms but also about implementation choices, programming languages, and hardware utilization. This experiment explores matrix multiplication as a case study to analyze performance differences across programming languages and approaches.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#reminder-matrix-multiplication","title":"Reminder: Matrix Multiplication","text":"<p>If A and B are <code>n \u00d7 n</code> matrices, their product C = AB is also an <code>n \u00d7 n</code> matrix. Each element of the resulting matrix is obtained by combining a row of A with a column of B.</p> <p>Formally:</p> <p></p> <p>[ C[i,j] = \\sum_{k=1}^n A[i,k] \\cdot B[k,j] ]</p> <p>This operation is fundamental in scientific computing, graphics, and machine learning, but it is also computationally intensive, requiring O(n\u00b3) operations in its na\u00efve form.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#algorithm-naive-approach","title":"Algorithm (Na\u00efve Approach)","text":"<p>Pseudocode of the classic algorithm:</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#python","title":"Python","text":"<p>Python Example</p> <p></p> <p>Running time 409.45 seconds with 1024 x 1024 matrices</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#rust","title":"Rust","text":"<p>Rust Example</p> <p></p> <p>Running time 7.91 second with 1024 x 1024 matrices</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#java","title":"Java","text":"<p>Java Example</p> <p></p> <p>Running time 7.76 seconds with 1024 x 1024 matrices 52x faster than python</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#c","title":"C","text":"<p>C Example</p> <p></p> <p>Running time 0.677867 seconds with 1024 x 1024 matrices</p> <p>11x faster than java</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#tiobe-programming-community-index","title":"TIOBE Programming Community Index","text":"<p>The image represents the TIOBE Programming Community Index, which measures the relative popularity of programming languages over time. It does not indicate the best language but rather the most used and most visible in the global developer community, based on search engines, courses, and vendors.</p> \ud83d\udca1 Details <p>Java (green): Dominated the early 2000s with more than 25% share but has steadily declined, although it remains highly relevant.</p> <p>C (black): Extremely stable and consistently strong, often alternating with Java in the top positions.</p> <p>Python (light blue): Shows explosive growth after 2015, becoming the most popular language since 2020. This reflects the rise of data science, machine learning, and artificial intelligence.</p> <p>C++ (orange): Popular in the early 2000s, now stabilised at around 8\u201310%.</p> <p>C# (dark blue): Grew quickly with the .NET ecosystem in the 2000s and maintains a solid mid-level share.</p> <p>PHP (aqua): Very popular in web development between 2005\u20132010 but declined as JavaScript frameworks and other technologies took over.</p> <p>JavaScript (yellow): Maintains a stable share, though its dominance in web applications is not fully reflected in TIOBE\u2019s methodology.</p> <p>Other languages (SQL, Assembly, Visual Basic, etc.): Remain present in niche applications.</p> \ud83d\udca1 Conclusions <p>C and Java were the long-time leaders of the programming world.</p> <p>Python\u2019s meteoric rise illustrates how industry trends (AI, data analytics) can change the landscape of programming.</p> <p>The index shows that language popularity evolves with technological needs, and students should be aware of both long-standing and emerging languages.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#exercise-1-complexity-experiment","title":"EXERCISE 1 Complexity Experiment","text":"<p>Given the matrix multiplication algorithm, how would you optimize the storage and management of the input data to improve the efficiency of the computation? Consider both memory access patterns and the use of specialized data structures.</p> <p>Solution</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#benchmarking-in-complexity-management","title":"Benchmarking in Complexity Management","text":"<p>Benchmarking is the methodology of comparing processes with respect to specific performance measures such as execution time, memory usage, throughput, or scalability.  </p> <p>It allows us to: - Evaluate performance under different conditions. - Compare technologies and frameworks. - Identify bottlenecks in computation. - Optimize resources. - Ensure scalability as systems and data grow.  </p> <p>The benchmarking process generally follows three key steps:</p> <ol> <li>Setup the experiment: Define datasets, algorithms, and parameters.  </li> <li>Execution: Run tests under controlled conditions.  </li> <li>Analysis: Interpret results, compare metrics, and extract insights.  </li> </ol>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#case-study-examples","title":"Case Study Examples","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#1-performance-evaluation","title":"1. Performance Evaluation","text":"<p>Imagine a company processing large datasets in real time for fraud detection. Using a framework such as Apache Spark, benchmarking helps measure how many transactions per second the system can handle before latency increases. This determines the optimal configuration to maintain performance while minimizing delays.</p> \ud83d\udca1 Proposal <p>Here\u2019s a real-world case where benchmarking helps: fraud detection with Spark.</p> <p>If the system starts to show latency at 50,000 transactions per second, what changes could you propose to improve performance?</p> \ud83d\udca1 Reflexion <p>If the system shows latency at 50,000 transactions per second, you could improve performance by:</p> <p> Scaling horizontally \u2192 add more nodes or executors to increase parallelism.</p> <p> Optimizing resources \u2192 tune memory and CPU allocation.</p> <p> Partitioning the data stream \u2192 add more Kafka/Spark partitions for better distribution.</p> <p> Reducing state size \u2192 use watermarks or windowing to avoid unbounded memory growth.</p> <p> Optimizing the code \u2192 avoid unnecessary shuffles and use efficient libraries.</p> <p>In summary, the goal is to increase parallelism, make better use of resources, and control state growth, so the system can process more transactions without adding latency.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#2-technology-comparison","title":"2. Technology Comparison","text":"<p>A research team compares Apache Hadoop and Apache Flink for batch processing. - Hadoop: More efficient for massive, distributed storage and processing. - Flink: Better for real-time analytics. Benchmarking on workloads like social media data helps them select the best tool.</p> \ud83d\udca1 Proposal <p>This is an example of how benchmarking guides technology choice: Hadoop vs Flink.</p> <p>If you had to process social media streams, which system would you benchmark and why?</p> \ud83d\udca1 Reflexion If I had to process social media streams, I would benchmark Apache Flink because it is optimized for real-time and low-latency stream processing. Hadoop is more efficient for large-scale batch processing, but social media data requires continuous analysis, so Flink would likely perform better for this use case."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#3-resource-optimization","title":"3. Resource Optimization","text":"<p>In cloud environments, resources are billed by usage. Benchmarking reveals how different CPU/memory configurations affect the runtime of machine learning tasks. For example, doubling memory but keeping CPU constant may cut processing time by half \u2014 optimizing cost-performance balance.</p> \ud83d\udca1 Proposal <p>This is how benchmarking can guide resource allocation in cloud environments.</p> <p>If doubling memory halves the processing time, would you consider it cost-effective even if memory is twice as expensive as CPU?</p> \ud83d\udca1 Reflexion In order to process social media streams, maybe benchmark Apache Flink is a good choice because it is optimized for real-time and low-latency stream processing. Hadoop is more efficient for large-scale batch processing, but social media data requires continuous analysis, so Flink would likely perform better for this use case."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#4-identifying-bottlenecks","title":"4. Identifying Bottlenecks","text":"<p>In ETL (Extract, Transform, Load) workflows, benchmarking can reveal network transfer speed as the bottleneck between Amazon S3 and Hadoop clusters. This insight guides investment in better infrastructure or alternative transfer methods.</p> \ud83d\udca1 Proposal <p>Benchmarking can reveal whether the bottleneck lies in the network rather than in storage or compute.</p> <p>If your ETL pipeline is slow, how would you use benchmarking to determine whether the problem is CPU, storage, or network?</p> \ud83d\udca1 Reflexion To identify bottlenecks in an ETL pipeline, I would run benchmarks that measure the performance of each stage separately\u2014data transfer, storage, and processing. If the results show that network transfer between storage (e.g., S3) and the processing engine (e.g., Hadoop) is significantly slower than computation or storage access, then the network is the bottleneck. This insight helps decide whether to improve network capacity or adjust data transfer methods."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#5-ensuring-scalability","title":"5. Ensuring Scalability","text":"<p>An online retailer analyzing customer behavior starts with 1M transactions but expects exponential growth. Scalability benchmarks test workloads of 10M, 50M, or 100M transactions to ensure the system scales without performance degradation.</p> \ud83d\udca1 Proposal <p>Scalability benchmarking helps companies anticipate growth and ensure that performance remains stable as data volume increases.</p> <p>If your dataset grew 100 times larger, how would you benchmark your system to check whether it can scale without performance degradation?</p> \ud83d\udca1 Reflexion To benchmark scalability, the dataset size can be gradually increased (e.g., 10x, 50x, 100x) while measuring how the system\u2019s performance changes. If the system maintains acceptable response times and throughput as the data grows, then it scales effectively. Otherwise, the benchmark highlights where improvements in infrastructure or algorithms are required."},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#exercise-2-complexity-experiment","title":"EXERCISE 2 Complexity Experiment","text":"<p>Execute the algorithms associated to different methods or programming languages with different datasets (size of the matrix) and extract measures (execution time).</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#optimizing-matrix-multiplication","title":"Optimizing Matrix Multiplication","text":"<p>Students are expected to: 1. Implement algorithms for matrix multiplication (na\u00efve and optimized). 2. Execute them in different languages or frameworks (e.g., Python, Java, C, Rust). 3. Vary dataset size (matrix dimension) and record metrics (execution time, memory usage). 4. Compare results across methods. 5. Draw conclusions about efficiency, scalability, and hardware utilization.</p> <p></p> <p>Comparison of common time complexities (Big-O). The graph shows how algorithm performance scales with input size, from constant time O(1) to exponential O(2^n), highlighting the dramatic differences in growth rates.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/experiments/#documenting-benchmarks","title":"Documenting Benchmarks","text":"<p>When reporting experiments, follow a structured format:</p> <p>Title - Clear and concise, e.g. Benchmarking Matrix Multiplication Algorithms on Big Data Platforms.  </p> <p>Abstract - Briefly state the task, algorithms compared, performance measures, and conclusions.  </p> <p>Body - Describe setup, methodology, datasets, and tools. - Provide tables or charts (execution times, speedups, scalability curves).  </p> <p>Conclusions - Summarize key findings. - State recommendations for future research or practice.  </p> <p>Example Abstract: We study the behavior of several matrix multiplication algorithms used for large-scale computation. We analyze execution time, memory usage, and scalability. Our experiments provide a reproducible benchmark across a variety of datasets to guide future research in Big Data performance engineering. Based on our results, we recommend combining optimized libraries (BLAS, MKL) with GPU acceleration for the most efficient solutions.</p> <p>Scientific Papers</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/","title":"Lectures","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#introduction","title":"Introduction","text":"<p>Programmers are always surrounded by complexity; we cannot avoid it. \u2014 Tony Hoare (1934\u2013)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#brief-biography","title":"Brief Biography","text":"<p>Charles Antony Richard Hoare, known as Tony Hoare, was born in Colombo (Sri Lanka, then Ceylon) in 1934. Originally studying Classics at Oxford, he later turned to mathematics and computer science, becoming one of the most influential figures in programming languages and algorithms. In 1960, he created Quicksort, one of the most widely used and efficient sorting algorithms. Hoare worked on compiler design, programming language theory, and software verification. He was a professor at the University of Oxford and later a principal researcher at Microsoft Research. In recognition of his contributions, he received the ACM Turing Award in 1980.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#null-references-the-billion-dollar-mistake","title":"Null References \u2013 The \"Billion-Dollar Mistake\"","text":"<p>In 1965, while designing the programming language ALGOL W, Tony Hoare introduced the concept of null references (or null pointers) as a way to indicate the absence of a value in reference variables. Decades later, he admitted this design decision was a serious error, calling it his \"billion-dollar mistake\", since null references have caused countless errors, system crashes, vulnerabilities, and financial losses across the software industry. Hoare discussed this in a 2009 talk at QCon London, reflecting on the immense cost of null-related bugs. Modern programming languages such as Kotlin, Swift, and Rust have since introduced explicit type systems that differentiate between nullable and non-nullable values, reducing the risk of null pointer exceptions and improving software reliability.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o","title":"BiG O","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o_1","title":"Big O","text":"<p>The order of complexity in algorithms is a way to measure an algorithm's efficiency in terms of time and/or space as the input size grows. This measure is expressed using Big O notation, which provides a way to describe the asymptotic behaviour of the algorithm, meaning how it behaves when the input becomes very large.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#types-of-complexity","title":"Types of Complexity","text":"<ul> <li>Time Complexity: Measures the execution time of the algorithm based on the input size n. It focuses on how many operations the algorithm performs as the input size increases.  </li> <li>Space Complexity: Measures the amount of memory an algorithm needs as a function of the input size n.  </li> </ul>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o-notation","title":"Big O Notation","text":"<p>Big O notation describes the worst-case scenario, representing the upper limit of time or space required as n grows. </p> <p></p> \ud83d\udca1 Details <p>Also called Landau's symbol, is a symbolism used in complexity theory, computer science, and mathematics to describe the asymptotic behavior of functions.</p> <p>German mathematician Edmund Landau, who was one of the first to use this type of notation in mathematical analysis, to describe the behaviour of mathematical functions when they tend to infinity.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o1","title":"O(1)","text":"<p>O(1): Constant. The execution time does not change with the size of the input. Example: Print a message. </p> <p></p> \ud83d\udca1 Details <p>Constant time complexity (O(1)) because the execution time of the System.out.println statement is independent of the input size.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o1_1","title":"O(1)","text":"<p>O(1): Constant. Why is it \u201cprint a message\u201d O(1)? Because the execution time of the <code>System.out.println</code> statement is independent of the input size.  </p> <p></p> \ud83d\udca1 Details <p>Constant time complexity (O(1)) because the execution time of the System.out.println statement is independent of the input size.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#olog-n","title":"O(log n)","text":"<p>O(log n): Logarithmic. The execution time grows logarithmically as the input size increases. Example: Binary search. </p> <p></p> \ud83d\udca1 Details <p>For an array of size n = 1,000,000, binary search will take roughly log\u20612(1,000,000)\u224820 comparisons to find the target.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#olog-n_1","title":"O(log n)","text":"<p>O(log n): Logarithmic. Why is \u201cbinary search\u201d O(log n)? Because at each step, it halves the search space. So, if you double the size of the array, the number of steps required to find an element increases only by 1 (since log\u2082(2n) = log\u2082(n) + 1).  </p> <p></p> \ud83d\udca1 Details <p>Logarithmic Growth: Binary search is logarithmic because at each step, it halves the search space. So if you double the size of the array, the number of steps required to find an element increases only by 1 (since log\u20612(2n)=log\u20612(n)+1\\log_2(2n) = \\log_2(n) + 1log2\u200b(2n)=log2\u200b(n)+1).</p> <p>For example:</p> <p> - For an array of size 1,000,000, binary search requires about 20 comparisons.</p> <p> - For an array of size 2,000,000, it requires 21 comparisons.</p> <p> - Even though the input size doubled, the time to search increased by only a single step, which is characteristic of logarithmic growth.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on","title":"O(n)","text":"<p>O(n): Linear. The execution time grows proportionally with the input size. Example: Iterating through an array.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_1","title":"O(n)","text":"<p>O(n): Linear. Why is \u201citerating through an array\u201d O(n)? The for loop iterates exactly n times. The number of operations (iterations) grows linearly with the input size. If n = 1,000,000 (or 2,000,000) the loop runs 1,000,000 (2,000,000) times.  </p> <p></p> \ud83d\udca1 Details <p> Why is This O(n) Linear Time Complexity?</p> <p>In this example, the for loop runs exactly n times, which means the number of iterations grows linearly with the input size n.</p> <p>For example:</p> <p> - If n = 10, the loop runs 10 times.</p> <p> - If n = 1000, the loop runs 1000 times.</p> <p> - If n = 1,000,000, the loop runs 1,000,000 times.</p> <p>O(n) means that the execution time of the algorithm is proportional to the size of the input. If you double the size of the input (n), the time to complete the loop will approximately double as well.</p> <p>So, if it takes 1 second to run 1,000,000 iterations, it will take roughly 2 seconds to run 2,000,000 iterations (assuming constant factors like hardware performance stay the same).</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on-log-n","title":"O(n log n)","text":"<p>O(n log n): Linear-logarithmic. Common in sorting algorithms like Merge Sort and Heap Sort. Example: Sorting (Merge Sort).  </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on-log-n_1","title":"O(n log n)","text":"<p>O(n log n): Linear-logarithmic. Why is sorting (merge sort) O(n log n)?  </p> <p></p> \ud83d\udca1 Details <p> Why is this O(n log n)?</p> <p>Sorting an array of n elements using efficient algorithms like Merge Sort or Timsort takes O(n log n) time:</p> <p> - The array is divided into smaller subarrays recursively (halving the array size at each step), which takes logarithmic time O(log\u2061n)O(\\log n)O(logn).</p> <p> - Merging the sorted subarrays takes linear time O(n)O(n)O(n) because every element needs to be checked at least once.</p> <p> - Therefore, the overall time complexity is O(n \\log n).</p> <p>Example:</p> <p>For an array of size 100,000, the time complexity would be proportional to:</p> <p> - n=100,000n = 100,000n=100,000.</p> <p> - log\u2061n=log\u20612100,000\u224816.61\\log n = \\log_2 100,000 \\approx 16.61logn=log2\u200b100,000\u224816.61</p> <p> - So, the number of operations would be proportional to 100,000\u00d716.61\u22481,661,000100,000 \\times 16.61 \\approx 1,661,000100,000\u00d716.61\u22481,661,000 operations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on2","title":"O(n\u00b2)","text":"<p>O(n\u00b2): Quadratic. The execution time is proportional to the square of the input size (e.g., sorting algorithms like Bubble Sort or Selection Sort). Example: Bubble sort. </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on2_1","title":"O(n\u00b2)","text":"<p>O(n\u00b2): Quadratic. Why is Bubble Sort O(n\u00b2)?  </p> <p></p> \ud83d\udca1 Details <p> Why is Bubble sort O(n\u00b2)?</p> <p>The Bubble Sort algorithm has two nested loops: one that iterates over the array and another that iterates over the unsorted part of the array to compare and swap adjacent elements.</p> <p> The outer loop runs n times, and for each iteration of the outer loop, the inner loop also runs n times. This leads to a total of n\u00d7n=n\u00b2 operations. In simple terms, as the input size n grows, the time it takes to sort the array grows quadratically. </p> <p>For example:</p> <p> - If n = 1000, the algorithm performs approximately 1000\u00d71000=1,000,0001000 \\times 1000 = 1,000,0001000\u00d71000=1,000,000 operations.</p> <p> If n = 2000, the algorithm performs approximately 2000\u00d72000=4,000,0002000 \\times 2000 = 4,000,0002000\u00d72000=4,000,000 operations.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on3","title":"O(n\u00b3)","text":"<p>O(n\u00b3): Polynomial. The runtime of the algorithm increases cubically as the input size grows. Example: Matrix Multiplication (Triple Nested Loops).</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on3_1","title":"O(n\u00b3)","text":"<p>O(n\u00b3): Polynomial. Why is Matrix Multiplication O(n\u00b3)? </p> <p></p> \ud83d\udca1 Details <p> Why is This O(n\u00b3)?</p> <p>Triple Nested Loops: The three nested loops each iterate n times:</p> <p> - The outer loop iterates over the rows of matrix AAA. </p> <p> - The middle loop iterates over the columns of matrix BBB.</p> <p> - The inner loop computes the dot product for each element of the result matrix.</p> <p>The total number of iterations is n\u00d7n\u00d7n=n^3, which is why the algorithm has O(n\u00b3) time complexity.</p> <p>Example:</p> <p> - If n = 100, the algorithm performs approximately 100\u00d7100\u00d7100=1,000,000100 \\times 100 \\times 100 = 1,000,000100\u00d7100\u00d7100=1,000,000 operations.</p> <p> - If n = 200, the algorithm performs approximately 200\u00d7200\u00d7200=8,000,000200 \\times 200 \\times 200 = 8,000,000200\u00d7200\u00d7200=8,000,000 operations.</p> <p> As n increases, the number of operations grows very quickly, which reflects the cubic nature of the time complexity.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o2n","title":"O(2\u207f)","text":"<p>O(2\u207f): Exponential. The execution time doubles with each increase in the input size, common in brute force problems. Example: Fibonacci Sequence (recursive). </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#o2n_1","title":"O(2\u207f)","text":"<p>O(2\u207f): Exponential. Why is Fibonacci Sequence O(2\u207f)? </p> <p></p> \ud83d\udca1 Details <p> Why is this O(2\u207f)?</p> <p>The total number of calls for fib(n) is close to 2^n, which is why the time complexity is O(2\u207f).</p> <p>Formal Reason:</p> <p> The recurrence relation that describes the time complexity is: </p> <p> T(n)=T(n\u22121)+T(n\u22122)+O(1) </p> <p>This recurrence reflects the fact that each call to fib(n) involves two recursive calls: fib(n - 1) and fib(n - 2). The solution to this recurrence is O(2^n), which means the number of recursive calls grows exponentially with n.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_2","title":"O(n!)","text":"<p>O(n!): Factorial. Extremely inefficient, grows very quickly, seen in permutation and combinatorial problems. Example: Permutation Generation. </p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#on_3","title":"O(n!)","text":"<p>O(n!): Factorial. Why is Permutation Generation O(n!)? </p> \ud83d\udca1 Details <p> Why is Permutation Generation O(n!)?</p> <p> - For every element in the list, we generate all permutations of the remaining elements.</p> <p> - This results in a recursive process where the time complexity grows factorially as the size of the list increases.</p> <p> - The number of possible permutations of a list of n elements is n!, which means that the time complexity of generating all permutations is O(n!).</p> <p>Example:</p> <p> For a list of 5 elements ({1, 2, 3, 4, 5}), there are 5!=1205! = 1205!=120 possible permutations. The function generates all 120 permutations and measures how long this process takes.</p> <p> If you increase the number of elements to 6, the number of permutations increases to 6!=7206! = 7206!=720. For 7 elements, it becomes 7!=5,0407! = 5,0407!=5,040, and so on. The execution time grows factorially with the size of the input list.</p> <p></p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/lectures/#big-o_2","title":"Big O","text":"<p>Big O notation describes the worst-case scenario, representing the upper limit of time or space required as n grows.</p> <p></p> <p>Exercise</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/","title":"scientificpapers.md","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#how-to-write-a-scientific-paper-academic-style-guide-with-examples","title":"How to Write a Scientific Paper (Academic Style Guide with Examples)","text":"<p>Writing a scientific article requires following internationally accepted conventions that ensure clarity, reproducibility, and rigor. A scientific paper must follow a clear and structured format to effectively communicate research findings. Below is a detailed guide to the main sections, including style notes and sample phrases commonly used in academic writing.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#1-title-and-authors","title":"1. Title and Authors","text":"<ul> <li>The title should be concise, informative, and reflect the central contribution.  </li> <li>Avoid vague terms such as \"A Study on...\"; instead, use specific keywords.  </li> <li>Include all authors with institutional affiliations and a corresponding author email.  </li> </ul> \ud83d\udca1 Example titles <p>- Optimizing Neural Network Training with Hybrid Gradient Techniques</p> <p>- A Comparative Study of Quantum Algorithms for Linear Systems</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#2-abstract","title":"2. Abstract","text":"<ul> <li>Length: typically 150\u2013250 words.  </li> <li>Provides a complete but concise overview of the study.  </li> <li>Must include:  </li> <li>Problem or challenge.  </li> <li>Methods and approach.  </li> <li>Key results.  </li> <li>Main conclusion.  </li> </ul> \ud83d\udca1 Example titles <p>- This paper addresses the challenge of...</p> <p>- The proposed method was evaluated on...</p> <p>- Results show a significant improvement in...</p> <p>- These findings suggest that...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#3-keywords","title":"3. Keywords","text":"<ul> <li>Include 4\u20136 specific terms.  </li> <li>Facilitate search and indexing.  </li> </ul> \ud83d\udca1 Examples <p>- \"graph neural networks,\" \"computational linguistics,\" \"data privacy,\" \"blockchain security.\"</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#4-introduction","title":"4. Introduction","text":"<ul> <li>Establishes the context and significance of the research.  </li> <li>Summarizes existing work and identifies the gap.  </li> <li>Ends with objectives and contributions.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Recent studies have demonstrated that...</p> <p>- However, little attention has been paid to...</p> <p>- The aim of this paper is to...</p> <p>- Our contributions are as follows...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#5-problem-statement","title":"5. Problem Statement","text":"<ul> <li>Defines clearly what problem is being addressed.  </li> <li>May include formal definitions, hypotheses, or models.  </li> </ul> \ud83d\udca1 Problem Statement <p>- The problem can be formally defined as...</p> <p>- We hypothesize that...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#6-methodology-method-proposal-solution","title":"6. Methodology (Method / Proposal / Solution)","text":"<ul> <li>Explains how the research was conducted.  </li> <li>Must allow replication by other researchers.  </li> <li>Include: algorithms, models, datasets, tools, hardware/software.  </li> </ul> \ud83d\udca1 Sample phrases <p>- We implemented the proposed approach using...</p> <p>- Experiments were carried out on...</p> <p>- The parameters were set to...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Present experiments systematically and report results clearly.  </li> <li>Use consistent formatting for tables and figures.  </li> <li>Always provide units and ensure charts are readable.  </li> <li>Discuss results in relation to prior work.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Table 1 summarizes the performance of...</p> <p>- As shown in Figure 2, the proposed method outperforms...</p> <p>- These findings are consistent with...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#8-conclusions","title":"8. Conclusions","text":"<ul> <li>Recap the main findings without repeating details.  </li> <li>Emphasize originality and significance.  </li> </ul> \ud83d\udca1 Sample phrases <p>- In conclusion, this work demonstrates that...</p> <p>- The results confirm that...</p> <p>- The study contributes to the field by...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#9-future-work","title":"9. Future Work","text":"<ul> <li>Outline possible directions for extending the study.  </li> <li>Acknowledge current limitations.  </li> </ul> \ud83d\udca1 Sample phrases <p>- Future research could explore...</p> <p>- One limitation of this study is...</p> <p>- Further validation is required in...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#10-references","title":"10. References","text":"<ul> <li>Use a recognized citation style (IEEE, APA, etc.).  </li> <li>Ensure every citation in text has a corresponding reference.  </li> </ul> \ud83d\udca1 Sample phrases <p>- As discussed by Smith et al. [3]...</p> <p>- Following the method in [12]...</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Complexity_Management/scientificpapers/#11-general-style-and-recommendations","title":"11. General Style and Recommendations","text":"<ul> <li>Use precise, formal, and objective language.  </li> <li>Prefer passive or impersonal voice (e.g., \"was conducted\" instead of \"we conducted\").  </li> <li>Proofread for clarity and coherence.  </li> </ul> \ud83d\udca1 Sample academic transitions <p>- Moreover, </p> <p>- In contrast, </p> <p>- Therefore, </p> <p>- It is worth noting that... </p> <p>This academic-style guide not only explains the structure of a paper but also provides language templates that students can adapt in their own writing.</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#-","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#content","title":"Content","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#-_1","title":"-","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#resources","title":"Resources","text":""},{"location":"Block1_Theoretical_Concepts_of_Big_Data/Monitoring_and_Performance_Engineering/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Application_Development_for_Big_Data_Clusters/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Distributed_File_Systems/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Map_Reduce/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Parallel_Programming/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/","title":"Applications","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/applications/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/","title":"Experiments","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/experiments/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/","title":"Lectures","text":"<p>Summary: (Fill with PDA content)</p>"},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#learning-objectives","title":"Learning objectives","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#-","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#content","title":"Content","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#-_1","title":"-","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#resources","title":"Resources","text":""},{"location":"Block2_Distributed_and_Parallel_Programming/Vector_Programming/lectures/#suggested-exercises","title":"Suggested exercises","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/","title":"Complexity management","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/01_complexity_management/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/","title":"Monitoring and performance engineering","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/02_monitoring_and_performance_engineering/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/","title":"Architectures for Big Data","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#-","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#contenidos","title":"Contenidos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#-_1","title":"-","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#recursos","title":"Recursos","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque1_Theoretical_Concepts_of_Big_Data/03_architectures_for_big_data/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/","title":"Parallel programming","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/01_parallel_programming/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/","title":"Map reduce","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/02_map_reduce/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/","title":"Distributed file systems","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/03_distributed_file_systems/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/","title":"Development of applications for execution on Big Data clusters","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/04_app_development_for_big_data_clusters/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/","title":"Vector programming","text":"<p>Resumen: (Rellena con el PDA)</p>"},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#objetivos-de-aprendizaje","title":"Objetivos de aprendizaje","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#-","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#contenidos","title":"Contenidos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#-_1","title":"-","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#recursos","title":"Recursos","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#practicas-sugeridas","title":"Pr\u00e1cticas sugeridas","text":""},{"location":"Bloque2_Distributed_and_Parallel_Programming/05_vector_programming/#evaluacion-y-rubrica-si-aplica","title":"Evaluaci\u00f3n y r\u00fabrica (si aplica)","text":""}]}